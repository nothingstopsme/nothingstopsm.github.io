<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>http://localhost:4000/Blogs/</link>
    <atom:link href="http://localhost:4000/Blogs/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 18 Mar 2020 20:18:01 +0800</pubDate>
    <lastBuildDate>Wed, 18 Mar 2020 20:18:01 +0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Looking into Label Smoothing</title>
        <description>&lt;p&gt;I have forgotten where I obtained this impression about label smoothing: “Since data could be mislablled, to reflect the possibility of errors, labels are weighted to reduce our confidence about them.” While that description seems to present a straightforward intuition, it does not tell the full story, which I came to realise a few days ago after revisited this technique. So here are the missing pieces I have found.&lt;/p&gt;

&lt;h2 id=&quot;regularisation&quot;&gt;Regularisation&lt;/h2&gt;
&lt;p&gt;Given a label $L$ represented as a one-hot vector $L_{\text{one-hot}}$ and the total number of classes $K$, the authors of label smoothing defined the smoothed vector $L_{\text{ls}}$ by a small proportion $\alpha$ as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; L_{\text{ls}}[k] = (1 - \alpha) * L_{\text{one-hot}} [k] + \frac{\alpha}{K} \\
&amp; where\,\,L_{\text{one-hot}}[k] =
\begin{cases}
1,&amp; L = k \\
0,&amp; otherwise
\end{cases}
\end{aligned}
\label{eq:labelSmoothing}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Most probability models fitting categorical distributions are optimised towards a lower expected cross entropy between model estimations and labels. Suppose $P$ is a vector of probabilities for different classes outputted by a model; with the notations from \eqref{eq:labelSmoothing}, the cross entropy $E_{\text{one-hot}}$ between $L_{\text{one-hot}}$ and $P$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; E_{\text{one-hot}} = - \sum_{i=0}^K L_{\text{one-hot}} [i] * log(P[i]) = log(P[k]), L = k
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the one-hot version of labels is replaced with the smoothed one, a different version of cross entropy, $E_{\text{ls}}$ can be obtained:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; E_{\text{ls}} = - \sum_{i=0}^K L_{\text{ls}} [i] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = - \sum_{i=0}^K [(1 - \alpha) * L_{\text{one-hot}} [i] + \frac{\alpha}{K}] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * (-1) * \sum_{i=0}^K L_{\text{one-hot}} [i] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}} = } - \alpha * \sum_{i=0}^K \frac{1}{K} * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * E_{\text{one-hot}} - \alpha * \sum_{i=0}^K \frac{1}{K} * (log(P[i]) - log(\frac{1}{K})) \\
&amp; \phantom{ E_{\text{ls}} = } - \alpha * \sum_{i=0}^K \frac{1}{K} log(\frac{1}{K}) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * E_{\text{one-hot}} + \alpha * KL[\text{Uniform}_K \lVert P] \\
&amp; \phantom{ E_{\text{ls}} = } + \alpha * Entropy(\text{Uniform}_K)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $Entropy(\text{Uniform}_K)$ is a constant, the minimisation of $E_{\text{ls}}$ for a model effectively achieves two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Minimise $E_{\text{one-hot}}$, which is the original optimisation objective of the model, with a weight of $ (1 - \alpha)$.&lt;/li&gt;
  &lt;li&gt;Minimise $KL[\text{Uniform}_K \lVert P]$, which serves as a &lt;strong&gt;regularisor&lt;/strong&gt; to bring the distribution of $P$ towards the one of $\text{Uniform}_K$, with a weight of $\alpha$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So “reduce confidence” can be seen as the result of regularisation which causes models to produce flatter distributions.&lt;/p&gt;

&lt;h2 id=&quot;calibration&quot;&gt;Calibration&lt;/h2&gt;

&lt;p&gt;When doing classification, one will usually use softmax as the final step to produce a vector of the length equal to the number of classes. Despite the fact that all entries in that vector add up to 1, it does not necessarily mean that vector (approximately) describes a distribution of classes. For example, if the result of its two-class classification is $[0.9, 0.1]$, in the case of &lt;strong&gt;badly calibrated models&lt;/strong&gt; that only implies the first class receives a higher (normalised) score, and hence has a higher rank, than the second class does. As for the purpose of mere classification, ranking does do the job; nonetheless, there are some scenarios where distributions of classes are required, so that the accuracy of decisions (the probability that a chosen class is correct) can be estimated, or samples following the same distributions can be generated. To that end, we need to &lt;strong&gt;calibrate&lt;/strong&gt; our model, and one way to achieve that is to apply label smoothing during training.&lt;/p&gt;

&lt;p&gt;But, why does label smoothing have such power? A recent paper has presented an interesting insight into the penultimate layer, right before softmax, of classification models, and I would like to add my own interpretation based on their finding to offer some intuition.&lt;/p&gt;

&lt;p&gt;First, in most cases, a penultimate layer only involve a linear transformation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
y_k(x) = x^T \odot w_k
\end{aligned}&lt;/script&gt;

&lt;p&gt;where $x$ is the input to the penultimate layer, and $w_k$ describes for class $k$ the linear mapping which produces the logit for class $k$. The softmax result for class $k$ given $x$, $s_k(x)$, can then be expressed in terms of $x^T \dot w_k$ (assuming the number of classes is $K$).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{aligned}
s_k(x) = \frac{exp(x^T \odot w_k)}{\sum_{i=1}^{K} exp(x^T \odot w_i)}
\end{aligned}
\label{eq:softmax}
\end{equation}&lt;/script&gt;

&lt;p&gt;Since the multiplication of both numerator and denominator in \eqref{eq:softmax} by the same constant factors will not change the ratio, \eqref{eq:softmax} can be further derived as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; s_k(x) = \frac{exp(-0.5 \lVert x \rVert_2^2) * exp(-0.5 \lVert w_{max} \rVert_2^2)}{exp(-0.5 \lVert x \rVert_2^2) * exp(-0.5 \lVert w_{max} \rVert_2^2)} \\
&amp; \phantom{s_k(x) =} * \frac{exp(x^T \odot w_k)}{\sum_{i=1}^{K} exp(x^T \odot w_i)} \\
&amp; \phantom{s_k(x)} = \frac{exp(-0.5 \lVert x \rVert_2^2 + x^T \odot w_k - 0.5 \lVert w_{max} \rVert_2^2)}{\sum_{i=1}^{K} exp(-0.5 \lVert x \rVert_2^2 + x^T \odot w_i – 0.5 \lVert w_{max} \rVert_2^2)} \\
&amp; \phantom{s_k(x)} = \frac{exp(-0.5 * (\lVert x \rVert_2^2 - 2 * x^T \odot w_k + \lVert w_{max} \rVert_2^2))}{\sum_{i=1}^{K} exp(-0.5 * (\lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{max} \rVert_2^2))}
\end{aligned}
\label{eq:softmax2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $w_{max}$ refers to the vector with the largest length among all $w_i$, $i \in \{ j \lvert j \in \mathbb{Z}, 0 \leq j \leq K \}$.&lt;/p&gt;

&lt;p&gt;One might notice a high degree of similarity between the terms in the expressions inside exponetial functions in the last equality of \eqref{eq:softmax2}, and the ones which would have been involved in a L2 norm. In effect, they can be linked through the following inequality (let $z(x, w_i) = \lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{max} \rVert_2^2$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; z(x, w_i) = \lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{max} \rVert_2^2 \\
&amp; \phantom{z(x, w_i)} \geq \lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{i} \rVert_2^2 \\
&amp; \phantom{z(x, w_i)} = \lVert x - w_i \rVert_2^2 \geq 0
\end{aligned}
\label{eq:L2Inequality}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Meanwhile, \eqref{eq:softmax2} also suggests that a model trained with some example from class $k$ of which the penultimate representation is $x$ will produce small $z(x, w_k)$ and large $z(x, w_l)$ for all other classes $l \neq k$, so that the distinct differences between them can result in dominant $s_k(x)$ with tiny $s_l(x)$ to reflect the labelling. Since according to \eqref{eq:L2Inequality}, the function $z$ describes an upper bound of the L2 norm squared of the difference between its two input arguments, it can be further inferred that $\lVert x - w_k \rVert_2^2$ should be small while there is room for $\lVert x - w_l \rVert_2^2$ to grow. From a geometric perspective, if $x$ and $w_k$/$w_l$ are viewed as points in a hyperspace, the relation, in terms of bounds of L2 norms, between those points loosely describes a form of clustering, with cluster centres being $w_k$/$w_l$.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/Blogs/assets/images/2020-03-09-LabelSmoothing/PenultimateClustering.png&quot; alt=&quot;A schematic diagram for the geometric view of $x$ and $w_k$/$w_l$&quot; /&gt;
	&lt;figcaption&gt;A schematic diagram for the geometric view of $x$ and $w_k$/$w_l$&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This phenomenon of clustering is one of the discoveries presented in the aforementioned paper; besides, the authors also explored the effect of label smoothing to the widths of clusters, on which my interpretation is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When a model is trained given labels in one-hot encoding, $z(x, w_l)$ can become quite large as $s_l(x)$ is approaching 0. In fact, there is no theoretical upper bound for it; at the same time, rising $z(x, w_l)$ will also boost $s_k(x)$, so $z(x, w_k)$ does not have to be very small to generate a dominant response of softmax for class $k$. That potential increase in $z(x, w_k)$, which corresponds to the upper bounds of the distances between $x$ and $w_k$, suggest the possible presence of broad clusters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The use of label smoothing essentially limits the growth of $z(x, w_l)$ so that the resulting $s_l(x)$ can account for the slight probability assigned to class $l$. That also makes minimisation of $z(x, w_k)$ more necessary as $s_k(x)$ cannot be further increased by unboundedly large $z(x, w_l)$. Therefore, in contrast to the case where one-hot encoding is directly employed, clusters are more likely to be tighter due to a smaller distance bound $z(x, w_k)$ under the effect of label smoothing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, the connection between penultimate layers and clustering introduces a different angle for understanding the subsequent softmax. The equivalent form of the softmax, \eqref{eq:softmax2}, can now be seen an approximation of the posterior probabilities of classes given data under a Gaussian Mixture model, with the assumption that the covariance matrix of every Gaussian is $I * \sigma^2$, $\sigma^2 = 1$, and the prior for classes is a uniform distribution. That assumption, $\sigma^2 = 1$ in particular, implies the probability model is not quite suitable for data coming from broad clusters, since it cannot estimate their posterior probabilities very well, or put it differently, &lt;strong&gt;badly calibrated&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, without label smoothing, the clusters formed by the inputs to the penultimate layer could end up with large widths. In that case, poor calibration occurs because the distribution of those inputs cannot be fitted properly under the model assumption of $\sigma^2 = 1$; this analysis might also explain why previous researches have found temperature scaling, which divides the logits to the softmax function by a constant named temperature $T$, can help to calibrate models:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
s_k^{\text{TS}}(x) = \frac{exp(\frac{x^T \odot w_k}{T})}{\sum_{i=1}^{K} exp(\frac{x^T \odot w_i}{T})}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Since it is equivalent to dividing the function $z$ by $T$ in \eqref{eq:softmax2}, the scaling in effect adjusts $\sigma^2$ so that the sizes of clusters can be better described. Similarly, label smoothing provides the effect of improving the degree of calibration by matching the probability model to the underlying data as temperature scaling does, but it achieves that through encouraging tighter clusters, and hence increasing the validity of $\sigma^2 = 1$.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.02629&quot;&gt;Rafael Müller, Simon Kornblith, and Geoffrey E.Hinton. “When Does Label Smoothing Help?” In:CoRRabs/1906.02629 (2019). arXiv:1906.02629.url:http://arxiv.org/abs/1906.02629&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 09 Mar 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/Blogs/note/LabelSmoothing.html</link>
        <guid isPermaLink="true">http://localhost:4000/Blogs/note/LabelSmoothing.html</guid>
        
        <category>MachineLearning</category>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>The Probability of Winning a Lottery</title>
        <description>&lt;p&gt;For some time I have been reading an interesting book, in which the author analysed real-world events in our daily life from the perspective of statistics and probability. One of its topics, which no doubt relates to the two subjects, is gambling, and there is a section discussing about the strategy of playing a lottery. I have always heard that the chance of winning the top prize of a lottery is less the one of being struck by lightning (at least for the lottery held in Taiwan), so I have not bothered to pay even slight attention to it. Nonetheless, after having read that section describing a statistically reasonable approach to decide the timing of buying lottery tickets, I started wondering maybe winning a prize does not have to be all on the off chance.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;statistically reasonable approach&lt;/em&gt; is actually pretty simple: it is to calculate the expectation of money returned per ticket. Taking one of the major lotteries for example, the game is to pick 6 different numbers from 1 to 49, and based on how many your numbers match the randomly-drawn winner numbers, different prizes will be awarded. The smallest prize in that game is a fixed reward of 400 NTDs, which requires exactly 3 matches on your ticket. The probability of winning that reward is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;P(\text{Smallest Reward})= \frac{Count(\text{winning cases})}{Count(\text{all outcomes})} \nonumber \\
&amp;\phantom{P(\text{Smallest Reward})} = \frac{C_{3}^{6} * C_{3}^{49 - 6}}{C_{6}^{49}} \nonumber \\
&amp;\phantom{P(\text{Smallest Reward})} = 0.01765
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since each ticket costs 50 NTDs, given that &lt;strong&gt;only the smallest prize is offered&lt;/strong&gt;, the expectation of money returned for every ticket bought will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Smallest Reward}(\text{Money returned}) \nonumber \\
&amp; = P(\text{Smallest Reward}) * 400\, – 1 * 50 \nonumber \\
&amp; = -42.94
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;That expectation indicates on average players are destined to lose 42.94 NTDs for every ticket they buy, and there is nothing we can do to change our fate because every factor in the calculation is fixed. However, it does not mean no silver linings can be found when playing lotteries, and they lie in the prizes of which the size can grow.&lt;/p&gt;

&lt;p&gt;For example, in the lottery mentioned above, the size of the top prize is determined without an upper bound by a certain portion of the total wager collected from selling tickets. As it will be accumulated from one draw to another if winners of it are not found, the total amount of money can occasionally become huge.&lt;/p&gt;

&lt;p&gt;To win that top prize, one needs to have all six numbers matching the drawing result, and that makes the probability drop to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;P(\text{Top prize})= \frac{Count(\text{winning cases})}{Count(\text{all outcomes})} \nonumber \\
&amp;\phantom{P(\text{Top prize})} = \frac{1}{C_{6}^{49}} \nonumber \\
&amp;\phantom{P(\text{Top prize})} = \frac{1}{13983816}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let X be the variable representing the changeble amount of money, then the same calculation of expectation can be applied &lt;strong&gt;as far as the top prize is concerned&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Top prize}(\text{Money returned}) \nonumber \\
&amp; = P(\text{Top prize}) * X\, – 1 * 50 \nonumber \\
&amp; = \frac{X}{13983816} – 1 * 50
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now that X can grow unlimitedly, seeing that expectation go above 0 becomes possible. So how large X has to be? It can be answered by plugging the expression above into the following inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Top prize}(\text{Money returned}) &gt; 0 \nonumber \\
&amp;\Rightarrow \frac{X}{13983816} – 1 * 50 &gt; 0 \nonumber \\
&amp;\Rightarrow X &gt; 50 * 13983816 = 699190800
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;So in this specific case, when the top prize is over the threshold of 699,190,800 NTDs, statistically there would be a certain amount of money gain for every ticket bought. A good timing to try one’s luck.&lt;/p&gt;

&lt;p&gt;Finally, one thing needs to be added is that the analysis discussed above only focuses on winning one particular type of prize. In terms of the &lt;em&gt;true&lt;/em&gt; money returned on average, the calculation has to incorporate all possible prizes. To illustrate that, below is the table containing, alongside the probabilities, the prize detail copied from one of history records of the aforementioned lottery.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Prize&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Money (NTDs)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;400&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.017650403866870102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;400&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.012314235255955885&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1,000&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.0012314235255955885&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2,000&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.000968619724401408&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;11,856&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4.505208020471665e-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;60,026&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.84498995124077e-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;7&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3,065,656&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4.29067430521111e-07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Top&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;468,588,622&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7.151123842018516e-08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;where prize names not in bold indicate the sizes of the corresponding prizes are fixed.&lt;/p&gt;

&lt;p&gt;Interestingly, the top prize only reached 468,588,622 NTDs at that time, but the expected money returned over all prizes already amounted to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
E(\text{Money returned}) = 51.62 - 50 = 1.62
\end{equation}&lt;/script&gt;

&lt;p&gt;That implies it is actually easier to see a lottery game of a positive expectation of money returned than the threshold suggests. While it could be quite bothersome to constantly keep track of the change in the size of all prizes so as to make better estimation, it should probably not be a hindrance to those assiduous and avid players, unlike me.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Bruce Frey, “Statistics Hacks: Tips &amp;amp; Tools for Measuring the World and Beating the Odds”, O’Reilly Media, Inc. ©2006, ISBN:0596101643&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/Blogs/note/WinningLotteries.html</link>
        <guid isPermaLink="true">http://localhost:4000/Blogs/note/WinningLotteries.html</guid>
        
        <category>Fun</category>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Intuitions behind Wasserstein GAN</title>
        <description>&lt;p&gt;Since the appearance of Generative Adversarial Network, aka GAN, a large number of variants have been proposed in attempts to improve the training dynamics, and Wasserstein GAN (WGAN) is one of them. The reason I would like to make a note of it is because in my last project, this type of GAN, specifically, WGAN-GP, was the only one working among several types and architectures I had tried. Honestly, most of time I just pick a model and try it to see if it works; but since WGAN has proven its success in my own case, I think it is time to deepen my understanding about it. There have already been abundant discussions and explanations about WGAN on the Internet, so here I simply write down some informal interpretation of my own.&lt;/p&gt;

&lt;h2 id=&quot;wasserstein-distance&quot;&gt;Wasserstein Distance&lt;/h2&gt;
&lt;p&gt;The central idea of WGAN is to replace means of measuring the similarity between two distributions with the one called Wasserstein Distance (WD), which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned} 
&amp; WD_2[P, Q] = \inf_{\pi} \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi(x, y) dx dy \\
&amp; where
\begin{cases}
P(x) = \int_{y \in \text{support}_Q} \pi(x, y) dy \\
Q(y) = \int_{x \in \text{support}_P} \pi(x, y) dx
\end{cases}
\end{aligned}
\label{eq:WD2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the distance between every pair of $x$ and $y$ in \eqref{eq:WD2} is defined as 2-norm. The use of other metrics is also possible, but for the consistency with later discussion 2-norm is specifically adopted.&lt;/p&gt;

&lt;p&gt;$\pi$ can be viewed as a transport plan which depicts how one can shift around the densities in one distribution so that the resultant distribution matches the other one. This interpretation comes from the fact that $\pi$ satisfies the &lt;em&gt;where&lt;/em&gt; condition: for any point $x’ \in \text{support}_P$, if from every point $y’ \in \text{support}_Q$ the amount of density described by $\pi(x = x’, y = y’)$ is moved to $x’$, then the density of $P(x = x’)$ can be obtained. Hence, by applying such a transport plan the entire distribution $P$ can be constructed from the distribution $Q$, and vice versa.&lt;/p&gt;

&lt;p&gt;There are many possible transport plans to meet the &lt;em&gt;where&lt;/em&gt; condition, so in order to tell which one is preferable, a cost is designed to serve the purpose. From my point of view, the best way to interpret this cost is to follow the idea of &lt;a href=&quot;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&quot;&gt;Earth mover’s distance&lt;/a&gt;: considering there are several piles of dirt at different locations, and certain amounts of dirt from each pile is going to be moved to other locations, how can we quantify the effort required to complete this job? Well, one way to summarise the overall effort, or cost, of such transport is to calculate the sum of each amount of dirt to be moved times the corresponding distance to be travelled. By analogy, $\pi(x, y)$ describes amounts of density to be moved between every pair of points $x$ and $y$; if the definition of distance between two points is set to be 2-norm, then \eqref{eq:WD2} is equivalent to computing such costs across all transport plans and finding the lowest one. As a result, WD between two distributions $P$ and $Q$ indicates the minimum effort demanded to make them identical; and the larger this value is, the more dissimilar these two distributions are.&lt;/p&gt;

&lt;p&gt;It might look like WD is just another criterion for comparing distributions, but it indeed stands out from other distance measures due to its smoothness property. In the paper where WGAN is introduced, the authors used a simple example to illustrate this advantage. Supposed two distributions $P$ and $Q_{\theta}$ are defined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P(x, y) =
\begin{cases}
1,&amp; x = 0, 0 \leq y \leq 1 \\
0,&amp; otherwise
\end{cases} \\
&amp; Q_{\theta}(x, y) =
\begin{cases}
1,&amp; x = \theta, 0 \leq y \leq 1 \\
0,&amp; otherwise
\end{cases} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that $Q_{\theta}$ can be seen as a parametric distribution with $\theta$ being the parameter. In this case, Kullback Leibler divergence (KL), reverse KL, and Jensen-Shannon divergence (JS) can be explicted computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
KL[P \lVert Q_{\theta}] &amp; = \int P(x, y) ln(\frac{P(x, y)}{Q_{\theta}(x, y)}) dx dy \\
&amp; = [\int P(x, y) ln(P(x, y)) dx dy] \\
&amp; \phantom{=} - [\int P(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; = 
\begin{cases}
0,&amp; \theta = 0 \\
\infty,&amp; \theta \neq 0
\end{cases} \\
KL[Q_{\theta} \lVert P] &amp; = \int Q_{\theta}(x, y) ln(\frac{Q_{\theta}(x, y)}{P(x, y)}) dx dy \\
&amp; = [\int Q_{\theta}(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; \phantom{=} - [\int Q_{\theta}(x, y) ln(P(x, y)) dx dy] \\
&amp; = 
\begin{cases}
0,&amp; \theta = 0 \\
\infty,&amp; \theta \neq 0
\end{cases} \\
JS[P \lVert Q_{\theta}] &amp; = 0.5 * KL[P \lVert 0.5 * (P + Q) ] \\
&amp; \phantom{=} + 0.5 * KL[Q \lVert 0.5 * (P + Q)] \\
&amp; = 0.5 * [\int P(x, y) ln(P(x, y)) dx dy] \\ 
&amp; \phantom{=} - 0.5 * [\int P(x, y) ln(0.5 * (P(x, y) + Q_{\theta}(x, y))) dx dy] \\
&amp; \phantom{=} + 0.5 * [\int Q_{\theta}(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; \phantom{=} - 0.5 * [\int Q_{\theta}(x, y) ln(0.5 * (P(x, y) + Q_{\theta}(x, y))) dx dy] \\
&amp; = -0.5 * (-ln(2)) \\
&amp; \phantom{=} - 0.5 * \int P(x, y) * ln(P(x, y) + Q_{\theta}(x, y)) dx dy \\
&amp; \phantom{=} -0.5 * (-ln(2)) \\
&amp; \phantom{=} - 0.5 * \int Q_{\theta}(x, y) * ln((P(x, y) + Q_{\theta}(x, y))) dx dy \\
&amp; =
\begin{cases}
ln(2) - ln(2) = 0, &amp; \theta = 0 \\
ln(2),&amp; \theta \neq 0
\end{cases} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;All three distance measures lead to a discontinuous drop at $\theta = 0$; moreover , KL and reverse KL give a numerically unstable result of $\infty$ when $\theta \neq 0$.&lt;/p&gt;

&lt;p&gt;On the other hand, with the definition of $WD_2$, the minimum cost of adjusting $Q_{\theta}$ to match $P$ can be achived by moving the distribution $Q_{\theta}$ a distance of $\lvert \theta \rvert$ along x-axis towards $0$, and that results in:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;WD_2[P, Q_{\theta}] = \lvert \theta \rvert * 1 = \lvert \theta \rvert&lt;/script&gt;

&lt;p&gt;Clearly, $WD_2$ demonstrates better smoothness and continuousness. Note that this property is rather important when such distance measures are used as objectives of optimisation via gradient descent. For example, in the particular case above the respective gradients of 4 distances w.r.t $\theta$ are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \nabla_{\theta} KL[P \lVert Q_{\theta}] = 
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} KL[Q_{\theta} \lVert P] =
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} JS[P \lVert Q_{\theta}] =
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} WD_2[P, Q_{\theta}]
\begin{cases}
undefined,&amp; \theta = 0\\
-1,&amp; \theta &lt; 0 \\
1,&amp; \theta &gt; 0 
\end{cases}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;It can been seen that optimisation via gradient descent in that case will not work with KL, reverse KL and JS due to lack of gradient information; but it is still possible to reach the optimum $\theta$ through the gradient of $WD_2$.&lt;/p&gt;

&lt;h2 id=&quot;wgan&quot;&gt;WGAN&lt;/h2&gt;
&lt;p&gt;Standard GANs are known for its unstable and difficult training process, and some researches have suggested that it might be due to the objective it uses, which leads to the minimisation of JS distance between model distribution and data distribution. As pointed out in the previous &lt;a href=&quot;#dir0&quot;&gt;section&lt;/a&gt;, JS sometime can be problematic in problems of optimisation, while other measures of good properties, such as WD, might be better choices for construction of GAN. Therefore, the inventors of WGAN tried to bring WD (particularly, $WD_2$) into and GAN architecture, resulting in a new objective (given $Q$ is a model distribution and $P$ is the data distribution which needs to be modelled):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \min_{Q} WD_2[P, Q] = \\
&amp; \phantom{min_Q} \min_{Q} \inf_{\pi} \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi(x, y) dx dy \\
&amp; \phantom{min_Q} where
\begin{cases}
P(x) = \int_{y \in \text{support}_Q} \pi(x, y) dy \\
Q(y) = \int_{x \in \text{support}_P} \pi(x, y) dx
\end{cases}
\end{aligned}
\label{eq:WGANGoal}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;In contrast to implicit minimisation of the JS distance between $P$ and $Q$ in standard GANs, \eqref{eq:WGANGoal} explicitly expresses the goal of reducing $WD_2$. However, that seems to make the objective quite different from the one of standard GANs, since it does not directly correspond to a minimax game anymore; in addition, the calculation of \eqref{eq:WGANGoal} involves finding a $\pi$ which satisfies both infimum and the &lt;em&gt;where&lt;/em&gt; condition, which makes the whole equation looks rather intimidating. Fortunately, it turns out that \eqref{eq:WGANGoal} can be transformed into its dual form according to the theorem of Kantorovich-Rubinstein Duality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
\min_{Q} WD_2[P, Q] = \min_{Q} \sup_{f, \lVert f \rVert_L \le 1} E_{x \sim P}[f(x)] - E_{y \sim Q}[f(y)] 
\label{eq:WGANDualGoal}
\end{equation}&lt;/script&gt;

&lt;p&gt;where $\lVert f \rVert_L$ stands for a Lipschitz constant of $f$ under norm-2 metric. If a neural network of enough capacity with parameter set $\phi$ is used to model $f$ such that supremum can be reached; and samples of $Q$ are generated via mapping each sample $z$ from a known distribution $Z$ with another neural network $g$ parameterized by $\theta$, then \eqref{eq:WGANDualGoal} can be rewritten as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\min_{Q} WD_2[P, Q] = \min_{\theta} \max_{\phi} E_{x \sim P}[f_{\phi}(x)] - E_{z \sim Z}[f_{\phi}(g_\theta(z))] 
\label{eq:WGANPraticalGoal}
\end{equation}&lt;/script&gt;

&lt;p&gt;It can be seen from \eqref{eq:WGANPraticalGoal} that a new minimax game between a gnerator of $g_{\theta}$ and a critic $f_{\phi}$ emerges: the generator is optimised to produce samples from a better Q which reduce $WD_2$, while the job of the critic is to compute the $WD_2$ of the current $Q$ and the data distribution $P$ through maximisation of the objective.&lt;/p&gt;

&lt;p&gt;Arguably, the most important part which allows us to be able to perform minimax optimisation is the conversion from the original problem into its dual form, with the help of Kantorovich-Rubinstein Duality. While one could go through a solid mathematical derivation to explain how this conversion works, here I would simply like to provide some intuitions based on my understanding about this dual form from the aspect of Lipschitz continuity.&lt;/p&gt;

&lt;p&gt;As expounded in &lt;a href=&quot;https://en.wikipedia.org/wiki/Lipschitz_continuity&quot;&gt;here&lt;/a&gt;, Lipschitz continuity is basically a notion to represent how fast a function $f:X \to W$ can change, described by a number $K \ge 0$ called Lipschitz constant which satisfies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; d_W(f(x_1) - f(x_2)) \le K * d_X(x_1, x_2), \\
&amp; \forall x_1, x_2 \in X
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $d_X$ and $d_W$ are two metrics for the space $X$ and $W$ respectively. With that definition, the $\lVert f \rVert_L \le 1$ appearing in \eqref{eq:WGANDualGoal} can be expressed more precisely as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \lVert f(x) - f(y) \rVert_2 = \lvert f(x) - f(y) \vert \le 1 * \lVert x - y \rVert_2 \\
&amp; \implies -\lVert x - y \rVert_2 \le f(x) - f(y) \le \lVert x - y \rVert_2, \\
&amp; \forall x, y \in \text{support}_Q \cup \text{support}_P
\end{aligned}
\label{eq:lipschitz1Norm2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first equality in \eqref{eq:lipschitz1Norm2} comes from the fact that $f$ is a real-valued function.&lt;/p&gt;

&lt;p&gt;Now back to the definition of $WD_2$. Supposing an optimal transport plan $\pi^*$, which is one of the solutions to \eqref{eq:WD2}, is given, then it can be rewritten as (&lt;em&gt;where&lt;/em&gt; condition is omitted):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
WD_2[P, Q] = \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy
\label{eq:WD2OptimalPI}
\end{equation}&lt;/script&gt;

&lt;p&gt;The combination of \eqref{eq:lipschitz1Norm2} and \eqref{eq:WD2OptimalPI} yields the following inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \int_{x \in \text{support}_P, y \in \text{support}_Q} (f(x) - f(y)) \pi^*(x, y) dx dy \nonumber \\
&amp; \le \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy \nonumber \\ 
&amp; \implies \int_{x \in \text{support}_P, y \in \text{support}_Q} (f(x) - f(y)) \pi^*(x, y) dx dy \nonumber \\
&amp; \phantom{\implies} \le WD_2[P, Q] \label{eq:WD2LowerBound}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that \eqref{eq:WD2LowerBound} has suggested when the two sides of the equation are equal: if some function $f^*$ satisfies $f^*(x) - f^*(y) = \lVert x - y \rVert_{2}$ for every pair $x$, $y$ in the transport plan described by $\pi^*$, i.e. $\pi^*(x, y) \ne 0$, then it can be concluded that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \int_{x \in \text{support}_P, y \in \text{support}_Q} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; = \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) \ne 0} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; \phantom{=} + \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) = 0} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; = \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) \ne 0} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy + 0 \\
&amp; = WD_2[P, Q]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $WD_2[P, Q]$ represents the upper bound of \eqref{eq:WD2LowerBound}, and it has been shown that such upper bound can be reached with $f^*$. That implies the following equation holds under the assumption that the parameterised form of $f$, $f_{\phi}$, can express any f whose $\lVert f \rVert_L \le 1$, including $f^*$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; WD_2[P, Q] \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P, y \in \text{support}_Q} (f_{\phi}(x) - f_{\phi}(y)) \pi^*(x, y) dx dy
\end{aligned}
\label{eq:maximisationWRTPhi}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Interestingly, \eqref{eq:maximisationWRTPhi} can be further simplified through the derivation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; WD_2[P, Q] \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P, y \in \text{support}_Q} f_{\phi}(x) * \pi^*(x, y) dx dy \\
&amp; \phantom{= \max_{\phi}} - \int_{x \in \text{support}_P, y \in \text{support}_Q} f_{\phi}(y) * \pi^*(x, y) dx dy \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P} f_{\phi}(x) * P(x) dx \\
&amp; \phantom{= \max_{\phi}} - \int_{y \in \text{support}_Q} f_{\phi}(y) * Q(y) dy \\
&amp; = \max_{\phi} E_{x \sim P}[f_{\phi}(x)] - E_{y \sim Q}[f_{\phi}(y)]
\end{aligned}
\label{eq:WD2DualForm}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out that there is no need to know what the exact $\pi^*$ is in order to compute $WD_2[P, Q]$, and that is why the minimisation of \eqref{eq:WGANGoal} can be achieved by solving \eqref{eq:WGANDualGoal}.&lt;/p&gt;

&lt;h2 id=&quot;geometric-interpretation-of-eqrefeqwd2dualform&quot;&gt;Geometric Interpretation of \eqref{eq:WD2DualForm}&lt;/h2&gt;
&lt;p&gt;Inspired by the idea of high-dimensional data visualisation, I have found it helpful to approach \eqref{eq:WD2DualForm} with this geometric interpretation. Although I have to emphasise that this is only my personal understanding, which is rather informal and might be theoretically inaccurate.&lt;/p&gt;

&lt;p&gt;The essence of Wasserstein Distance is to describe how far two distributions are separate. If a set of samples are used to represent the corresponding distribution from which they are generated, such a distance can also be viewed as the degree of separation between two groups of sample points on average. One way to estimate this average distance is to compute it directly in the space of supports of distributions, as seen in \label{eq:WD2}; the other way is to project samples onto a space of few dimensions, and then do the estimation in that reduced space. \eqref{eq:WD2DualForm}, where $f$ can be viewed as a projection function brining every data point onto the real line, is in a sense analogous to the latter approach.&lt;/p&gt;

&lt;p&gt;The use of projection is rather common in tasks such as high-dimensional data visualisation. The type of projection is specifically selected so that information of interest is retained after projection, and hence the result lie in a more meaningful and representative space. So what information is preserved through the projection introduced in \eqref{eq:WD2DualForm}? Well, note that that projection is described by some function $f^*$ which can maximise \eqref{eq:WD2DualForm} and satisfies ($\pi^*$ is the joint distribution describing the optimal transport plan corresponding to $f^*$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \lVert f^* \rVert_L \le 1 \\
&amp; f^*(x) - f^*(y) = \lVert x - y \rVert_{2}, if \pi^*(x, y) \ne 0  \\
&amp; f^*(x) - f^*(y) \le \lVert x - y \rVert_{2}, if \pi^*(x, y) = 0 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;That suggests that $f^*$ preserves the distances of pairs of points in the optimal transport plan, while distances between other points are allowed to decrease (resulting in information loss to some extent). Since pairs in the plan most likely contain data points from two respective distributions for the purpose of density matching, such preservation of distance information leads to a phenomenon where projected points from one distribution tend to stay the same distances away from those coming from the other distribution; in other words, the projection gives rise to two clusters with similiar degree of separation as the original distributions have, and each cluster corresponds to one distribution, respectively.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/Blogs/assets/images/2019-06-11-WassersteinGAN/GeometricInterpretationOfWD2.png&quot; alt=&quot;An illustration of the projection $f^*$&quot; /&gt;
	&lt;figcaption&gt;An illustration of the projection $f^*$&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Just as what projection benefits in tasks of high-dimensional data visualisation, $f^*$ also gives advantage in terms of the calculation of $WD_2$. It can be seen from \eqref{eq:WD2DualForm} that $WD_2$ becomes the distance between the means of two resulting clusters of projected points. That implies distributions being compared are in a sense better described after the projection, as one single piece of statistics (mean) is representative enough to reflect the relative location of an entire cluster.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lipschitz_continuity&quot;&gt;https://en.wikipedia.org/wiki/Lipschitz_continuity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&quot;&gt;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v70/arjovsky17a.html&quot;&gt;Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Networks. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR, June 2017, pp. 214223.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 11 Jun 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/Blogs/note/WassersteinGAN.html</link>
        <guid isPermaLink="true">http://localhost:4000/Blogs/note/WassersteinGAN.html</guid>
        
        <category>MachineLearning</category>
        
        <category>GAN</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Jekyll Markdown Syntax Reference</title>
        <description>&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[link](url)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/roachhd/779fa77e9b90fe945b0c&quot;&gt;Reference link&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 27 May 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/Blogs/note/JekyllMarkdownSyntaxReference.html</link>
        <guid isPermaLink="true">http://localhost:4000/Blogs/note/JekyllMarkdownSyntaxReference.html</guid>
        
        <category>Jekyll</category>
        
        <category>Markdown</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Why Can Gumbel Distribution Be Used to Sample from Discrete Distributions</title>
        <description>&lt;p&gt;I got to know Gumbel distribution when I surveyed papers for my project extension. In addition to its capability to relax the bottleneck of discrete distributions and allow gradients to pass through, I am impressed by that it can really simplify processes of generating discrete samples. Given a discrete distribution of $\lvert C \rvert$ categories, and the probability of each category expressed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
P(C_j) = \frac{exp(\text{logit}_j)}{\sum_{i=1}^{\lvert C \rvert}exp(\text{logit}_i)}
\end{equation}&lt;/script&gt;

&lt;p&gt;where $C_i$ refers to i-th category; then to generate a sample $Sample_c$ from that distribution, we can make use of samples from standard Gumbel distribution (Gumbel distribution with mean $0$, scale $1$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \label{eq:samplingFromDiscrete}
Sample_c = argmax_{i \in Z, 1 \leq i \leq \lvert C \rvert} (\text{logit}_i + Sample_{g,i})
\end{equation}&lt;/script&gt;

&lt;p&gt;where $Sample_{g,i}$ is a sample drawn independently from standard Gumbel distribution for the i-th category.&lt;/p&gt;

&lt;p&gt;The convenience comes from the fact that sampling from standard Gumbel distribution is rather easy, as it has an invertible, closed-form CDF:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
&amp;p = CDF_{Gumbel}(g) = exp(-exp(-g)) \label{eq:CDFOfGumbel} \\
&amp;g = CDF_{Gumbel}^{-1}(p) = -ln(-ln(p))
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;As a result, we can get $Sample_{g,i}$ by sampling $u_{i}$ from $Uniform[0, 1]$ and computing $Sample_{g,i} = CDF_{Gumbel}^{-1}(u_{i})$.&lt;/p&gt;

&lt;p&gt;I had always taken \eqref{eq:samplingFromDiscrete} for granted until I saw a reference a few days ago, which explains why \eqref{eq:samplingFromDiscrete} is equivalent to sampling from the original discrete distribution. The proof is not too hard to derive with my limited mathematical knowledge, so here I just write it down using my own words to help me better capture the intuition behind.&lt;/p&gt;

&lt;p&gt;Note that samples generated via \eqref{eq:samplingFromDiscrete} also follow a distribution, which describes the probability of $z_i = logit_i + Sample_{g,i}$ being the largest, for each cateogry $i$. If each $Sample_{g,i}$ is replaced with a random variable $G_i$, then it can be thought of as there are $\lvert C \rvert$ independent random variables $Z_i = logit_i + G_i$, each of which follows a Gumbel distribution with mean $logit_i$ and scale $1$. To show what those probabilities really are, we start from assuming that for some category $j$, $Z_j$ is given to be some $z_j$, and write down the conditional probability of $Z_j = z_j$ being the largest as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(Z_j \text{ is the largest} \vert Z_j = z_j, logit_{1 .. \lvert C \rvert}) \nonumber \\
&amp; = \prod_{i = 1, i \neq j}^{\lvert C \rvert}P(logit_i + G_i &lt; z_j) \nonumber \\
&amp; = \prod_{i = 1, i \neq j}^{\lvert C \rvert}P(G_i &lt; z_j - logit_i) \label{eq:conditionalProbabilityOfzjIsTheLargest}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Each term in the product of \eqref{eq:conditionalProbabilityOfzjIsTheLargest} is the cumulative probability of standard Gumbel distribution ranging from $-\inf$ to $z_j - logit_i$, which can be expanded using the CDF from \eqref{eq:CDFOfGumbel}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(Z_j \text{ is the largest} \vert Z_j = z_j, logit_{1 .. \lvert C \rvert}) \nonumber \\
&amp; = \prod_{i = 1, i \neq j}^{\lvert C \rvert}exp(-exp(-(z_j - logit_i)) \nonumber \\
&amp; = exp(\sum_{i = 1, i \neq j}^{\lvert C \rvert}-exp(-z_j + logit_i)) \nonumber \\
&amp; = exp(-exp(-z_j) * \sum_{i = 1, i \neq j}^{\lvert C \rvert}exp(logit_i)) \nonumber \\
&amp; = exp(-exp(-z_j) * (\sum_{i = 1}^{\lvert C \rvert}exp(logit_i) - exp(logit_j))) \nonumber \\
&amp; = exp(-exp(-z_j) * (S - exp(logit_j)))
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $S = \sum_{i = 1}^{\lvert C \rvert}exp(logit_i)$. With that conditional probability, the target distribution can be obtained by Bayes rule and marginalising out $Z_j$, which
is also a Gumbel distribution with mean $logit_j$ and scale $1$ (So the PDF of it is $P(Z_j) = exp(-(Z_j - logit_j + exp(-(Z_j - logit_j))))$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(Z_j \text{ is the largest} \vert logit_{1 .. \lvert C \rvert}) \nonumber \\
&amp; = \int_{-\infty}^{\infty} P(Z_j \text{ is the largest} \vert Z_j, logit_{1 .. \lvert C \rvert}) * P(Z_j) dZ_j \nonumber \\
&amp; = \int_{-\infty}^{\infty} exp(-exp(-Z_j) * (S - exp(logit_j))) * \nonumber \\
&amp; \phantom{= \int_{-\infty}^{\infty}} exp(-(Z_j - logit_j + exp(-(Z_j - logit_j)))) dZ_j \nonumber \\
&amp; = \int_{-\infty}^{\infty} exp(-exp(-Z_j) * S + exp(-Z_j) * exp(logit_j)) * \nonumber \\
&amp; \phantom{= \int_{-\infty}^{\infty}} exp(-(Z_j - logit_j) - exp(-Z_j) * exp(logit_j)) dZ_j \nonumber \\
&amp; = exp(logit_j) * \int_{-\infty}^{\infty} exp(-exp(-Z_j) * S - Z_j) dZ_j \label{eq:integralOverZj}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The integral part still looks intimidating, but it turns out we can introduce a new variable $\hat{Z_j} = Z_j - ln(S)$ to further simplify \eqref{eq:integralOverZj}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; exp(logit_j) * \int_{-\infty}^{\infty} exp(-exp(-Z_j) * S - Z_j) dZ_j \nonumber \\
&amp; = exp(logit_j) * \int_{-\infty}^{\infty} exp(-exp(-\hat{Z_j} - ln(S)) * \nonumber \\
&amp; \phantom{= exp(logit_j) * \int_{-\infty}^{\infty}} S - \hat{Z_j} - ln(S)) d\hat{Z_j} \nonumber \\
&amp; = \frac{exp(logit_j)}{S} * \int_{-\infty}^{\infty} exp(-(exp(-\hat{Z_j}) + \hat{Z_j})) d\hat{Z_j} \label{eq:integralOverZHatj}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the integral in \eqref{eq:integralOverZHatj} is equivalent to integrating the PDF of standard Gumbel distribution over its support, so that integral is just 1, and we arrive at:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(Z_j \text{ is the largest} \vert logit_{1 .. \lvert C \rvert}) = \frac{exp(logit_j)}{S} \nonumber \\
&amp; \phantom{P(Z_j \text{ is the largest} \vert logit_{1 .. \lvert C \rvert})} = \frac{exp(logit_j)}{\sum_{i = 1}^{\lvert C \rvert}exp(logit_i)} \label{eq:finale}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;As a result, the probability of each category being sampled via \eqref{eq:samplingFromDiscrete} is exactly the same amount of probability assigned to each one by the original discrete distribution.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gumbel_distribution&quot;&gt;https://en.wikipedia.org/wiki/Gumbel_distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/&quot;&gt;https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 27 May 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/Blogs/note/Gumbel-Distribution.html</link>
        <guid isPermaLink="true">http://localhost:4000/Blogs/note/Gumbel-Distribution.html</guid>
        
        <category>MachineLearning</category>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
  </channel>
</rss>
