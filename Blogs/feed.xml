<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>/Blogs/</link>
    <atom:link href="/Blogs/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 16 Jan 2022 00:43:07 +0800</pubDate>
    <lastBuildDate>Sun, 16 Jan 2022 00:43:07 +0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Priors That Unlock the Potential of VBVAE</title>
        <description>&lt;p&gt;In my earlier post, a new variant of &lt;strong&gt;V&lt;/strong&gt;ector &lt;strong&gt;Q&lt;/strong&gt;uantised &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder was proposed in an attempt to improve for better gradient estimation and efficient discrete coding. The essential difference is the use of bit-strings rather than integer codes to index vectors in a latent space, and therefore its name &lt;strong&gt;V&lt;/strong&gt;ector &lt;strong&gt;B&lt;/strong&gt;inarised &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder was coined to reflect that distinction. Just as &lt;strong&gt;VQVAE&lt;/strong&gt;, obtaining discrete representation is only the first half of the story, and this post is to complete the full picture by introducing a simple modification to autoregressive priors commonly used with &lt;strong&gt;VQVAE&lt;/strong&gt;, and accordingly establishing similar generative models which takes advantage of &lt;strong&gt;VBVAE&lt;/strong&gt; in handling a huge code space.&lt;/p&gt;

&lt;h2 id=&quot;recap-vbvae&quot;&gt;Recap: &lt;strong&gt;VBVAE&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The idea of &lt;strong&gt;VBVAE&lt;/strong&gt; originated from the fact that the entire quantisation process (vectors $\implies$ codes $\implies$ embeddings) in &lt;strong&gt;VQVAE&lt;/strong&gt; can be viewed as one full Gibbs step at which samples of the highest probabilities are always picked, and the sampling distributions are governed by a &lt;strong&gt;R&lt;/strong&gt;estricted &lt;strong&gt;B&lt;/strong&gt;oltzmann &lt;strong&gt;M&lt;/strong&gt;achines whose conditional of hidden and visible variables given the other are a categorical distribution (under one-hot encoding) and a gaussian with identity covariance, respectively. Therefore by generalising the configuration of the &lt;strong&gt;RBM&lt;/strong&gt; to have the following energy function $E_{\text{bi}}(v, h)$, as well as the resulting conditionals for $h$ (a random column vector of $K$ binary hidden variables) and $v$ (a random column vector of continuous visible variables):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
&amp; E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h \\
\end{aligned} \\
\begin{aligned}
&amp; P_{\text{bi}}(h|v) = \prod_{i=1}^{K} \frac{exp(S(v)[i] * h[i])}{\sum_{\hat{h}[i] \in \{0, 1\}} exp(S(v)[i] * \hat{h}[i])} \\
&amp; \phantom{P_{\text{bi}}(h|v)} \propto \prod_{i=1}^{K} \sigma(S(v)[i])
\end{aligned}\\
\begin{aligned}
&amp; P_{\text{bi}}(v|h) \propto N(\mu + \Sigma^{0.5} C h, \Sigma)
\end{aligned}
\end{cases}
\label{eq:RGBM}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\mu$ being the mean bias vector of $v$; $\Sigma$ being the diagonal covariance matrix of $v$; $C$ being a matrix of which each column corresponds to an embedding vector; $\sigma$ being the entrywise sigmoid operator; and $S(v)$ defined as $v^T \Sigma^{-0.5} C + a^T$ given the bias vector $a$ of $h$. Then based on the entrywise floor operator $\lfloor\;\rfloor$ a new quantisation function outputting discrete codes in the form of bit-strings, denoted by $Qn_{\text{bi}}$, can be attained:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; Qn_{\text{bi}}(v) \nonumber \\
&amp; = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber \\
&amp; = \mu + \Sigma^{0.5} C \lfloor \sigma(S(v)) + 0.5 \rfloor \label{eq:Qn_bi}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For more detailed explanations and discussions please check this &lt;a href=&quot;./VBVAE.html&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sampling-bit-strings-&quot;&gt;Sampling Bit-strings &lt;a name=&quot;sampling_bit-strings&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the most exciting ways of utilising the codes generated from &lt;strong&gt;VQVAE&lt;/strong&gt; is to fit a probabilistic model (such as autoregressive ones) over that discrete space as an external prior, and set up a generative process via first sampling codes from the learned distribution, and then decoding them with the decoder of &lt;strong&gt;VQVAE&lt;/strong&gt; to obtain target samples. While this is still the case for &lt;strong&gt;VBVAE&lt;/strong&gt;, the discrete space is now encoded in terms of bit-strings as opposed to integer codes, which means our probalistic models need to be capable of producing samples in that form. How can we do that?&lt;/p&gt;

&lt;p&gt;Indeed, one can always treat bit-strings as some integer codes in its binary form, then with a quick conversion between these two discrete descriptions the same modelling techniques for codes from &lt;strong&gt;VQVAE&lt;/strong&gt; can immediately apply; but ironically, that seemingly straightforward approach is rather backward from the perspective of &lt;strong&gt;VBVAE&lt;/strong&gt;, as the return of integer codes brings back the limit on the size of the code space that can be manageable, restricting the power of &lt;strong&gt;VBVAE&lt;/strong&gt; in exploiting the expressiveness of a greater number of codes (in the form of bit-string).&lt;/p&gt;

&lt;p&gt;Thus for making the most out of &lt;strong&gt;VBVAE&lt;/strong&gt; and its bit-string coding, an alternative, named “autoregressive &lt;strong&gt;M&lt;/strong&gt;ixtures of &lt;strong&gt;L&lt;/strong&gt;ogistic distributions for &lt;strong&gt;B&lt;/strong&gt;it-strings”, is proposed based on existing autoregressive models over sequences of codes with a few modifications that turn them into ones over sequences of bit-strings. These changes include:&lt;/p&gt;

&lt;h5 id=&quot;normalisation-&quot;&gt;Normalisation &lt;a name=&quot;normalisation&quot;&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;As the state of a bit ($0$ or $1$) in a bit-string produced by &lt;strong&gt;VBVAE&lt;/strong&gt; is determined by how the input reacts to the associated embedding in terms of dot-product, it can be regarded as a quantised intensity in response to the particular embedding as a filter. That perspective allows us to adopt a common practice of normalisation to transform $h$ to $z$ by shifting and rescaling bit values from a discrete space of $\{0, 1\}$ into a continuous one of $[-1, 1]$, which is often used to prepare quantised intensities like RGB colours for a more balanced value distribution across a desired range&lt;/p&gt;

&lt;h5 id=&quot;bit-strings-to-embeddings&quot;&gt;Bit-strings to Embeddings&lt;/h5&gt;
&lt;p&gt;The intesity view given &lt;a href=&quot;#normalisation&quot;&gt;above&lt;/a&gt; also implies that entries of bit-strings can be interpreted as channels related to different abstract features; therefore one or several fully-connected layers taking as input normalised bit-strings would be a reasonable choice for embedding them into a continuous vector space, and by doing so to avoid the need of a code book listing every possible code for looking-up.&lt;/p&gt;

&lt;h5 id=&quot;parameterisation-for-molb&quot;&gt;Parameterisation for &lt;strong&gt;MoLB&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;In order to target bit-strings directly, the last layer of autoregressive models for parameterising categorical distributions is replaced with the one for &lt;strong&gt;MoLB&lt;/strong&gt;, a special case of the discretised logistic mixture likelihood introduced in &lt;em&gt;PixelCNN++&lt;/em&gt; (see &lt;a href=&quot;#included_image_Fig1&quot;&gt;Fig1&lt;/a&gt;). The density function of &lt;strong&gt;MoLB&lt;/strong&gt; with $M$ components given a normalised bit-string $z$ is defined as follows (denoting half of the uniform distance between every two consecutive discrete outcomes by $d$, which in this case is $1$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(z[i]|\pi, l[i], s[i]) = \sum_{j = 0}^{M} \pi_j P_j(z[i]|l_j[i], s_j[i]) \\
&amp; P_j(z[i]|l_j[i], s_j[i]) = \begin{cases}
\sigma(\frac{-1 + d - l_j[i]}{s_j[i]}),\; if\;z[i] &lt; -1 + d \\
1 - \sigma(\frac{1 - d - l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
&amp; \phantom{P_j(z[i]|l_j[i], s_j[i]) } = \begin{cases}
\sigma(\frac{- l_j[i]}{s_j[i]}),\; if\;z[i] &lt; 0 \\
1 - \sigma(\frac{- l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
&amp; \phantom{P_j(z[i]|l_j[i], s_j[i]) } = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;z[i] &lt; 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
&amp; l_j[i] = w_j[i,i] + \sum_{1 \le k &lt; i} w_j[i,k] * z[k]
\end{aligned}
\label{eq:MoLB}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\pi$ is a vector of mixture probabilities for the $M$ component; $l[i]$ and $s[i]$ are the mean and the scale parameter which describes the logistic distribution for $z[i]$; the subscript $j$ is used to indicate which component a parameter is associated with; and $w_j[i,k]$ (subscripted for the mixture component $j$) is the weight of $z[k]$ used in computing $l_j[i]$, the result of a linear combination of all entries preceding $z[i]$, plus a bias term $w_j[i,i]$. Note that if $\pi$, $w$, and $s$ are set to come from the underlying autoregressive model (can be of any type such as &lt;em&gt;PixelCNN&lt;/em&gt; or &lt;em&gt;Transformer&lt;/em&gt;) in an autoregressive &lt;strong&gt;MoLB&lt;/strong&gt; architecture, each bit will also depend on all bit-strings positioned before the one in which that bit resides, and consequently the entire model can been seen acting autoregressive at bit-level.&lt;/p&gt;

&lt;figure&gt;
	
  &lt;a name=&quot;included_image_Fig1&quot;&gt;&lt;/a&gt;
	
	&lt;img src=&quot;/Blogs/assets/images/2022-01-15-AutoregressiveMoLB/comparison_autoregressive_categorical_dist_vs_autoregressive_MoLB.png&quot; alt=&quot;Fig1: The architecture comparison between an autoregressive model parameterised based on categorical distribution (left), and the one based on MoLB (right)&quot; /&gt;
	&lt;figcaption&gt;
	Fig1: The architecture comparison between an autoregressive model parameterised based on categorical distribution (left), and the one based on MoLB (right)
	&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;further-improvement-incorporating-bit-dependency-into-vbvae&quot;&gt;Further Improvement: Incorporating Bit Dependency into VBVAE&lt;/h2&gt;
&lt;p&gt;Maybe clever readers like you have already noticed that there is a discrepancy between the way bits in a bit-string are treated in \eqref{eq:MoLB} and in &lt;strong&gt;VBVAE&lt;/strong&gt;: while autoregressive dependency is assumed present between these bits, they are actually generated by random variables designed to be (conditionally) independent! If our external prior model is to be parameterised in congruence with \eqref{eq:MoLB}, why not making the assumption more valid by constructing &lt;strong&gt;VBVAE&lt;/strong&gt; to generate bits accordingly in the first place?&lt;/p&gt;

&lt;p&gt;To that end, let us first derive $P_j(h[i]|l_j[i], s_j[i])$ from \eqref{eq:MoLB} by assigning the probability of $z[i] &amp;lt; 0$ to $h[i] = 0$ and the one of $z[i] &amp;gt;= 0$ to $h[i] = 1$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P_j(h[i]|l_j[i], s_j[i]) = \begin{cases}
P_j(z[i] &lt; 0|l_j[i], s_j[i]) if\;h[i] = 0 \\
P_j(z[i] &gt;= 0|l_j[i], s_j[i]) if\;h[i] = 1 \\
\end{cases} \\
&amp; \phantom{P_j(h[i]|l_j[i], s_j[i])} = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;h[i] = 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; if\;h[i] = 1
\end{cases} \\
&amp; \phantom{P_j(h[i]|l_j[i], s_j[i])} \propto \sigma(\frac{l_j[i]}{s_j[i]}) \\
&amp; \phantom{P_j(h[i]|l_j[i], s_j[i])} = \sigma(\underbrace{\frac{w_j[i,i]}{s_j[i]}}_{\hat{w}_j[i,i]} + \sum_{1 \le k &lt; i} \underbrace{\frac{w_j[i,k]}{s_j[i]}}_{\hat{w}_j[i,k]} * z[k]) \\
&amp; \phantom{P_j(h[i]|l_j[i], s_j[i])} = \sigma(\hat{w}_j[i,i] + \sum_{1 \le k &lt; i} \hat{w}_j[i,k] * f_{\text{map}}(h[k])) \\
\end{aligned}
\label{eq:conditional_P_j_of_h_i}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which describes a logistic model for $h[i]$ taking as features its preceding bits with the feature mapping $f_{\text{map}}$. Based on the nature of \eqref{eq:conditional_P_j_of_h_i}, an adaptation named &lt;strong&gt;A&lt;/strong&gt;utoregressive &lt;strong&gt;VBVAE&lt;/strong&gt; is introduced, featuring this tweaked energy function $E_{\text{abi}}(v, h)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; E_{\text{abi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h \\
&amp; \phantom{E_{\text{abi}}(v, h) =} - \sum_i h[i] \big( A[i, i] + \sum_{1 \le k &lt; i} A[i,k] f_{\text{map}}(h[k]) \big)
\end{aligned}
\label{eq:E_abi}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $A[i, k]$ is the weight associated with the pair ($h[i]$, $h[k]$), and $A[i, i]$ is substituted for the i-th entry of original bias vector $a$. Since there are no new terms involving $v$ added in \eqref{eq:E_abi}, the resulting conditional for $v$ remains unchanged ($P_{\text{abi}}(v|h) \propto N(\mu + \Sigma^{0.5} C h, \Sigma)$); however, its conditional for $h$ turns into a form similar to what is seen in \eqref{eq:conditional_P_j_of_h_i}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
&amp; P_{\text{abi}}(h|v) = P_{\text{abi}}(h[1]|v) \\
&amp; \phantom{P_{\text{abi}}(h|v) =} * \prod_{i=2} P_{\text{abi}}(h[i]|v, \underbrace{h[1], \dotsc, h[i-1]}_{h_{&lt;i}})
\end{aligned} \\
\begin{aligned}
&amp; P_{\text{abi}}(h[1]|v) \propto exp\Big(h[1] * (v^T \Sigma^{-0.5} C)[1] + h[1] * A[1, 1] \Big) \\
&amp; \phantom{P_{\text{abi}}(h[1]|v)} = exp\Big(h[1] * \big(\underbrace{(v^T \Sigma^{-0.5} C)[1] + A[1, 0]}_{S(v)[1]} \big) \Big) \\
&amp; \phantom{P_{\text{abi}}(h[1]|v)} \propto \sigma\big(S(v)[1]\big)
\end{aligned} \\
\begin{aligned}
&amp; P_{\text{abi}}(h[i]|v, h_{&lt;i}) \\
&amp; \propto exp\Big(h[i] * (v^T \Sigma^{-0.5} C)[i] \\ 
&amp; \phantom{\, \propto exp\Big(} + h[i] * \big( A[i, i] + \sum_{1 \le k &lt; i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
&amp; = exp\Big(h[i] \\ 
&amp; \phantom{= exp\Big(} * \big( \underbrace{(v^T \Sigma^{-0.5} C)[i] + A[i, i]}_{S(v)[i]} + \sum_{1 \le k &lt; i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
&amp; = exp\Big(h[i] * \big(S(v)[i] + \sum_{1 \le k &lt; i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
&amp; \propto \sigma\big(S(v)[i] + \sum_{1 \le k &lt; i} A[i,k] f_{\text{map}}(h[k])\big) 
\end{aligned} \\
\end{cases}
\label{eq:conditional_P_abi_of_h}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which leads to $Qn_{\text{abi}}(v)$, the autoregressive version of \eqref{eq:Qn_bi}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Qn_{\text{abi}}(v) \\
&amp; = \mu \\
&amp; \phantom{=} + \sum_{i=1}^{K} Qn_{\text{abi}}(v)_i \\ 
&amp; = \mu \\
&amp; \phantom{=} + \Sigma^{0.5} C \; one\_hot(1) * \lfloor \sigma\big(\underbrace{\hat{S}(v)[1] + A[1,1]}_{S(v)[1]}\big) + 0.5 \rfloor \\
&amp; \phantom{=} + \dotsb \\
&amp; \phantom{=} + \Sigma^{0.5} C \; one\_hot(K) \\ 
&amp; \phantom{= +} * \lfloor \sigma\big(\underbrace{\hat{S}(v)[k] + A[K,K]}_{S(v)[K]} + \sum_{1 \le k &lt; K} A[K,k] f_{\text{map}}(h[k])\big) + 0.5 \rfloor
\end{aligned}
\label{eq:Qn_abi}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $one\_hot(i)$ stands for a column vector of which the only non-zero entry is the i-th one, which contains a value $1$. It should be pointed out that unlike &lt;strong&gt;VQVAE&lt;/strong&gt; and &lt;strong&gt;VBVAE&lt;/strong&gt;, \eqref{eq:Qn_abi} in &lt;strong&gt;AVBVAE&lt;/strong&gt; does not necessarily produce samples with the highest probability (they are only locally optimal); nonetheless, it is still a valid function describing a deterministic way of quantisation, and thus &lt;strong&gt;AVBVAE&lt;/strong&gt; exhibits the same attracting properties possessed by the two predecessors, such as less noise and a simplified training loss owing to the constant &lt;strong&gt;KL&lt;/strong&gt; term.&lt;/p&gt;

&lt;h2 id=&quot;better-resource-efficiency-parameter-sharing-enabled-by-avbvae&quot;&gt;Better Resource Efficiency: Parameter Sharing Enabled by AVBVAE&lt;/h2&gt;
&lt;p&gt;In addition to modelling consistency, there is a bonus benefit when a &lt;strong&gt;AVBVAE&lt;/strong&gt; and a autoregressive &lt;strong&gt;MoLB&lt;/strong&gt; prior are used together. Recall that all the parameters involved in our quantisation functions are designed to be global; that means the autoregressive relationship in every bit-string generated by a &lt;strong&gt;AVBVAE&lt;/strong&gt;, no matter where they are located in a sequence or what context the are conditioned on, will be overseen by a single dependency model, and hence it should be well sufficient to share autoregressive weights to decrease the number of parameters, and in turns the memory cost of the &lt;strong&gt;MoLB&lt;/strong&gt; part, which would normally takes $O(N * M * K^2)$ for such a prior of $M$ components over sequences of a size ($N$).&lt;/p&gt;

&lt;p&gt;Moreover, this parameter sharing is not just limited within autoregressive &lt;strong&gt;MoLB&lt;/strong&gt; itself; it is also possible to reuse the autoregressive weights from &lt;strong&gt;AVBVAE&lt;/strong&gt; and realise a memory complexity of $O(N * M * K)$! That reduction is based on the observation that the actual code distribution of a &lt;strong&gt;AVBVAE&lt;/strong&gt;, which can be explicitly written out decomposed as follows (let $I(x, y) = 1$ when $x = y$ and $I(x, y) = 0$ when $x \neq y$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P_{\text{abi}}(h) \\
&amp; = \int_v P_{\text{abi}}(\underbrace{\hat{S}(v)}_{=r}) \\ 
&amp; \phantom{=} * P_{\text{abi}}(h[1]| \underbrace{\hat{S}(v)[1]}_{r[1]}) * \dotsb * P_{\text{abi}}(h[K]| \underbrace{\hat{S}(v)[K]}_{r[K]}, h_{&lt;K}) \\
&amp; = \int_r P_{\text{abi}}(r) \\ 
&amp; \phantom{=} * I\Big(h[1], \lfloor \sigma\big(r[1] + A[1,1]\big) + 0.5 \rfloor\Big) \\
&amp; \phantom{=} * \dotsb \\
&amp; \phantom{=} * I\Big(h[K], \\
&amp; \phantom{= * I\Big(} \lfloor \sigma\big(r[K] + A[K,K] + \sum_{1 \le k &lt; K} A[K,k] f_{\text{map}}(h[k])\big) + 0.5 \rfloor\Big) \\
\end{aligned}
\label{P_code_of_h}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;suggests that one only needs to model the contribution of uncertainty from $P(r)$ for the reconstruction of the code distribution; consequently, \eqref{eq:MoLB} might be reformulated as such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(z[i]|\pi, l[i], s[i]) = \sum_{j = 0}^{M} \pi_j P_j(z[i]|l_j[i], s_j[i]) \\
&amp; P_j(z[i]|l_j[i], s_j[i]) = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;z[i] &lt; 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
&amp; l_j[i] = r_j[i] + A[i,i] + \sum_{1 \le k &lt; i} A[i,k] * z[i]
\end{aligned}
\label{eq:MoLB_lite}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;to approximate the distribution of $r$ through a categorical distribution $\pi$ for mixture components, while reuse the same linear model (which is fixed in \eqref{eq:MoLB_lite}) to allow the computation of losses and the learning process to be conducted in terms of $z$/$h$.&lt;/p&gt;

&lt;figure&gt;
	
  &lt;a name=&quot;included_image_Fig2&quot;&gt;&lt;/a&gt;
	
	&lt;img src=&quot;/Blogs/assets/images/2022-01-15-AutoregressiveMoLB/autoregressive_MoLB_lite.png&quot; alt=&quot;Fig2: Autoregressive MoLB lite, a low-memory-consumption version of autoregressive MoLB through parameter sharing&quot; /&gt;
	&lt;figcaption&gt;
	Fig2: Autoregressive MoLB lite, a low-memory-consumption version of autoregressive MoLB through parameter sharing
	&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;the-advantages&quot;&gt;The Advantages&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, the motivation of autoregressive &lt;strong&gt;MoLB&lt;/strong&gt; is to allow the direct distribution modelling over sequences of bits-strings generated by &lt;strong&gt;VBVAE&lt;/strong&gt;, and circumvent the use of code books as well as categorical distributions; hence as the expressiveness of the discrete representation increases, i.e. more bits involved, training an external prior can still be manageble, without being overwhelmed by the demand for memory resources in handling a large code space; moreover, the likely memory requirement of &lt;strong&gt;MoLB&lt;/strong&gt; itself can also be relaxed with the introduction of &lt;strong&gt;AVBVAE&lt;/strong&gt;, where further parameter reduction to a linear complexity with respect to the number of bits is achievable. In other words, when it comes to building a &lt;strong&gt;VBVAE&lt;/strong&gt;/&lt;strong&gt;AVBVAE&lt;/strong&gt;-based generative model, the expressive potential of &lt;strong&gt;VBVAE&lt;/strong&gt;/&lt;strong&gt;AVBVAE&lt;/strong&gt; is better fulfilled via the memory efficiency of autoregressive &lt;strong&gt;MoLB&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Besides, as a special case of the discretised logistic mixture likelihood proposed in &lt;em&gt;PixelCNN++&lt;/em&gt;, &lt;strong&gt;MoLB&lt;/strong&gt; also enjoys the fact that its likelihood function produces denser gradients. That means the learning of a autoregressive &lt;strong&gt;MoLB&lt;/strong&gt; could benefit from stronger training signals, which leads to possibly faster convergence.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00937&quot;&gt;Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete RepresentationLearning”. In:CoRR abs/1711.00937 (2017)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1701.05517&quot;&gt;Tim Salimans et al. “PixelCNN++: Improving the PixelCNN with Discretized Logistic Mix-ture Likelihood and Other Modifications”. In:CoRRabs/1701.05517 (2017)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 15 Jan 2022 00:00:00 +0800</pubDate>
        <link>/Blogs/research/AutoregressiveMoLB.html</link>
        <guid isPermaLink="true">/Blogs/research/AutoregressiveMoLB.html</guid>
        
        <category>Probability</category>
        
        <category>MachineLearning</category>
        
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>Basic Modular Arithmetic and a Simple Application</title>
        <description>&lt;p&gt;Speaking of modular arithmetic, the first impression coming to my mind is the terrifying cryptography, and therefore I have long been shy away from it. Inevitably, this topic showed up again during the course of my recent paper reading, but this time it was presented as a technique to define invertible linear functions for integers $\ge 0$. Despite the same fundamental concept as cryptography, that use of modular arithmetic looks much more approachable, emboldening me to write down this post, which contains the overdue study I have finally conducted about modular arithmetic, illustrated by that interesting application.&lt;/p&gt;

&lt;h2 id=&quot;definition-&quot;&gt;Definition &lt;a name=&quot;definition&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Given an integer $a$, one can specify it in terms of a positive integer $n$ by the equation $a = q_a * n + r$, where $q_a$ and $r$ are uniquely defined under the constraints $0 \le r \lt n$ and $r, q_a \in \mathbb{Z}$; in fact, this is just another way of encoding integer division, with $a$ being the dividend, $n$ being the divisor, $q_a$ being the quotient, and $r$ being the remainder. In modular arithmetic, as remainders are of the most interest, that expression is turned further succinct into $a \equiv r \; (mod \; n)$, which states $a$ and $r$ have the same remainder when divided by $n$ ($n$ now has an alternative name: &lt;em&gt;modulus&lt;/em&gt;), regardless of their quotients. By that definition, if there is another integer $b$ satisfying $b = q_b * n + r$, then it can be also expressed as $a \equiv b \; (mod \; n)$.&lt;/p&gt;

&lt;h2 id=&quot;modular-additionsubtraction-&quot;&gt;Modular Addition/Subtraction &lt;a name=&quot;addition_subtraction&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A natural question arises as to how arithmetical operations can be defined in this context, and let us start with the easy case of addition. Supposing $a \equiv b \; (mod \; n)$ and $c \equiv d \; (mod \; n)$, according to the &lt;a href=&quot;definition&quot;&gt;definition&lt;/a&gt; it follows that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a = q_a * n + r_{ab} \\
&amp; b = q_b * n + r_{ab} \\
&amp; c = q_c * n + r_{cd} \\
&amp; d = q_d * n + r_{cd}
\end{aligned}
\label{eq:a_b_c_d_rewritting}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $0 \le r_{ab}, r_{cd} \lt n$ and $r_{ab}, r_{cd}, q_a, q_b, q_c, q_d \in \mathbb{Z}$. From that the addition of $a$ and $c$ and the addition of $b$ and $d$ can be expressed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a + c = (q_a + q_c) * n + r_{ab} + r_{cd} \\
&amp; b + d = (q_b + q_d) * n + r_{ab} + r_{cd} \\
\end{aligned}
\label{eq:a_plus_c_b_plus_d}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and hence it can be concluded that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a + c \equiv r_{ab} + r_{cd} \; (mod \; n) \\
&amp; b + d \equiv r_{ab} + r_{cd} \; (mod \; n) \\
&amp; \llap{\implies} a + c \equiv b + d \; (mod \; n)
\end{aligned}
\label{modular_addition}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;In connection with substraction, the same argument applies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a - c = (q_a - q_c) * n + r_{ab} - r_{cd} \\
&amp; b - d = (q_b - q_d) * n + r_{ab} - r_{cd} \\
\end{aligned}
\label{a_minus_c_b_minus_d}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a - c \equiv r_{ab} - r_{cd} \; (mod \; n) \\
&amp; b - d \equiv r_{ab} - r_{cd} \; (mod \; n) \\
&amp; \llap{\implies} a - c \equiv b - d \; (mod \; n)
\end{aligned}
\label{eq:modular_subtraction}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;multiplication-&quot;&gt;Multiplication &lt;a name=&quot;multiplication&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Based on \eqref{eq:a_b_c_d_rewritting}, the multiplication of $a$ and $c$, as well as the one of $b$ and $d$, can be expressed as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a * c = (q_a * n + r_{ab}) * (q_c * n + r_{cd}) \\
&amp; \phantom{a * c} = (q_a * n * q_c * n + r_{ab} * q_c * n + \\
&amp; \phantom{a * c = (\;} r_{cd} * q_a * n + r_{ab} * r_{cd})
\end{aligned}
\label{eq:a_times_c}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; b * d = (q_b * n + r_{ab}) * (q_d * n + r_{cd}) \\
&amp; \phantom{b * d} = (q_b * n * q_d * n + r_{ab} * q_d * n + \\
&amp; \phantom{b * d = (\;} r_{cd} * q_b * n + r_{ab} * r_{cd})
\end{aligned}
\label{eq:b_times_d}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and accordingly,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a * c \equiv r_{ab} * r_{cd} \; (mod \; n) \\
&amp; b * d \equiv r_{ab} * r_{cd} \; (mod \; n) \\
&amp; \llap{\implies} a * c \equiv b * d \; (mod \; n)
\end{aligned}
\label{eq:modular_multiplication}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is worth noting an interesting phenomenon about the progression of remainders under multiplication: if a sequence of remainders defined as $0 = r_0 = (0 * a) \% n, r_1 = (1 * a) \% n, \dotsc, r_k = (k * a) \% n, \dotsc$ is written out ($\%$ stands for the operation of division but with the remainder returned instead), at some point it will return back to $0$, and thus forms a loop repeating forever. While the looping behaviour seems obvious due to the fact that it will eventually occur at $k = n$ ($0 = (n * a) \% n$), there is actually a hidden relationship between $a$, $n$, and how remainders loop; in particular, let us consider the maximum length of such a loop. Since for the progression from $0$ to $0$ to happen, there can not be any inner loop inbetween, or otherwise the succession will be stuck in that inner loop. The absence of inner loops suggests all remainders between two $0$s must be distinct, and therefore the length of a loop, starting from the first $0$ until reaching the remainder before the next $0$, can only be up to $n$, which is the number of different values available for remainders.&lt;/p&gt;

&lt;p&gt;Furthermore, when a remainder sequence containing a loop of a maximum length, it can be rearranged like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underbrace{(0 * a) \% n, (1 * a) \% n, \dotsc, ((n-1) * a) \% n}_{loop \; 0}, \\
&amp; \underbrace{(n * a) \% n, ((n + 1) * a) \% n, \dotsc, ((2n-1) * a) \% n}_{loop \; 1}, \\
&amp; \dotsc \\
&amp; \underbrace{(l * n * a) \% n, \dotsc, (((l+1) * n - 1) * a) \% n}_{loop \; l}, \\
&amp; \dotsc

\end{aligned}
\label{eq:remainder_sequence}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $o * a \equiv (l * n + o) * a \; (mod \; n)$ for $0 \le o \lt n$, $0 \le l$ and $o, l \in \mathbb{Z}$. Considering integers from $1$ to $n-1$ as all potential divisors for $n$ which are small than $n$, $loop \; 0$ shows that none of those divisors of $n$ times $a$ can generate a product divisible by $n$, which implies $a$ and $n$ must have no positive common divisors except for the one that is always shared across all numbers: $1$; in other words, their &lt;strong&gt;G&lt;/strong&gt;reatest &lt;strong&gt;C&lt;/strong&gt;ommon &lt;strong&gt;D&lt;/strong&gt;ivisor, $gcd(a, n)$, is equal to 1, and thus they are &lt;em&gt;coprime&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Also it is true the other way around: if $a$ and $n$ are coprime, then the remainder sequence generated by the two numbers will appear similar to \eqref{eq:remainder_sequence}, and hence the length of its $0$-to-$0$ loop reaches the maximum value, $n$.&lt;/p&gt;

&lt;h2 id=&quot;multiplication-by-multiplicative-inverses&quot;&gt;Multiplication by Multiplicative Inverses&lt;/h2&gt;
&lt;p&gt;In case you wonder why this section is not named “division”, the title used here is just another way to refer to the same operation but as the multiplication by the inverse of a number, rather than the division by that number; however, it is invalid to simply take the reciprocal of an integer as its inverse and conduct the modular multiplication with it, since by definition all operands involved need to be integers. To make the notion of “division” applicable in modular arithmetic, mathematicians define the multiplicative inverse of an integer $a$, which is another integer $a’$, in the context of $(mod \; n)$ to be any integer that satisfies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a' * a \equiv 1 \; (mod \; n)
\end{aligned}
\label{eq:inverse_definition}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;so that one can perform calculations like $a’ * a * c \equiv 1 * c \equiv c \; (mod \; n)$, just as though dividing $a * c$ by $a$ (when $a \ne 0$) to obtain $c$.&lt;/p&gt;

&lt;p&gt;While \eqref{eq:inverse_definition} successfully translates multiplicative inverses and allows for the realisation of “modular division”, sadly $a’$ does not always exist for every $a$ and $n$ pair unless a certain condition is met. Recall the &lt;a href=&quot;multiplication&quot;&gt;discussion&lt;/a&gt; about the link between $a$ and $n$ being coprime, and the corresponding remainder sequence containing a longest loop of a length $n$; as that loop will go through every possible remainder under the modulus $n$, there must exist entries of $a$ multiplied by some respective integer in that sequence at which the resulting remainder is $1$; as a result, the coprime relationship of $a$ and $n$ is a sufficient condition for $a’$ to exist.&lt;/p&gt;

&lt;p&gt;In addition, it is also a necessary condition: as per \eqref{eq:inverse_definition} $a’$ can not be zero; so if $a’$ exists and $a’ \gt 0$, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underbrace{0 * a'}_{k_0} * a \equiv 0 \; (mod \; n) \\
&amp; \underbrace{1 * a'}_{k_1} * a \equiv 1 \; (mod \; n) \\
&amp; \underbrace{2 * a'}_{k_2} * a \equiv 2 \; (mod \; n) \\
&amp; \vdots \\
&amp; \underbrace{(n-1) * a'}_{k_{n-1}} * a \equiv n-1 \; (mod \; n)
\end{aligned}
\label{eq:derivation_from_positive_a_prime}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;On the other hand, when $a’ \lt 0$ we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underbrace{0 * a'}_{k_0} * a \equiv 0 \; (mod \; n) \\
&amp; \underbrace{-1 * a'}_{k_1} * a \equiv -1 \equiv n-1 \; (mod \; n) \\
&amp; \underbrace{-2 * a'}_{k_2} * a \equiv -2 \equiv n-2 \; (mod \; n) \\
&amp; \vdots \\
&amp; \underbrace{-(n-1) * a'}_{k_{n-1}} * a \equiv -n+1 \equiv 1 \; (mod \; n)
\end{aligned}
\label{eq:derivation_from_negative_a_prime}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Either case suggests every possible remainder given the modulus $n$ can be produced via the modular multiplication of $a$ by some non-negative integer $k_i$, and therefore they all appear in the remainder sequence described in the &lt;a href=&quot;multiplication&quot;&gt;last section&lt;/a&gt;; since that sequence is merely comprised of a continuously repeating remainder loop, the appearance of these remaiders implies they must be the members of the loop. Consequently, the existance of $a’$ leads to a remainder loop of a length $n$, and as shown &lt;a href=&quot;multiplication&quot;&gt;previously&lt;/a&gt;, if the length of such a remainder loop is $n$ (the maximum length for the modulus $n$), $a$ and $n$ are coprime.&lt;/p&gt;

&lt;p&gt;On a side note, there are two relating facts to \eqref{eq:inverse_definition} and the coprime condition: firstly, because $a$ can be viewed as the multiplicative inverse of $a’$, the argument above also applies to $a’$, which means for any $(a, a’)$ pair satisfying \eqref{eq:inverse_definition}, both of them are coprime with $n$ at the same time; secondly, since \eqref{eq:inverse_definition} can be equivalently expressed as $a’ * a = q * n + 1$ for some $q \in \mathbb{Z}$, $a$ coprime with $n$ indicates one can rewrite the equation into the form of Bézout’s identity $a’ * a - q * n = gcd(a, n) = 1$, and obtain $a’$ via the efficient &lt;a href=&quot;https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm&quot;&gt;&lt;em&gt;extended euclidean algorithm&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;invertible-linear-functions-for-integers-ge-0&quot;&gt;Invertible Linear Functions for Integers $\ge 0$&lt;/h2&gt;
&lt;p&gt;In the general case of real numbers, a linear function is any one of the form $y = f(x) = a*x + b$ with $a, b, x, y \in \mathbb{R}$. Its invertibility is guaranteed as long as $a \ne 0$, and its inverse function can be directly read out as $x = f’(y) = \frac{y - b}{a}$. Easy, right? But how about when $a, b, x, y$ are constrained to be intergers $\ge 0$? Is it possible to define similar invertible functions for that particular scenario?&lt;/p&gt;

&lt;p&gt;It tures out through modular arithmetic it is! Let us assume our integers will not exceed an upperbound integer $n \gt 0$, i.e. $0 \le a, b, x, y \lt n$; so to define modular arithmetic in this case it is natural to pick $n$ as the modulus, and the following equations are immediately true based on the &lt;a href=&quot;addition_subtraction&quot;&gt;discussion&lt;/a&gt; above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
y \equiv x + b \; (mod \; n) \\
y - b \equiv x + b - b \equiv x \; (mod \; n) 
\end{cases}
\label{eq:invertible_integer_addition_subtraction}
\end{equation}&lt;/script&gt;

&lt;p&gt;Note that as $0 \le b \lt n$, there are only $n$ possible ways to shift the value of $x$ and $y$, each of which in turn uniquely shift their corresponding remainder to one of the $n$ different remainder choices; so with the range limit $0 \le x, y \lt n$, the equations in \eqref{eq:invertible_integer_addition_subtraction} are in effect two one-to-one remainder mappings, and inverse to each other.&lt;/p&gt;

&lt;p&gt;Next, like the counterpart of real numbers, an extra constraint for $a$ is imposed to make sure its involvement still renders the resulting function invertible. That constraint, as you might have imagined, is that $a$ and $n$ also needs to be coprime, which allows us to exploit the multiplicative inverse of $a$, $a’$, for mapping back and forth between $x$ and $y$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
y \equiv a * x \; (mod \; n) \\
a' * y \equiv a' * a * x \equiv 1 * x \equiv x \; (mod \; n)
\end{cases}
\label{eq:invertible_integer_multiplication_division}
\end{equation}&lt;/script&gt;

&lt;p&gt;with $0 \le a’ \lt n$ to meet the range limit. Due to $0 \le x, y \lt n$, each result of $a * x \; (mod \; n)$ and $a’ * y \; (mod \; n)$ given different $x$ and $y$ coincides with a unique remainder in the $loop \; 0$ of their respective remainder sequence as \eqref{eq:remainder_sequence}; hence \eqref{eq:invertible_integer_multiplication_division} describes again two one-to-one remainder mappings with one being the inverse of the other.&lt;/p&gt;

&lt;p&gt;Lastly, putting all pieces together, the form of the invertible linear functions for integers $\ge 0$ defined in the context of modular arithmetic appear just as the one for real numbers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
y \equiv a * x + b \; (mod \; n) \\
x \equiv a' * (y - b) \; (mod \; n)
\end{cases}
\end{equation}&lt;/script&gt;

&lt;p&gt;where $0 \le a, a’, b, x, y \lt n$; $a, a’, b, x, y, n \in \mathbb{Z}$; and $a’ * a \equiv 1 \; (mod \; n)$ ($a$ and $a’$ are both coprime with $n$)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1905.10347&quot;&gt;Dustin Tran et al. “Discrete Flows: Invertible Generative Models of Discrete Data”. In:CoRRabs/1905.10347 (2019)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Modular_arithmetic&quot;&gt;https://en.wikipedia.org/wiki/Modular_arithmetic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ericlippert.com/2013/11/12/math-from-scratch-part-thirteen-multiplicative-inverses/&quot;&gt;https://ericlippert.com/2013/11/12/math-from-scratch-part-thirteen-multiplicative-inverses/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 28 Sep 2021 00:00:00 +0800</pubDate>
        <link>/Blogs/note/Modular-Arithmetics.html</link>
        <guid isPermaLink="true">/Blogs/note/Modular-Arithmetics.html</guid>
        
        <category>NumberTheory</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>A Dissection of Attention: Kernel Interpretation</title>
        <description>&lt;p&gt;In the field of machine learning, one of the most influential breakthroughs would be no doubt the advent of attention, and it has become a common technique applied in many different types of learning problems. Such prevalence, however, somehow had left me with the impression that it had been just a handy trick to conduct a data-dependent and normalised linear combination until recently, when I once again stood corrected. Brilliant researchers has now demonstrated how attention could be examined through the lens of kernels, which opens up the possibility for computing the attention weights in a much more efficient way. Surprisingly, its derivation only requires the basic understanding of the conventional &lt;em&gt;kernel trick&lt;/em&gt;, and hence this resulting post contains my &lt;del&gt;successful&lt;/del&gt; attempt to go through that idea with my rudimentary knowledge about kernels.&lt;/p&gt;

&lt;h2 id=&quot;what-are-kernels-&quot;&gt;What Are Kernels? &lt;a name=&quot;what_are_kernels&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Since the new interpretation is from the perspective of kernels, please bear with me digressing to that question for a moment, and here is my rather simplified and informal answer: an alternative way to compute the dot product of any two vectors under the associated feature mapping. For example, given a mapping function $\phi:\mathbb{R}^m \to \mathbb{R}^n$, for any two column vectors $x, t \in \mathbb{R}^m$ the kernel $Ker_{\phi}$ (corresponding to $\phi$) will satisfy ($\odot$ stands for the dot product):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Ker_{\phi}(x, t) = Ker_{\phi}(t, x) = \phi(x) \odot \phi(t)
\end{aligned}
\label{eq:kernel_definition}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that $Ker_{\phi}$ is defined as a function taking $x$ and $t$ as its inputs. It suggests that if the evaluation of $Ker_{\phi}$ does not involve the associated feature mapping, then one can obtain the dot product of $\phi(x)$ and $\phi(t)$ without needing to know the explicit forms of those vectors under the mapping, and that is the most common use case when it comes to the application of the concept of kernels. Why? Imagine that $\phi$ is a complicated function which produces vectors of large dimensionality ($n \gg m$); a dot product in the output space of such a $\phi$ requires potentially high complexity of $O(n)$, whereas a cleverly designed $Ker_{\phi}$ might only take the complexity as small as $O(m)$. That is to say, even with a very expressive $\phi$ (which incurs the surge in the dimensionality of its output space) the complexity of computing the dot products can still be kept at a manageable level, via the corresponding kernel $Ker_{\phi}$.&lt;/p&gt;

&lt;p&gt;To demonstrate such exploitation, consider the following kernel:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Ker_{\phi_{exp}}(x, t) = exp(x \odot t) = \prod_{i=0}^{m-1} exp(x_i * t_i)
\end{aligned}
\label{eq:exponetial_kernel}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Each term $exp(x_i * t_i)$ can be expressed in terms of &lt;em&gt;Taylor series&lt;/em&gt; at $x_i * t_i = 0$, which yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; exp(x_i * t_i) \\
&amp; = 1 + \frac{x_i * t_i}{1!} + \frac{(x_i * t_i)^2}{2!} + \frac{(x_i * t_i)^3}{3!} + \dotsb \\
&amp; = 1 + \frac{x_i}{\sqrt{1!}} * \frac{t_i}{\sqrt{1!}} + \frac{x_i^2}{\sqrt{2!}} * \frac{t_i^2}{\sqrt{2!}} + \frac{x_i^3}{\sqrt{3!}} * \frac{t_i^3}{\sqrt{3!}} + \dotsb \\
&amp; = [1, \frac{x_i}{\sqrt{1!}}, \frac{x_i^2}{\sqrt{2!}}, \frac{x_i^3}{\sqrt{3!}}, \dotsc] \odot [1, \frac{t_i}{\sqrt{1!}}, \frac{t_i^2}{\sqrt{2!}}, \frac{t_i^3}{\sqrt{3!}}, \dotsc] \\
&amp; = {\phi_{exp}}_{i}(x) \odot {\phi_{exp}}_{i}(t)
\end{aligned}
\label{eq:exponetial_kernel_term_i_in_taylor_series}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;That indicates for $m = 1$, $\phi_{exp}$ in effect produces vectors of infinite dimensions; furthermore, following the same principle of \eqref{eq:exponetial_kernel_term_i_in_taylor_series}, it can be shown that when $m \ge 2$ the dimensionality of the output space can only become “more infinite” (if infinity can really be compared). For instance, below is the case where $m = 2$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \prod_{i=0}^{1} exp(x_i * t_i) \\
&amp; = exp(x_0 * t_0) * exp(x_1 * t_1) \\
&amp; = (1 + \frac{x_0}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}} + \frac{x_0^2}{\sqrt{2!}} * \frac{t_0^2}{\sqrt{2!}} + \frac{x_0^3}{\sqrt{3!}} * \frac{t_0^3}{\sqrt{3!}} + \dotsb) * \\
&amp; \phantom{=} (1 + \frac{x_1}{\sqrt{1!}} * \frac{t_1}{\sqrt{1!}} + \frac{x_1^2}{\sqrt{2!}} * \frac{t_1^2}{\sqrt{2!}} + \frac{x_1^3}{\sqrt{3!}} * \frac{t_1^3}{\sqrt{3!}} + \dotsb) \\
&amp; = (1 + \frac{x_0}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}} + \frac{x_0^2}{\sqrt{2!}} * \frac{t_0^2}{\sqrt{2!}} + \dotsb) + \\
&amp; \phantom{=} (\frac{x_1}{\sqrt{1!}} * \frac{t_1}{\sqrt{1!}} + \frac{x_0}{\sqrt{1!}}\frac{x_1}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}}\frac{t_1}{\sqrt{1!}} + \dotsb) + \\
&amp; \phantom{=} (\frac{x_1^2}{\sqrt{2!}} * \frac{t_1^2}{\sqrt{2!}} + \frac{x_0}{\sqrt{1!}}\frac{x_1^2}{\sqrt{2!}} * \frac{t_0}{\sqrt{1!}}\frac{t_1^2}{\sqrt{2!}} + \dotsb) + \dotsb \\
&amp; = [1, \frac{x_0}{\sqrt{1!}}, \frac{x_0^2}{\sqrt{2!}}, \dotsc, \frac{x_1}{\sqrt{1!}}, \frac{x_0}{\sqrt{1!}}\frac{x_1}{\sqrt{1!}}, \frac{x_0}{\sqrt{1!}}\frac{x_1^2}{\sqrt{2!}}, \dotsc] \odot \\
&amp; \phantom{=} [1, \frac{t_0}{\sqrt{1!}}, \frac{t_0^2}{\sqrt{2!}}, \dotsc, \frac{t_1}{\sqrt{1!}}, \frac{t_0}{\sqrt{1!}}\frac{t_1}{\sqrt{1!}}, \frac{t_0}{\sqrt{1!}}\frac{t_1^2}{\sqrt{2!}}, \dotsc] \\
&amp; = {\phi_{exp}}_{0,1}(x) \odot {\phi_{exp}}_{0,1}(t)
\end{aligned}
\label{eq:exponetial_kernel_term_0_1_in_taylor_series}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;If \eqref{eq:exponetial_kernel_term_0_1_in_taylor_series} is evident enough for convincing you that $\phi_{exp}(x)$/$\phi_{exp}(t)$ will be infinite-dimensional, then from the conclusion it follows that conducting the direct dot product of them is literally impossible; nonetheless, its kernel alternative $Ker_{\phi_{exp}}(x, t)$ can still output the evaluated result in a computationally feasible way.&lt;/p&gt;

&lt;h2 id=&quot;the-connection-to-attention-&quot;&gt;The Connection to Attention &lt;a name=&quot;the_connection_to_attention&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The attention given a query $q \in \mathbb{R}^m$ over a set of $L$ key-value pairs $\{(k_j, v_j)| j \in \mathbb{Z}, 0 \le j \le L-1, k_j \in \mathbb{R}^m, v_j \in \mathbb{R}^o\}$ is commonly defined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a = \sum_{j = 0}^{L-1} \frac{ exp(\frac{q \odot k_j}{\sqrt{m}})}{\sum_{l = 0}^{L-1} exp(\frac{q \odot k_l}{\sqrt{m}})} v_j \\
&amp; \phantom{a} = \sum_{j = 0}^{L-1} \frac{ exp(\overbrace{\frac{q}{\sqrt[4]{m}}}^{\hat{q}} \odot \overbrace{\frac{k_j}{\sqrt[4]{m}}}^{\hat{k}_j})}{\sum_{r = 0}^{L-1} exp(\frac{q}{\sqrt[4]{m}} \odot \frac{k_r}{\sqrt[4]{m}})} v_j \\ 
&amp; \phantom{a} = \sum_{j = 0}^{L-1} \frac{ exp(\hat{q} \odot \hat{k}_j)}{\sum_{r = 0}^{L-1} exp(\hat{q} \odot \hat{k}_r)} v_j 
\end{aligned}
\label{eq:basic_attention}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that those terms involving exponetial functions just match the definition of \eqref{eq:exponetial_kernel} discussed in the last &lt;a href=&quot;#what_are_kernels&quot;&gt;section&lt;/a&gt;; therefore \eqref{eq:basic_attention} is equivalent to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a = \sum_{j = 0}^{L-1} \frac{ Ker_{\phi_{exp}}(\hat{q}, \hat{k}_j)}{\sum_{r = 0}^{L-1} Ker_{\phi_{exp}}(\hat{q}, \hat{k}_r)} v_j
\end{aligned}
\label{eq:exponential_kernel_attention}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;\eqref{eq:exponential_kernel_attention} shows that the attention weights can be viewed as the results returned by the kernel function $Ker_{\phi_{exp}}$ with normalisation; and since there are no constraints on what types of kernels can be used except for having non-negative outputs (attention weights need to be $\ge 0$), it is possible to derive the formula based on different kernels, which yield this general form of attention:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; a_{\phi_p} = \sum_{j = 0}^{L-1} \frac{ Ker_{\phi_p}(\hat{q}, \hat{k}_j)}{\sum_{r = 0}^{L-1} Ker_{\phi_p}(\hat{q}, \hat{k}_r)} v_j  \\
&amp; \phantom{a_{\phi_p}} = \sum_{j = 0}^{L-1} \frac{\phi_p(\hat{q}) \odot \phi_p(\hat{k}_j)}{\sum_{r = 0}^{L-1} \phi_p(\hat{q}) \odot \phi_p(\hat{k}_r)} v_j 
\end{aligned}
\label{eq:kernel_attention}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\phi_p:\mathbb{R}^m \to \mathbb{R}^n$ being some mapping function which satisfies $\phi_p(x) \odot \phi_p(t) \ge 0$ for any $x$, $t \in \mathbb{R}^m$.&lt;/p&gt;

&lt;h2 id=&quot;fast-attention-&quot;&gt;Fast Attention &lt;a name=&quot;fast_attention&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;One of the main advantages of the attention mechanism is its capability for gathering distant information as every part of data being attended to can be accessed directly; however, that property is also a curse due to the fact that its complexity is largely dictated by the range of attention, and it could be rather costly when modelling data containing long-range dependencies. To see that, let us extend the attention definition of \eqref{eq:basic_attention} to the multi-query version written in matrix form (assuming there are $L$ queries):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; A = \underbrace{\Big(\frac{exp(\frac{Q K^T}{\sqrt{m}})}{\big( exp(\frac{Q K^T}{\sqrt{m}})1_L \big) }\Big)}_{= W} V
\end{aligned}
\label{eq:basic_attention_in_matrix_form}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $1_L$ is the all-ones column vector of $L$ rows; $diag(.)$ is the diagonal matrix of which the diagonal is equal to its input vector; both the $exp(.)$ and the division are applied elementwise and broadcast if necessary; $Q \in \mathbb{R}^{L*m}$, $K \in \mathbb{R}^{L*m}$, and $V \in \mathbb{R}^{L*o}$ stand for the matrices with each row representing a query, key, and value vector respectively. The analysis of \eqref{eq:basic_attention_in_matrix_form} suggests that the time complexity for obtaining $A$ is dominated by the matrix multiplications of $Q K^T$ and $W V$, which take $O(L^2m+L^2o) = O(L^2*max(m, o))$; while the storage of $Q$, $K$, $V$, and $W$ comprises the main part of the memory consumption, resulting in the space complexity of $O(L*max(m, L, o))$. Consequently, when the dimesion of the size $L$ becomes the largest one (as is usually the case), both of the time and space complexity would be quadratic with respect to $L$, and an increase in $L$ could easily render the computation of \eqref{eq:basic_attention_in_matrix_form} prohibitively expensive.&lt;/p&gt;

&lt;p&gt;So how can we alleviate that cost requirement? Well, having a close look at \eqref{eq:basic_attention_in_matrix_form}, you might notice that the calculation of $Q K^T$ consitutes the major source of the computational demand; therefore its circumvention would be the key to reduce the complexities, and that is where the kernel connection comes into play. Recall the general form of attention under the kernel interpretation introduced in the last &lt;a href=&quot;#the_connection_to_attention&quot;&gt;section&lt;/a&gt;; approaching the multi-query attention from that view, in particular from \eqref{eq:kernel_attention}, allows us to define the following formulation named fast attention:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; A_{\phi_p} = \Big(\frac{(\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T )}{(\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T 1_L )} \Big) V \\
&amp; \phantom{A_{\phi_p}} = \frac{\hat{Q}_{\phi_p} \overbrace{\hat{K}_{\phi_p}^T V}^{= U}}{\hat{Q}_{\phi_p} \underbrace{\hat{K}_{\phi_p}^T 1_L}_{= S}} \\
&amp; \phantom{A_{\phi_p}} = \frac{\hat{Q}_{\phi_p} U}{\hat{Q}_{\phi_p} S } \\
\end{aligned}
\label{eq:dot_attention_in_matrix_form}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the i-th row of $\hat{Q}_{\phi_p}$ (denoted by $(\hat{Q}_{\phi_p})_i$) being the output of $\phi_p$ given as input the i-th row of $\frac{Q}{\sqrt[4]{m}}$, and the j-th row of $\hat{K}_{\phi_p}$ (denoted by $(\hat{K}_{\phi_p})_j$) being the output of $\phi_p$ given as input the j-th row of $\frac{K}{\sqrt[4]{m}}$, for every row $i$ in $Q$ and every row $j$ in $K$, respectively. Note that how the calculation of the product $\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T$ (analogous to the term $Q K^T$ in \eqref{eq:basic_attention_in_matrix_form}) is avoided by the intermediate results of $U$ and $S$ according to the associative property of matrix multiplication enabled by the feature mapping $\phi_p$, and that is where the “fastness” comes from: the computation for $U$ and $S$ is only of a time complexity of $O(Lno+Ln) = O(Lno)$, while their storage requires merely $O(no+n) = O(no)$; furthermore, the evaluation of $A_{\phi_p}$ through $U$ and $S$ is also cheap, taking $O(Lno)$ in time and $O(Lo)$ in space. As a result, \eqref{eq:dot_attention_in_matrix_form} offers a potentially efficient way for conducting attention with the time complexity of $O(Lno)$ and the space complexity of $O(L*max(n, o))$, both of which are just linear with respect to $L$.&lt;/p&gt;

&lt;p&gt;But, if you have noticed the word “potentially”, there is a caveat. The entire discussion about the improvement in efficiency is premised on that $L$ remains the dominant factor in the complexity while the overheads of $\phi_p(.)$ are negligible, which implies \eqref{eq:dot_attention_in_matrix_form} can only be faster if&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The dimensionality of the output space of $\phi_p(.)$ is less than the number of queries/key-value pairs, i.e. $n \lt L$.&lt;/li&gt;
  &lt;li&gt;The cost of the evaluation of $\phi_p(\hat{q})$ and that of $\phi_p(\hat{k})$ are ralatively small compared with the one of \eqref{eq:dot_attention_in_matrix_form}.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore the choice of the kernel (and thus its associated $\phi_p$) for producing attention weights really determines the performance of \eqref{eq:dot_attention_in_matrix_form}, and it is possible to end up being (desperately) worse than the original form! For example, the fast attention equivalent for \eqref{eq:basic_attention_in_matrix_form}, which indicates the use of $Ker_{\phi_{exp}}(., .)$, is $A_{\phi_{exp}}$; nonetheless, to evaluate it is completely impractical due to the infinite-dimensional output space of $\phi_{exp}(.)$ (that is, $n = \infty$), as pointed out in the previous &lt;a href=&quot;#what_are_kernels&quot;&gt;section&lt;/a&gt;. In other words, if $A_{\phi_{exp}}$ were to be evaluated, it would require infinite time and space!&lt;/p&gt;

&lt;h2 id=&quot;positive-random-features&quot;&gt;Positive Random Features&lt;/h2&gt;
&lt;p&gt;It is really disappointing that the fast attention can not be applied to speed up \eqref{eq:basic_attention_in_matrix_form}, and the main obstacle which blows up the complexity is the presence of $\phi_{exp}(.)$. Hence just as what has been done in \eqref{eq:dot_attention_in_matrix_form}, researchers sought to circumvent again that computationally demanding part, and in doing so they have found that actually $Ker_{\phi_{exp}}(\hat{q}, \hat{k})$ can be expressed in terms of the expectation over the standard normal distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Ker_{\phi_{exp}}(\hat{q}, \hat{k}) = exp(\hat{q} \odot \hat{k}) \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) \\
&amp; exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) = exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) * \\
&amp; \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2} =} \underbrace{(2\pi)^{-\frac{m}{2}} \int exp(-\frac{\|z - (\hat{q} + \hat{k})\|_2^2}{2}) dz}_{= 1} \\
&amp; \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2})} = (2\pi)^{-\frac{m}{2}} * \\
&amp; \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) =} \int exp(-\|z\|_2^2) exp(z \odot (\hat{q} + \hat{k})) dz \\
&amp; \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2})} = E_{z\sim Normal(0, I)}[exp(z \odot (\hat{q} + \hat{k}))] \\
&amp; \implies Ker_{\phi_{exp}}(\hat{q}, \hat{k}) = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\
&amp; \phantom{\implies Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} E_{z\sim Normal(0, I)}[exp(z \odot (\hat{q} + \hat{k}))]
\end{aligned}
\label{eq:kernel_exp_with_expectation}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then estimating the expectation in \eqref{eq:kernel_exp_with_expectation} via the &lt;em&gt;Monte Carlo method&lt;/em&gt; with random samples $Z = \{z_0,\dotsc, z_i,\dotsc,z_{N-1} | z_i \sim Normal(0, I)\}$ yields:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Ker_{\phi_{exp}}(\hat{q}, \hat{k}) \approx exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\ 
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \frac{1}{N} \sum_{i=0}^{N-1}exp(z_i \odot (\hat{q} + \hat{k})) \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \frac{1}{N} \sum_{i=0}^{N-1}exp(z_i \odot \hat{q}) exp(z_i \odot \hat{k}) \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = \underbrace{\begin{aligned} &amp; \Big( \frac{exp(-\frac{\|\hat{q}\|_2^2}{2})}{\sqrt{N}} * \\
																															&amp; \phantom{\Big(} [exp(z_0 \odot \hat{q}),\dotsc, exp(z_{N-1} \odot \hat{q})] \Big) \end{aligned}}_{= \phi_{Z}(\hat{q})} \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \odot \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \underbrace{\begin{aligned} &amp; \Big( \frac{exp(-\frac{\|\hat{k}\|_2^2}{2})}{\sqrt{N}} * \\
																															&amp; \phantom{\Big(} [exp(z_0 \odot \hat{k}),\dotsc, exp(z_{N-1} \odot \hat{k})] \Big) \end{aligned}}_{= \phi_{Z}(\hat{k})} \\
&amp; \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = \phi_{Z}(\hat{q}) \odot \phi_{Z}(\hat{k})
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and there you go! a new feature mapping function $\phi_{Z}(.)$ for $Ker_{\phi_{exp}}(\hat{q}, \hat{k})$, which is one realisation of the notion called &lt;em&gt;Positive Random Features&lt;/em&gt;, is attained. As the name suggests, all of the output features of $\phi_{Z}(.)$ are not only positive (just as desired for calculating attention weights), but also derived from random samples; in particular, the dimensionality of its output space is also decided by, or to be specific, equal to, the number of the samples utilised. That indicates through the trade-off between the approximation accuracy and the sample size, conducting the fast attention with $\phi_{Z}(.)$ can always be made affordable, and therefore $\phi_{Z}(.)$ serves as a viable as well as flexible alternative to $\phi_{exp}(.)$ when it comes to performing the fast attention version of \eqref{eq:basic_attention_in_matrix_form}.&lt;/p&gt;

&lt;p&gt;It should also be mentioned that the nature of the uncertainty of &lt;em&gt;Positive Random Features&lt;/em&gt; would inevitably bring about noise, which in turn creates the variance in the attention results and has shown to be harmful to training; so besides having an adequate number of samples, in the paper where this method was proposed other techniques such as &lt;em&gt;the orthogonalisation of random samples&lt;/em&gt; or &lt;em&gt;periodic resampling&lt;/em&gt; were also recommended to mitigate that negative impact. Interested readers are referred to &lt;a href=&quot;https://arxiv.org/abs/2009.14794&quot;&gt;the original paper&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;extending-to-autoregressive-attention&quot;&gt;Extending to Autoregressive Attention&lt;/h2&gt;

&lt;p&gt;This is also termed &lt;em&gt;unidirectional attention&lt;/em&gt;, where each query is only allowed to attend to the previous information (excluding/including the current one) based on a given ordering. For instance, \eqref{eq:basic_attention_in_matrix_form} can be turned autoregressive by plugging in the $LT(.)$ operator which returns the lower triangular version of its input matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; A^{ar} = \Big( \frac{ LT(exp(\frac{Q K^T}{\sqrt{m}})) }{ LT(exp(\frac{Q K^T}{\sqrt{m}}))1_L } \Big) V
\end{aligned}
\label{eq:basic_autoregressive_attention_in_matrix_form}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the implicit ordering and autoregressive rule imposed by $LT(.)$ state that $K_j$/$V_j$, the j-th row of $K$/$V$, is available to $Q_i$, the i-th row of $Q$, if $j \le i$. In order to express \eqref{eq:basic_autoregressive_attention_in_matrix_form} in a fast-attention fashion, the trick done in \eqref{eq:dot_attention_in_matrix_form} has to be applied to individual queries separately due to the fact that the attention range of each query is now non-shareable:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; (A^{ar}_{\phi_Z})_i = \sum_{j = 0}^{i} \Big( \frac{(\hat{Q}_{\phi_Z})_i ((\hat{K}_{\phi_Z})_j)^T }{ \sum_{r = 0}^{i} (\hat{Q}_{\phi_Z})_i ((\hat{K}_{\phi_Z})_r)^T } \Big) V_j \\
&amp; \phantom{(A^{ar}_{\phi_Z})_i} = \frac{(\hat{Q}_{\phi_Z})_i \overbrace{\big( \sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T V_j \big)}^{= U^i} }{ (\hat{Q}_{\phi_Z})_i \underbrace{\big(\sum_{r = 0}^{i} ((\hat{K}_{\phi_Z})_r)^T \big)}_{= S^i}  } \\
&amp; \phantom{(A^{ar}_{\phi_Z})_i} = \frac{ \overbrace{(\hat{Q}_{\phi_Z})_i U^i}^{\hat{U}_{\phi_Z}^i} }{ \underbrace{(\hat{Q}_{\phi_Z})_i S^i}_{\hat{S}_{\phi_Z}^i} } = \frac{\hat{U}_{\phi_Z}^i}{\hat{S}_{\phi_Z}^i}
\end{aligned}
\label{eq:dot_autoregressive_attention_in_matrix_form}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here in \eqref{eq:dot_autoregressive_attention_in_matrix_form} it is assumed that $\phi_{Z}(.)$ is adopted and $(A^{ar}_{\phi_Z})_i$ represents the i-th row of $A^{ar}_{\phi_Z}$, the approximation of $A^{ar}$ obtained via the corresponding fast attention mechanism; furthermore, by conducting a similar analysis as given for \eqref{eq:dot_attention_in_matrix_form}, it can be concluded that for any specific $i$, the time and space complexity are $O(i*no)$ and $O(i*max(n, o))$, which accounts for the iterations required by the summation, respectively.&lt;/p&gt;

&lt;p&gt;One might think that because $0 \le j, r \le i \le L-1$, to obtain a full $A^{ar}_{\phi_Z}$ would requires $O(\frac{L*(L+1)}{2}*no) = O(L^2no)$ in time as well as $O(L^2*max(n, o))$ in space, and thus formulating autoregressive attention this way is no better than its orginal form in terms of computational efficiency; however, if based on the fact that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{aligned}
\begin{cases}
U^i = U^{i-1} + ((\hat{K}_{\phi_Z})_i)^T V_i  \\
S^i = S^{i-1} + ((\hat{K}_{\phi_Z})_i)^T
\end{cases}
\end{aligned}
\end{equation}&lt;/script&gt;

&lt;p&gt;\eqref{eq:dot_autoregressive_attention_in_matrix_form} is rewritten to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; (A^{ar}_{\phi_Z})_i = \frac{ (\hat{Q}_{\phi_Z})_i U^i }{  (\hat{Q}_{\phi_Z})_i S^i } \\ 
&amp; \phantom{(A^{ar}_{\phi_Z})_i} = \frac{ (\hat{Q}_{\phi_Z})_i \big(U^{i-1} + ((\hat{K}_{\phi_Z})_i)^T V_i \big) }{ (\hat{Q}_{\phi_Z})_i \big( S^{i-1} + ((\hat{K}_{\phi_Z})_i)^T \big) }
\end{aligned}
\label{eq:cumulative_dot_autoregressive_attention_in_matrix_form}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $U^{-1}$ and $S^{-1}$ equal to $0$; then a sequential algorithm can be designed accordingly such that it only takes the time complexity of $O(no)$ and the space complexity of $O(max(n, o))$ to evaluate $(A^{ar}_{\phi_Z})_i$ via the reuse of $U^{i-1}$ and $S^{i-1}$ obtained during the previous computation of $(A^{ar}_{\phi_Z})_{i-1}$, for every $i$. Consequently, the overall complexity linear with respect to $L$ is still achievable.&lt;/p&gt;

&lt;p&gt;Another place where this idea of reuse can be shown applicable is the calculation of the associated gradients with \eqref{eq:dot_autoregressive_attention_in_matrix_form} for back-propagation. Given $\mathcal{L}$ as the single-valued objective for optimisation, the derivation of the gradient formulations similar to \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form} begins with the following concatenation matrix $J_i$ of the size $1$ by $o+1$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; J_i = [\hat{U}_{\phi_Z}^i, \hat{S}_{\phi_Z}^i] \\
&amp; \phantom{J_i} = (\hat{Q_{\phi_Z}})_i [U^i, S^i] \\
&amp; \phantom{J_i} = (\hat{Q_{\phi_Z}})_i \big(\sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T \underbrace{[V_j, 1]}_{= \tilde{V}_j} \big) \\
&amp; \phantom{J_i} = (\hat{Q_{\phi_Z}})_i \big(\sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T \tilde{V}_j \big)
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with respect to which the gradient of $\mathcal{L}$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \nabla_{J_i} \mathcal{L} = [\nabla_{\hat{U}_{\phi_Z}^i} \mathcal{L}, \nabla_{\hat{S}_{\phi_Z}^i} \mathcal{L}]
\end{aligned}
\label{eq:gradient_L_J_i}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then the three essential gradients can be defined based on \eqref{eq:gradient_L_J_i}, including $\nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial \mathcal{L}}{\partial (\hat{Q}_{\phi_Z})_{i, a}}  = \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \frac{\partial J_{i, b}}{\partial (\hat{Q}_{\phi_Z})_{i, a}} \\
&amp; \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{Q}_{\phi_Z})_{i, a}}} = \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \big(\sum_{j = 0}^{i} (\tilde{V}_{j, b})^T (\hat{K}_{\phi_Z})_{j, a} \big) \\
&amp; \implies \nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L} = \nabla_{J_i} \mathcal{L} \underbrace{\big(\sum_{j = 0}^{i} (\tilde{V_j})^T (\hat{K}_{\phi_Z})_j \big)}_{U_{\hat{Q}_{\phi_Z}}^i} \\
&amp; \phantom{\implies \nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L}} = \nabla_{J_i} \mathcal{L} \big(U_{\hat{Q}_{\phi_Z}}^{i-1} + (\tilde{V_i})^T (\hat{K}_{\phi_Z})_i \big) 
\end{aligned}
\label{eq:gradient_L_hat_Q_phi_Z_i}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}} = \sum_{i=j}^{L-1} \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \frac{\partial J_{i, b}}{\partial (\hat{K}_{\phi_Z})_{j, a}} \\
&amp; \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}}} = \sum_{i=j}^{L-1} \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} (\hat{Q}_{\phi_Z})_{i, a} \tilde{V}_{j, b} \\
&amp; \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}}} = \sum_{b=0}^{o} \tilde{V}_{j, b} * \big( \sum_{i=j}^{L-1}  \frac{\partial \mathcal{L}}{\partial J_{i, b}} (\hat{Q}_{\phi_Z})_{i, a} \big) \\
&amp; \implies \nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L} = \tilde{V}_j \underbrace{\big(\sum_{i=j}^{L-1} (\nabla_{J_i} \mathcal{L})^T (\hat{Q}_{\phi_Z})_i \big)}_{= U_{\hat{K}_{\phi_Z}}^{j}} \\
&amp; \phantom{\implies \nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L}} = \tilde{V}_j \big(U_{\hat{K}_{\phi_Z}}^{j+1} + (\nabla_{J_j} \mathcal{L})^T (\hat{Q}_{\phi_Z})_j \big)
\end{aligned}
\label{eq:gradient_L_hat_K_phi_Z_j}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $\nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}} = \sum_{i=j}^{L-1} \frac{\partial \mathcal{L}}{\partial J_{i, a}} * \frac{\partial J_{i, b}}{\partial (\tilde{V}_{\phi_Z})_{j, a}} \\
&amp; \phantom{\frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}}} = \sum_{i=j}^{L-1} \frac{\partial \mathcal{L}}{\partial J_{i, a}} \big( \sum_{b=0}^{n-1} (\hat{Q}_{\phi_Z})_{i, b} (\hat{K}_{\phi_Z})_{j, b} \big) \\
&amp; \phantom{\frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}}} = \sum_{b=0}^{n-1} (\hat{K}_{\phi_Z})_{j, b} \big(\sum_{i=j}^{L-1} (\hat{Q}_{\phi_Z})_{i, b} \frac{\partial \mathcal{L}}{\partial J_{i, a}} \big)  \\
&amp; \implies \nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L} = (\hat{K}_{\phi_Z})_j \underbrace{\big( \sum_{i=j}^{L-1} ((\hat{Q}_{\phi_Z})_i)^T \nabla_{J_i} \mathcal{L} \big)}_{= U_{\tilde{V}_{\phi_Z}}^{j}} \\
&amp; \phantom{\implies \nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L}} = (\hat{K}_{\phi_Z})_j \big(U_{\tilde{V}_{\phi_Z}}^{j+1} + ((\hat{Q}_{\phi_Z})_j)^T \nabla_{J_j} \mathcal{L} \big) 

\end{aligned}
\label{eq:gradient_L_tilde_V_j}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the two subscripts of a matrix is used to index the entries of that matrix at locations encoded by (row, column) coordinates; for example, $(\hat{Q}_{\phi_Z})_{i, a}$ refers to the entry of $\hat{Q}_{\phi_Z}$ at the row $i$ and the column $a$. Since \eqref{eq:gradient_L_hat_Q_phi_Z_i}, \eqref{eq:gradient_L_hat_K_phi_Z_j}, and \eqref{eq:gradient_L_tilde_V_j} are expressed as some previous result plus an additonal update, they are certainly eligible for the sequential algorithm suggested for \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form}, albeit there is a subtle distinction: the computational sequence of \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form} should be arranged from the position $0$ to the position $L-1$ (forward) following the autoregressive order, which is the same as the one for \eqref{eq:gradient_L_hat_Q_phi_Z_i}; in contrast, both \eqref{eq:gradient_L_hat_K_phi_Z_j} and \eqref{eq:gradient_L_tilde_V_j} would need to start their sequences at the position $L-1$ and progress down to the position $0$ (backward).&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.14794&quot;&gt;Krzysztof Choromanski et al. “Rethinking Attention with Performers”. In:CoRRabs/2009.14794 (2020)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.16236&quot;&gt;Angelos Katharopoulos et al. “Transformers are RNNs: Fast Autoregressive Transformerswith Linear Attention”. In:CoRRabs/2006.16236 (2020)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 13 May 2021 00:00:00 +0800</pubDate>
        <link>/Blogs/note/Attention-and-Kernel.html</link>
        <guid isPermaLink="true">/Blogs/note/Attention-and-Kernel.html</guid>
        
        <category>MachineLearning</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>From VQVAE to VBVAE</title>
        <description>&lt;p&gt;Living in the Digital Age, we have witnessed how important that the notion of “digitalisation” is in the advance of modern technologies. Arguably the main advantange brought by digitalisation, or quantisation from the perspective of digital signal processing, is the transformation of complicated signals into a form which can be not only processed and stored efficiently, but also robust to noise. Lately, such an idea has also be deployed into the field of machinge learning, and led to a new model of neural networks called &lt;strong&gt;V&lt;/strong&gt;ector &lt;strong&gt;Q&lt;/strong&gt;uantised &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder. While the modelling method has been proven successful in many applications, issues have been observed in my attempt to incorporate it into my recent project, where &lt;strong&gt;VQVAE&lt;/strong&gt; have had difficulty to deal with a large code space. Therefore, a new variant named &lt;strong&gt;V&lt;/strong&gt;ector &lt;strong&gt;B&lt;/strong&gt;inarised &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder, which retains the spirit and benefit of quantisation but does not suffer from the shortcomings of the original &lt;strong&gt;VQVAE&lt;/strong&gt;, is proposed, and in this post the derivation from &lt;strong&gt;VQVAE&lt;/strong&gt; to &lt;strong&gt;VBVAE&lt;/strong&gt; as well as the intuitions behind it are presented. Note that no thorough and comprehensive experiments have been conducted to support this new concept, so please read with a huge grain of salt. ; )&lt;/p&gt;

&lt;h2 id=&quot;review-vqvae&quot;&gt;Review: &lt;strong&gt;VQVAE&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For a data point $x$, the objective (or called &lt;strong&gt;E&lt;/strong&gt;vidence &lt;strong&gt;L&lt;/strong&gt;ower &lt;strong&gt;BO&lt;/strong&gt;und in some literature) of a &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder to be maximised is normally written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Obj_{\text{VAE}}(x) = E_{z \sim q(z|x)}[log(p(x|z))] \\ 
&amp; \phantom{Obj_{\text{VAE}}(x) =\;} + E_{z \sim q(z|x)}[log(\frac{p(z)}{q(z|x)})]
\end{aligned}
\label{eq:ELBO_VAE}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $z$ is a point in the latent space; $q(z|x)$, $p(x|z)$, and $p(z)$ are the approximate posterior distribution, likelihood, and prior distribution over the joint space of $x$ and $z$, respectively. In the common setup of &lt;strong&gt;VAE&lt;/strong&gt;, $q(z|x)$ is usually described by some parametric distribution, whose parameters are generated by a neural network (referred to as encoder $En$) taking $x$ as input. Such an arrangement is also adopted in &lt;strong&gt;VQVAE&lt;/strong&gt;, excpet that the authors have designed a special distribution for $q(z|x)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; q(z = e_k|x) \\
&amp; = \begin{cases}
1 \;\;\; \text{if }e_k = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| En(x) - e_i \|_2 \\
0 \;\;\; \text{otherwise}
\end{cases}
\end{aligned}
\label{eq:q_VQVAE}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\{e_1,\dotsc,e_K\}$ being a codebook, a set of $K$ codes of which the dimensionnality is the same as the one of $En(x)$. This particular choice turns the process of sampling from $q(z|x)$ into the one which simply maps $En(x)$ to one of the codes via nearest-neighbour-search, and hence the latent space is effectively constrained to contain only $K$ instead of an infinite number of points; in other words, under \eqref{eq:q_VQVAE} the whole latent space can be considered quantised into $K$ discrete values, and $z$ is just the quantised result of $En(x)$ (formulated as $z = Qn(En(x))$). Note that by assuming the prior is a uniform distribution, i.e. $p(z) = \frac{1}{K}, z \in \{e_1,\dotsc,e_K\}$, \eqref{eq:ELBO_VAE} in this case can be further simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Obj_{\text{Qn}}(x) = log(p(x|Qn(En(x))) + log(\underbrace{p(Qn(En(x)))}_{= \frac{1}{K}}) \\
&amp; \phantom{Obj_{Qn}(x) =} + \underbrace{Entropy(q(z|x))}_{= 0} \\
&amp; \phantom{Obj_{Qn}(x)} = log(p(x|Qn(En(x))) + log(\frac{1}{K})
\end{aligned}
\label{eq:ELBO_Qn}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which implies for the purpose of maximisation, the effective objective is simply $log(p(x|Qn(En(x)))$. Practically, the direct optimisation of $log(p(x|Qn(En(x)))$ via gradient ascent is impossible, since there is no gradient defined for the quantisation function $Qn$; as a result, the authors of &lt;strong&gt;VQVAE&lt;/strong&gt; proposed an alternative where an straight-through gradient estimator is used to pass directly to $En(x)$ the gradients that are supposed into $Qn$, and this bypassing operation can be realised by the stop-gradient operator (denoted as $SG$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Obj_{\text{VQVAE}}(x) \\ 
&amp; = log(p(x| SG(Qn(En(x)) - En(x)) + En(x))) \\ 
&amp; \phantom{=\;}+ \beta \| En(x) - SG(Qn(En(x))) \|_2^2 \\
\end{aligned}
\label{eq:ELBO_VQVAE}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the second term in \eqref{eq:ELBO_VQVAE} is purposely introduced to serve as a regulariser weighted by the hyper-parameter $\beta$. It corresponds to the fact that other objectives, which will differ from the goal described by \eqref{eq:ELBO_Qn}, are required to provide training signals for $Qn$, as they no longer receive any gradients under \eqref{eq:ELBO_VQVAE}. Once additional optimisation targets dedicated to $Qn$ are incorporated, there will be two disjoint gradient flows (one for parameters involved in the first term of \eqref{eq:ELBO_VQVAE}, and the other for the codebook involved in $Qn$) present in the training dynamic; therefore it needs some form of regularisation, such as the one introduced here, to keep their updates synchronised and make sure the output space of $En$ is learned at the same pace as the codeboock is.&lt;/p&gt;

&lt;p&gt;In relation to providing training signals for updating the codebook, one popular strategy is to regard the training of the codebook used in nearest-neighbour-search as K-mean learning, in which codes (or mean vectors from the perspective of K-mean) are updated via &lt;em&gt;on-line EM&lt;/em&gt; with the following rules:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
&amp; N_i^{(t)} =  N_i^{(t-1)} * \gamma + n_i^{(t)} * (1-\gamma)
\end{aligned}\\
\begin{aligned}
&amp; e_i^{(t)} = e_i^{(t-1)} * \gamma + \sum_{\substack{j \\ Qn(En(x_j)) = e_i^{(t-1)}}} \frac{En(x_j)}{N_i^{(t)}} * (1 - \gamma)
\end{aligned}
\end{cases}
\label{eq:K_Mean_online_EM}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $N_i^{(t)}$ is the average use count for code $i$ up to iteration $t$; $n_i^{(t)}$ is the recorded usage count for code $i$ at iteration $t$; and $\gamma$ is the decaying rate. Despite being effective in many scenarios, training the codebook in this way when the number of codes becomes large could suffer from &lt;em&gt;index collapse&lt;/em&gt;, where the utilisation of codebook capacity is inefficient due to the increasing difficulty of exploring and updating the entire code space under nearest-neighbour-search.&lt;/p&gt;

&lt;h2 id=&quot;alternative-interpretation-of-quantisation-in-vqvae-&quot;&gt;Alternative Interpretation of Quantisation in &lt;strong&gt;VQVAE&lt;/strong&gt; &lt;a name=&quot;alternative_interpretatino&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the previous section, the quantisation function $Qn$ was introduced to neatly describe the action of sampling from the distribution \eqref{eq:q_VQVAE}, but there was no explicit definition given. Now it is time to make up for it by expressing the nearest-neighbour-search with the condition from \eqref{eq:q_VQVAE}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Qn(v) \\
&amp; = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| v - e_i \|_2 \\
&amp; = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \| v - e_i \|_2^2 \\
&amp; = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2 \\
&amp; = argmax_{e_i \in \{e_1,\dotsc,e_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2)
\end{aligned}
\label{eq:Qn_rewriting}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Next, let $C$ denote a matrix where the i-th column corresponds to the code $e_i$, $b$ denote a column vector where the i-th entry is equal to $-\frac{1}{2} \| e_i \|_2^2$, and $h_i$ denote a one-hot column vector where the i-th entry is 1; the last equation of \eqref{eq:Qn_rewriting} can be rewritten in the following way:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Qn(v) \\
&amp; = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)) \\
&amp; = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{Z}) \\
&amp; = C h_{max} = e_{max}
\end{aligned}
\label{eq:Qn_special_RBM}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $h_{max}$ and $e_{max}$ are used to refer to the corresponding vectors picked under the $argmax$ operator; and $Z$ is a constant with respect to all $h_i$. If $Z$ is deliberately chosen to be $\sum_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)$, then $h_{max}$ can be viewed as the result of picking the most likely outcome from the categorical distribution $P(h_i|v)$, which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(h_i|v) \\
&amp; = \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)} \\
&amp; = \frac{exp(v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(v^T C h_j + b^T h_j)} \\
\end{aligned}
\label{eq:P_h_i}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and that is exactly the expression one would derive for the conditional distribution (hidden given visible) from a special case of &lt;strong&gt;R&lt;/strong&gt;estricted &lt;strong&gt;B&lt;/strong&gt;oltzmann &lt;strong&gt;M&lt;/strong&gt;achines, in which binary hidden variables are grouped together to represent a categorical one; visible variables are continuous with its covariance equal to $I$; and the corresponding energy function is $E(v, h_i) = \frac{1}{2} \|v \|_2^2 - v^T C h_i - b^T h_i$. Moreover, as such a &lt;strong&gt;RBM&lt;/strong&gt; also suggests $P(v|h_i) = N(C h_i, I)$, \eqref{eq:Qn_special_RBM} essentially depicts $Qn(v)$ as taking one Gibbs step starting from $v$, except that there is no randomness involved: samples at the first half of the step ($v \to h_{max}$) are always the outcomes with highest probability, while samples at the second half of the step ($h_{max} \to e_{max}$) are simply the distribution mean.&lt;/p&gt;

&lt;h2 id=&quot;generalising-to-vbvae&quot;&gt;Generalising to VBVAE&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;RBM&lt;/strong&gt; interpretation not only sheds light on how the quantisation function in a &lt;strong&gt;VQVAE&lt;/strong&gt; works, but also suggests that very component needed to compose a quantisation function: A &lt;strong&gt;RBM&lt;/strong&gt; with discrete hidden variables and continuous visible variables. So it is possible to define other forms of qunantisation by introducing different configurations of &lt;strong&gt;RBM&lt;/strong&gt;s which provide the similar functionality, for example, the generic one with binary hidden and guassian visible variables (denoted by $h$ and $v$ respectively as in &lt;a href=&quot;#alternative_interpretatino&quot;&gt;the previous section&lt;/a&gt;), whose energy function and conditionals are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
&amp; E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h \\
\end{aligned} \\
\begin{aligned}
&amp; P_{\text{bi}}(h|v) = \frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}}exp(-E_{\text{bi}}(v, \hat{h}))} \\
&amp; \phantom{P_{\text{bi}}(h|v)} = \frac{exp(v^T \Sigma^{-0.5} C h + a^T h)}{\sum_{\hat{h}} exp(v^T \Sigma^{-0.5} C \hat{h} + a^T \hat{h})} \\
&amp; \phantom{P_{\text{bi}}(h|v)} = \frac{exp((\overbrace{v^T \Sigma^{-0.5} C + a^T}^{= S(v)^T}) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma^{-0.5} C + a^T}_{= S(v)^T}) \hat{h})} \\
&amp; \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{\sum_{\hat{h}[m] \in \{0, 1\}} exp(S(v)[m] * \hat{h}[m])} \\
&amp; \phantom{P_{\text{bi}}(h|v)} \propto \prod_{m=1}^{K} \sigma(S(v)[m])
\end{aligned}\\
\begin{aligned}
&amp; P_{\text{bi}}(v|h) = \frac{exp(-E_{\text{bi}}(v, h))}{\int_{\hat{v}}exp(-E_{\text{bi}}(\hat{v}, h))} \\ 
&amp; \phantom{P_{\text{bi}}(v|h)} \propto N(\mu + \Sigma^{0.5} C h, \Sigma)
\end{aligned}
\end{cases}
\label{eq:RGBM}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mu$ is the mean vector; $\Sigma$ is the covariance matrix with all off-diagnal entries equal to 0; $C$ is the same matrix as defined previously; $a$ is the bias vector (which is a free parameter not related to the length of codes, unlike $b$ in \eqref{eq:Qn_special_RBM}); and $S(v) = C \Sigma^{-0.5} v + a$. Also note that under this &lt;strong&gt;RBM&lt;/strong&gt;, $h$ is no longer a one-hot vector but a bit string of which each entry contains either a bit 1 or a bit 0. With \eqref{eq:RGBM}, a new quantisation function can be defined accordingly:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; Qn_{\text{bi}}(v) \nonumber \\
&amp; = \mu + \Sigma^{0.5} C h_{max} \nonumber \\
&amp; = \mu + \Sigma^{0.5} C (argmax_{h} exp(-E_{\text{bi}}(v, h))) \nonumber \\
&amp; = \mu + \Sigma^{0.5} C (argmax_{h} \underbrace{\frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}} exp(-E_{\text{bi}}(v, h))}}_{= P_{\text{bi}}(h|v)}) \nonumber \\
&amp; = \mu + \Sigma^{0.5} C (argmax_{h} \prod_m P_{\text{bi}}(h[m]|v)) \nonumber \\
&amp; = \mu + \Sigma^{0.5} C 
\begin{bmatrix} 
argmax_{h[1]} P_{\text{bi}}(h[1]|v) \\
\vdots \\
argmax_{h[K]} P_{\text{bi}}(h[K]|v)
\end{bmatrix} \nonumber \\
&amp; = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber \\
&amp; = \mu + \Sigma^{0.5} C \lfloor \sigma(S(v)) + 0.5 \rfloor \label{eq:Qn_bi} \\
&amp; = e_{max} \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\sigma(.)$ denoting the entrywise sigmoid function and $\lfloor . \rfloor$ representing the entrywise floor operator. Compared with \eqref{eq:Qn_special_RBM}, the troubling $argmax$ operators, which are the cause of undefined gradients for $Qn$, has disappeared in \eqref{eq:Qn_bi}. While the introduction of floor operators induces another blockage of gradient flows, the same straight-through gradient estimator can still be utilised to send training signals over; in fact, such an estimation becomes more effective and accurate when applied to \eqref{eq:Qn_bi} (see &lt;a href=&quot;#included_image_Fig1&quot;&gt;Fig1&lt;/a&gt;), as not only does the number of nodes along the gradient path affected by the rerouting is reduced, but no parameters are excluded due to the bypass, which means all parameters can receive the training signals generated with respect to the true objective.&lt;/p&gt;

&lt;figure&gt;
	
  &lt;a name=&quot;included_image_Fig1&quot;&gt;&lt;/a&gt;
	
	&lt;img src=&quot;/Blogs/assets/images/2021-01-18-VBVAE/Qn_vs_Qn_bi.png&quot; alt=&quot;Fig1: The comparison between the straight-through gradient estimator for $Qn(v)$ (left) and the one for $Qn_{\text{bi}}(v)$ (right)&quot; /&gt;
	&lt;figcaption&gt;
	Fig1: The comparison between the straight-through gradient estimator for $Qn(v)$ (left) and the one for $Qn_{\text{bi}}(v)$ (right)
	&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Finally, by the substitution of $Qn_{\text{bi}}$ for $Qn$ in \eqref{eq:ELBO_Qn}, an new variant of &lt;strong&gt;VQVAE&lt;/strong&gt;, named &lt;strong&gt;V&lt;/strong&gt;ector &lt;strong&gt;B&lt;/strong&gt;inarised &lt;strong&gt;V&lt;/strong&gt;ariational &lt;strong&gt;A&lt;/strong&gt;uto&lt;strong&gt;E&lt;/strong&gt;ncoder, is established. The notion of binarisation, as the obvious distinction between them, comes from the fact that under $Qn_{\text{bi}}$, discrete codes are indexed by $h$ of the form of bit strings instead of one-hot vectors; as a result, picking a representing code in this quantisation process can be viewed as though deciding whether each bit in $h$ should be on or off. Besides, the objective for &lt;strong&gt;VBVAE&lt;/strong&gt; highlights another difference:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; Obj_{\text{VBVAE}}(x) \\ 
&amp; = log\Bigg(p\bigg(x| SG\Big( \lfloor \sigma(S(En(x))) + 0.5 \rfloor - \sigma(S(En(x))) \Big) \\ 
&amp; \phantom{= log\Bigg(p\bigg(x|\;} + \sigma(S(En(x))) \bigg)\Bigg) \\
\end{aligned}
\label{eq:ELBO_VBVAE}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which unlike &lt;strong&gt;VQVAE&lt;/strong&gt; does not require a separate optimisation target to provide training signals for the codebook, and therefore involves no regularisers.&lt;/p&gt;

&lt;h2 id=&quot;the-advantanges-of-vbvae-over-vqvae&quot;&gt;The Advantanges of VBVAE over VQVAE&lt;/h2&gt;

&lt;p&gt;Firstly, as mentioned earlier, all parameters in &lt;strong&gt;VBVAE&lt;/strong&gt; receive (estimate) gradients with respect to \eqref{eq:ELBO_Qn} directly. That allows the removal of additional objectives and regularisers which are essential in &lt;strong&gt;VQVAE&lt;/strong&gt; to the contrary, so the distortion of the true optimisation contour that those extra terms might bring about could be avoided.&lt;/p&gt;

&lt;p&gt;Secondly, since codes in &lt;strong&gt;VBVAE&lt;/strong&gt; are indexed by bit strings, the number of distinct codes that a codebook is capable of representing can be as large as $2^{|\text{the size of the codebook}|}$; in other words, to model a discrete space containing $K$ codes might only take a codebook of the size $log_2(K)$, rather than $K$ as required in &lt;strong&gt;VQVAE&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thirdly, instead of \eqref{eq:K_Mean_online_EM} in &lt;strong&gt;VQVAE&lt;/strong&gt;, where individual codes are trained only when being used, in &lt;strong&gt;VBVAE&lt;/strong&gt; the quantisation to any code always involves all parameters in the codebook; that implies every update in &lt;strong&gt;VBVAE&lt;/strong&gt; will revise the whole code space, or equivalently, revise every code in it, regardless of which code being used. Hence &lt;strong&gt;VBVAE&lt;/strong&gt; is considered less susceptible to the issue of &lt;em&gt;index collapse&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Overall, the efficiency in code use and training suggests the competence of &lt;strong&gt;VBVAE&lt;/strong&gt; for handling large code spaces.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00937&quot;&gt;Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete RepresentationLearning”. In:CoRR abs/1711.00937 (2017)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.03382&quot;&gt;Lukasz Kaiser et al. “Fast Decoding in Sequence Models using Discrete Latent Variables”.In:CoRRabs/1803.03382 (2018)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.00446&quot;&gt;Ali Razavi, Aäron van den Oord, and Oriol Vinyals. “Generating Diverse High-Fidelity Images with VQ-VAE-2”. In:CoRRabs/1906.00446 (2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 18 Jan 2021 00:00:00 +0800</pubDate>
        <link>/Blogs/research/VBVAE.html</link>
        <guid isPermaLink="true">/Blogs/research/VBVAE.html</guid>
        
        <category>Probability</category>
        
        <category>MachineLearning</category>
        
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>Approximating Partition Functions of Restricted Boltzmann Machines via High-Temperature Expansion</title>
        <description>&lt;p&gt;It is often a difficult task when it comes to computing the gradient of the partition function (i.e. normalising constant) of a probability distribution with a large (and possibly infinite) size of support, due to the intractability of computing the partition function itself, which requires summing/integrating all possible values which an unnormalised &lt;strong&gt;P&lt;/strong&gt;robability &lt;strong&gt;M&lt;/strong&gt;ass &lt;strong&gt;F&lt;/strong&gt;unction/&lt;strong&gt;P&lt;/strong&gt;robability &lt;strong&gt;D&lt;/strong&gt;ensity &lt;strong&gt;F&lt;/strong&gt;unction can take. A famous method called &lt;strong&gt;C&lt;/strong&gt;ontrastive &lt;strong&gt;D&lt;/strong&gt;ivergence (and its variants) is one of many works developed on tackling the issue, and has been widely applied in the case of &lt;strong&gt;R&lt;/strong&gt;estricted &lt;strong&gt;B&lt;/strong&gt;oltzmann &lt;strong&gt;M&lt;/strong&gt;achines. Being a stochastic algorithm, &lt;em&gt;CD&lt;/em&gt; relies on samples from the model distribution to function, which is typically not a problem for the blocked Gibbs sampling enabled by the graph structure of &lt;em&gt;RBM&lt;/em&gt;s; however, the Markov chain described by the conditionals of a &lt;em&gt;RBM&lt;/em&gt; can sometimes turn less ergodic (or in the worst case, non-ergodic) during training. The loss of ergodicity means random walks of Gibbs sampling are unable to jump to every point with a non-zero probability in the model distribution so as to generate samples of it; as a result, samples collected under the circumstances pooly reflect the underlying distribution, which in turn makes &lt;em&gt;CD&lt;/em&gt; fail to produce quality estimation. To escape out of this predicament, there are advanced sampling techniques like &lt;em&gt;Simulated Annealing&lt;/em&gt; or &lt;em&gt;Parallel Tempering&lt;/em&gt;, which allow more exploratory random walks through the support space; nonetheless, from my inadequate knowledge and experience in sampling, it does not seem easy to incorporate those sampling procedures and have them stable within the training dynamic, where the model distribution constantly changes. So what else can we do? Well, just appeal to other methods which do not involve sampling, and here comes &lt;em&gt;high-temperature expansion&lt;/em&gt; to rescue.&lt;/p&gt;

&lt;h2 id=&quot;objective-&quot;&gt;Objective &lt;a name=&quot;objective&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Unlike &lt;em&gt;CD&lt;/em&gt;, in &lt;em&gt;high-temperature expansion&lt;/em&gt; the goal is to directly obtain the logarithm of the partition function of a &lt;em&gt;RBM&lt;/em&gt;, and it is achieved via approximation. For instance, considering the basic form of a &lt;em&gt;RBM&lt;/em&gt; in which both the hidden varibles $h$ and visible variables $v$ are binary, its energy function is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; E(v, h) = -\sum_i a_i v_i - \sum_j b_j h_j - \sum_{i,j} w_{i,j} v_i h_j
\end{aligned}
\label{eq:energyFunction}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $a$ is the bias vector for $v$, $b$ is the bias vector for $h$, and $w$ is the weight matrix of which each entry corresponds to a distinct $(v_i, h_j)$ pair. By definition, the logarithm of the partition function $Z$ of the system above is written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; log(Z) \\
&amp; = log(\sum_{v, h} exp(-E(v, h))) \\
&amp; = log(\sum_{v, h} exp(\sum_i a_i v_i + \sum_j b_j h_j + \sum_{i,j} w_{i,j} v_i h_j))
\end{aligned}
\label{eq:log_Z}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that \eqref{eq:energyFunction} can also be seen as the energy function with its $\beta$, the reciprocal of the current temperature, set to 1; restoring $\beta$, therefore, brings back the general form of the partition function $Z(\beta)$ which allows for evaluation at different temperatures:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; log(Z(\beta)) \nonumber \\
&amp; = log(\sum_{v, h} exp(-\beta * E(v, h))) \nonumber \\
&amp; = log(\sum_{v, h} exp(\beta \sum_i a_i v_i + \beta \sum_j b_j h_j + \beta \sum_{i,j} w_{i,j} v_i h_j)) \label{eq:log_Z_of_beta}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is \eqref{eq:log_Z_of_beta} that &lt;em&gt;high-temperature expansion&lt;/em&gt; is aimed at approximating in this particular example.&lt;/p&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Continuing with the same example in &lt;a href=&quot;#objective&quot;&gt;Objective&lt;/a&gt;, the approximation is started off with the relaxation of $\beta$, so that $\beta \in \mathbb{R}$ (it can be thought of as lifting the restriction that the system must be at a temperature greater or equal to 0). Next, a helping function $G$ is defined with two groups of additional auxiliary variables, $m$ and $\lambda(\beta)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; G(v, h, m, \lambda(\beta)) \\
&amp; = -\sum_i \lambda_{v_i}(\beta) (v_i - m_{v_i}) - \sum_j \lambda_{h_j}(\beta) (h_j - m_{h_j})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where subscripts attached to $\lambda(\beta)$ and $m$ are used to indicate what hidden/visible variable to which a subscripted one is linked, and for each hidden/visible variable, there is a variable in $\lambda(\beta)$ and one in $m$ corresponding to it; also all variables in the group $\lambda(\beta)$ are dependent on $\beta$. This function $G$ is then inserted into the system description to form a modified energy function $\hat{E}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \hat{E}(v, h, m, \beta, \lambda(\beta)) = \beta * E(v, h) + G(v, h, m, \lambda(\beta))
\end{aligned}
\label{eq:E_hat}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;and consequently a modified partition function $\hat{Z}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; log(\hat{Z}(\beta, m, \lambda(\beta))) \nonumber \\
&amp; = log(\sum_{v, h} exp(-\hat{E}(v, h, m, \beta, \lambda(\beta))) \nonumber \\
&amp; = log(\sum_{v, h} exp(-\beta * E(v, h) - G(v, h, m, \lambda(\beta))) \nonumber \\
&amp; = log\Big( \sum_{v, h} exp \big( \beta \sum_i a_i v_i + \beta \sum_j b_j h_j + \beta \sum_{i,j} w_{i,j} v_i h_j \nonumber \\
&amp; \phantom{= log\Big(} + \sum_i \lambda_{v_i}(\beta) (v_i - m_{v_i}) + \sum_j \lambda_{h_j}(\beta) (h_j - m_{h_j})  \big)\Big) \label{eq:log_Z_hat}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;One might start wondering: “Wait a minute, \eqref{eq:log_Z_hat} is no longer the true partition function we want to approximate, and why do we need to make life even difficult by including more variables?”. Sure, that is a legitimate question, and here is my explanation (based on my understanding about this method) for it: Since all $v$ and $h$ have been summed out, if the value of $\beta$ is given, \eqref{eq:log_Z_hat} can be viewed as a real-valued function taking $m$ and $\lambda(\beta)$ as inputs. So let us analyse that function through finding its stationary points (I deliberately use “stationary points” to avoid overclaiming their roles, while you could think of them as optimal points if you like). To that end, the derivatives of \eqref{eq:log_Z_hat} are taken and set to zero with respect to all variables including $\lambda_{v_i}(\beta)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial \lambda_{v_i}(\beta)} \\
&amp; = \sum_{v, h} \frac{exp(-\beta * E(v, h))}{\hat{Z}(\beta, m, \lambda(\beta))} * \frac{\partial exp(-G(v, h, m, \lambda(\beta)))}{\partial \lambda_{v_i}(\beta)}  \\
&amp; = \sum_{v, h} \underbrace{\frac{exp(-\beta * E(v, h) - G(v, h, m, \lambda(\beta)))}{\hat{Z}(\beta, m, \lambda(\beta))}}_{P_{\beta, m, \lambda(\beta)}(v, h)} (v_i - m_{v_i})  \\
&amp; = \sum_{v_i} P_{\beta, m, \lambda(\beta)}(v_i) (v_i - m_{v_i})  \\
&amp; = \underbrace{\mathbb{E}_{P_{\beta, m, \lambda(\beta)}}[v_i]}_{\text{The expectation of }v_i\,=\,\mu_{\beta, m, \lambda(\beta)}(v_i)} - m_{v_i} = 0 \\
&amp; \\
&amp; \implies m_{v_i} = \mu_{\beta, m, \lambda(\beta)}(v_i)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\lambda_{h_j}(\beta)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial \lambda_{h_j}(\beta)} \\
&amp; = \sum_{v, h} \frac{exp(-\beta * E(v, h))}{\hat{Z}(\beta, m, \lambda(\beta))} * \frac{\partial exp(-G(v, h, m, \lambda(\beta)))}{\partial \lambda_{h_j}(\beta)}  \\
&amp; = \sum_{v, h} \frac{exp(-\beta * E(v, h) - G(v, h, m, \lambda(\beta)))}{\hat{Z}(\beta, m, \lambda(\beta))} (h_j - m_{h_j})  \\
&amp; = \sum_{h_j} P_{\beta, m, \lambda(\beta)}(h_j) (h_j - m_{h_j})  \\
&amp; = \underbrace{\mathbb{E}_{P_{\beta, m, \lambda(\beta)}}[h_j]}_{\mu_{\beta, m, \lambda(\beta)}(h_j)} - m_{h_j} = 0 \\
&amp; \\
&amp; \implies m_{h_j} = \mu_{\beta, m, \lambda(\beta)}(h_j)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$m_{v_i}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial m_{v_i}} \\
&amp; = \sum_{v, h} \frac{exp(-\beta * E(v, h))}{\hat{Z}(\beta, m, \lambda(\beta))} * \frac{\partial exp(-G(v, h, m, \lambda(\beta)))}{\partial m_{v_i}}  \\
&amp; = \sum_{v, h} P_{\beta, m, \lambda(\beta)}(v, h) * (-\lambda_{v_i}(\beta))  \\
&amp; = -\lambda_{v_i}(\beta) = 0
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $m_{h_j}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial m_{h_j}} \\
&amp; = \sum_{v, h} \frac{exp(-\beta * E(v, h))}{\hat{Z}(\beta, m, \lambda(\beta))} * \frac{\partial exp(-G(v, h, m, \lambda(\beta)))}{\partial m_{h_j}}  \\
&amp; = \sum_{v, h} P_{\beta, m, \lambda(\beta)}(v, h) * (-\lambda_{h_j}(\beta))  \\
&amp; = -\lambda_{h_j}(\beta) = 0
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As indicated by those derivatives, stationary points will be those points which satisfies the relationships of $m_{v_i} = \mu_{\beta, m, \lambda(\beta)}(v_i)$ and $\lambda_{v_i}(\beta) = 0$ for all $v_i$, $m_{h_j} = \mu_{\beta, m, \lambda(\beta)}(h_j)$ and $\lambda_{h_j}(\beta) = 0$ for all $h_j$; therefore, when stationary points are evaluated, \eqref{eq:log_Z_hat} can be further simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; log\Big( \sum_{v, h} exp \big( \beta \sum_i a_i v_i + \beta \sum_j b_j h_j + \beta \sum_{i,j} w_{i,j} v_i h_j \big)\Big) \\
&amp; = log(Z(\beta))
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;due to the fact that all $\lambda(\beta)$ variables are 0. The connection above suggests that instead of computing \eqref{eq:log_Z_of_beta} for some $\beta$ of interest directly, one can do the equivalent, which is finding the value of \eqref{eq:log_Z_hat} at it stationary points in terms of $m$ and $\lambda(\beta)$ given $\beta$ fixed to that value; furthermore, unlike the former approach (which is intractable), the latter one can be done manageably through the following approximation steps to avoid the notorious summation over $v$ and $h$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Assume the values of $\lambda(\beta)$ with respect to which the partial derivatives of \eqref{eq:log_Z_hat} equal 0 are known, and denote them as $\lambda^\ast_{v_i}(\beta)$ and $\lambda^\ast_{h_j}(\beta)$.&lt;/li&gt;
  &lt;li&gt;Conduct a &lt;em&gt;Taylor expansion&lt;/em&gt; of $log(\hat{Z}(\beta, m, \lambda^\ast_{v_i}(\beta), \lambda^\ast_{h_j}(\beta)))$, i.e. \eqref{eq:log_Z_hat} with $\lambda^\ast_{v_i}(\beta)$ and $\lambda^\ast_{h_j}(\beta)$ plugged in, around $\beta = 0$ (corresponding to the highest temperature).&lt;/li&gt;
  &lt;li&gt;Locate stationary points using the approximate function obtained at &lt;a href=&quot;#approximation_step2&quot;&gt;step 2&lt;/a&gt; with a designated $\beta$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the assumption made at &lt;a href=&quot;#approximation_step1&quot;&gt;step 1&lt;/a&gt; is in effect a valid one because all $\lambda^\ast_{v_i}(\beta)$ and $\lambda^\ast_{h_j}(\beta)$ can be, and will be represented in terms of corresponding $m$ variables; as a result, by estimating $m$ variables, as will be performed at &lt;a href=&quot;#approximation_step3&quot;&gt;step 3&lt;/a&gt;, $\lambda^\ast_{v_i}(\beta)$ and $\lambda^\ast_{h_j}(\beta)$ are automatically known.&lt;/p&gt;

&lt;h3 id=&quot;step-1-&quot;&gt;Step 1 &lt;a name=&quot;approximation_step1&quot;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now let us go through each of the steps. At &lt;a href=&quot;#approximation_step1&quot;&gt;step 1&lt;/a&gt;, with the assumption the $\ast$ version of $\lambda(\beta)$ variables are plugged into \eqref{eq:log_Z_hat} to produce:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; log(\hat{Z}(\beta, m, \lambda^\ast(\beta))) \\
&amp; = log\Big( \sum_{v, h} exp \big( \beta \sum_i a_i v_i + \beta \sum_j b_j h_j + \beta \sum_{i,j} w_{i,j} v_i h_j \\
&amp; \phantom{= log\Big(} + \sum_i \lambda^\ast_{v_i}(\beta) (v_i - m_{v_i}) + \sum_j \lambda^\ast_{h_j}(\beta) (h_j - m_{h_j})  \big)\Big)
\end{aligned}
\label{eq:log_Z_hat_lambda_ast}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is just as the original \eqref{eq:log_Z_hat} but forced to obey the contraints of $\frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial \lambda_{v_i}(\beta)} = 0$ and $\frac{\partial log(\hat{Z}(\beta, m, \lambda(\beta)))}{\partial \lambda_{h_j}(\beta)} = 0$ for all $v_i$ and $h_j$. Consequently, the following relationships, as per the discussion above, are implied:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
m_{v_i} = \mu_{\beta, m, \lambda^\ast(\beta)}(v_i) \\
m_{h_j} = \mu_{\beta, m, \lambda^\ast(\beta)}(h_j) \\
\end{cases}
\label{eq:magnetisation}
\end{equation}&lt;/script&gt;

&lt;p&gt;for all $v_i$ and $h_j$. Note that \eqref{eq:magnetisation} will hold true no matter what value of $\beta$, such as $\beta = 0$, is given; in that case, an interesting result emerges from \eqref{eq:magnetisation}, based on the fact that $\hat{E}(v, h, m, \beta = 0, \lambda^\ast(\beta = 0)) = G(v, h, m, \lambda^\ast(\beta = 0))$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; m_{v_i} = \mu_{\beta = 0, m, \lambda^\ast(\beta = 0)}(v_i) \\
&amp; \phantom{m_{v_i}} = \sum_{v_i} P_{\beta = 0, m, \lambda^\ast(\beta = 0)}(v_i) * v_i \\
&amp; \phantom{m_{v_i}} = \sum_{v_i} \sum_{v_{\neg i}, h} \frac{exp(-G(v, h, m, \lambda^\ast(0)))}{\hat{Z}(\beta = 0, m, \lambda^\ast(\beta = 0))} * v_i \\
&amp; \phantom{m_{v_i}} = \frac{\sum_{v, h} exp(-G(v, h, m, \lambda^\ast(0))) * v_i}{\sum_{v, h} exp(-G(v, h, m, \lambda^\ast(0)))}  \\
&amp; \phantom{m_{v_i}} = \underbrace{\frac{\sum_{h} exp(\sum_{j} \lambda^\ast_{h_j}(0) (h_j - m_{h_j}))}{\sum_{h} exp(\sum_{j} \lambda^\ast_{h_j}(0) (h_j - m_{h_j}))}}_{= 1} \\
&amp; \phantom{m_{v_i} =} * \frac{\sum_{v} exp(\sum_{k} \lambda^\ast_{v_k}(0) (v_k - m_{v_k})) v_i}{\sum_{v} exp(\sum_{k} \lambda^\ast_{v_k}(0) (v_k - m_{v_k}))} \label{eq:factorisation} \\
&amp; \phantom{m_{v_i}} = \underbrace{\frac{\sum_{v' \in v_{\neg i}} exp(\sum_{k \neq i} \lambda^\ast_{v'_k}(0) (v'_k - m_{v'_k})) }{\sum_{v' \in v_{\neg i}} exp(\sum_{k \neq i} \lambda^\ast_{v'_k}(0) (v'_k - m_{v'_k}))}}_{= 1} \\
&amp; \phantom{m_{v_i} =} * \frac{\sum_{v_i} exp(\lambda^\ast_{v_i}(0) (v_i - m_{v_i})) v_i}{\sum_{v_i} exp(\lambda^\ast_{v_i}(0) (v_i - m_{v_i}))} \\
&amp; \phantom{m_{v_i}} = \underbrace{\frac{exp(-\lambda^\ast_{v_i}(0)m_{v_i})}{exp(-\lambda^\ast_{v_i}(0)m_{v_i})}}_{= 1} \frac{\sum_{v_i} exp(\lambda^\ast_{v_i}(0) v_i) v_i}{\sum_{v_i} exp(\lambda^\ast_{v_i}(0) v_i )} \\
&amp; \phantom{m_{v_i}} = \frac{exp(\lambda^\ast_{v_i}(0))}{1 + exp(\lambda^\ast_{v_i}(0))} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and similarly for $m_{h_j}$. It turns out that \eqref{eq:magnetisation} evaluated at $\beta = 0$ reveals a special case where invertible relationships between $m_{v_i}$/$m_{h_j}$ and $\lambda^\ast_{v_i}(0)$/$\lambda^\ast_{h_j}(0)$ are identified, suggesting how $\lambda^\ast(0)$ can be expressed in terms of $m$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
\lambda^\ast_{v_i}(0) = log(\frac{m_{v_i}}{1 - m_{v_i}}) \\
\lambda^\ast_{h_j}(0) = log(\frac{m_{h_j}}{1 - m_{h_j}}) \\
\end{cases}
\label{eq:lambda_in_terms_of_m}
\end{equation}&lt;/script&gt;

&lt;p&gt;It is worth noting that the distribution $P_{\beta = 0, m, \lambda^\ast(\beta = 0)}(v, h)$, which is dictated by the energy function $\hat{E}(v, h, m, \beta = 0, \lambda^\ast(\beta = 0)) = G(v, h, m, \lambda^\ast(\beta = 0))$, can be completely factorised in terms of each individual $v_i$ and $h_j$ (thanks to the particular arrangement of $G$, which contains no interactions between variables); in other words, their distributions turn independent of one another when $\beta = 0$. That is a useful property for simplifying the computation whenever the summation over all possible configurations is requried, and will be utilised repeatedly in the following derivation.&lt;/p&gt;

&lt;h3 id=&quot;step-2-&quot;&gt;Step 2 &lt;a name=&quot;approximation_step2&quot;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Moving onto step 2, the &lt;em&gt;Taylor expansion&lt;/em&gt; of \eqref{eq:log_Z_hat_lambda_ast} around $\beta = 0$ up to the second-order term (Note that the incorporation of other higher-order terms is also possible) is first written out:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; log(\hat{Z}(\beta, m, \lambda^\ast(\beta))) \\ 
&amp; \approx \left. log(\hat{Z}(\beta, m, \lambda^\ast(\beta))) \right|_{\beta = 0} \frac{\beta^0}{0!} \\
&amp; \phantom{\approx} \; + \left. \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta} \right|_{\beta = 0} \frac{\beta^1}{1!}  \\
&amp; \phantom{\approx} \; + \left. \frac{\partial^2 log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta^2} \right|_{\beta = 0} \frac{\beta^2}{2!} 
\end{aligned}
\label{eq:Taylor_expansion_with_unknown_coefficients}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;To complete the expansion, those coefficients associated with each term of different orders need to be computed. For the zeroth-order term:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \left. log(\hat{Z}(\beta, m, \lambda^\ast(\beta))) \right|_{\beta = 0}  \nonumber \\
&amp; = log(\sum_{v, h} exp(-G(v, h, m, \lambda^\ast(\beta = 0)))) \nonumber \\
&amp; = log\big(\sum_{v, h} exp(\sum_i \lambda^\ast_{v_i}(0) (v_i - m_{v_i}) \nonumber \\
&amp; \phantom{= log\big(} + \sum_j \lambda^\ast_{h_j}(0) (h_j - m_{h_j}))\big) \nonumber \\
&amp; = log\big(\sum_{v} \prod_i exp(\lambda^\ast_{v_i}(0) (v_i - m_{v_i})) \nonumber \\ 
&amp; \phantom{= log\big(} * \sum_{h} \prod_j exp(\lambda^\ast_{h_j}(0) (h_j - m_{h_j}))\big) \nonumber \\
&amp; = log\big(\prod_i \sum_{v_i} exp(\lambda^\ast_{v_i}(0) (v_i - m_{v_i})) \nonumber \\
&amp; \phantom{= log\big(} * \prod_j \sum_{h_j} exp(\lambda^\ast_{h_j}(0) (h_j - m_{h_j}))\big) \nonumber \\
&amp; = \sum_i log(exp(-\lambda^\ast_{v_i}(0)m_{v_i})) + exp(\lambda^\ast_{v_i}(0) (1 - m_{v_i}))) \nonumber \\
&amp; \phantom{=} + \sum_j log(exp(-\lambda^\ast_{h_j}(0) m_{h_j}) + exp(-\lambda^\ast_{h_j}(0) (1 - m_{h_j}))) \nonumber \\
&amp; = \sum_i log(exp(-\lambda^\ast_{v_i}(0)m_{v_i})) + log(1 + exp(\lambda^\ast_{v_i}(0))) \nonumber \\
&amp; \phantom{=} + \sum_j log(exp(-\lambda^\ast_{h_j}(0) m_{h_j})) + log(1 + exp(\lambda^\ast_{h_j}(0))) \nonumber \\
&amp; = \sum_i -m_{v_i}log(\frac{m_{v_i}}{1-m_{v_i}}) + log(1 + \frac{m_{v_i}}{1-m_{v_i}}) \nonumber \\
&amp; \phantom{=} + \sum_j -m_{h_j}log(\frac{m_{h_j}}{1-m_{h_j}}) + log(1 + \frac{m_{h_j}}{1-m_{h_j}}) \label{eq:zeroth_term_lambda_replacement} \\
&amp; = \sum_i -m_{v_i}log(m_{v_i}) + m_{v_i}log(1-m_{v_i}) - log(1-m_{v_i}) \nonumber \\
&amp; \phantom{=} + \sum_j -m_{h_j}log(m_{h_j}) + m_{h_j}log(1-m_{h_j}) - log(1-m_{h_j}) \nonumber \\
&amp; = \sum_i -(m_{v_i}log(m_{v_i}) + (1 - m_{v_i})log(1-m_{v_i})) \nonumber \\
&amp; \phantom{=} \sum_j -(m_{h_j}log(m_{h_j}) + (1 - m_{h_j})log(1-m_{h_j})) \label{eq:zeroth_order_coef}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the equality at \eqref{eq:zeroth_term_lambda_replacement} resulting from \eqref{eq:lambda_in_terms_of_m}.&lt;/p&gt;

&lt;p&gt;In relation to the coefficient of the first-order term, the derivatives involved in this coefficient are actually not as intimidating as it seems, since they can be zeroed out with the help of \eqref{eq:magnetisation}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \left. \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta} \right|_{\beta = 0} \nonumber \\
&amp; = \frac{1}{\hat{Z}(\beta = 0, m, \lambda^\ast(\beta = 0))} \left. \frac{\partial \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta} \right|_{\beta = 0} \nonumber \\
&amp; = \sum_{v, h} \Bigg[ \frac{exp(-\hat{E}(v, h, m, \beta = 0, \lambda^\ast(\beta = 0)))}{\hat{Z}(0, m, \lambda^\ast(0))} \nonumber \\ 
&amp; \phantom{= \sum_{v, h} \Bigg[} * \left.\frac{\partial -\hat{E}(v, h, m, \beta, \lambda^\ast(\beta))}{\partial \beta} \right|_{\beta = 0} \Bigg] \nonumber \\
&amp; = \sum_{v, h} \Bigg[ P_{\beta = 0, m, \lambda^\ast(\beta = 0)}(v, h) \nonumber \\
&amp; \phantom{= \sum_{v, h} \Bigg[} * (-E(v, h) +  \left.\frac{\partial -G(v, h, m, \lambda^\ast(\beta))}{\partial \beta}\right|_{\beta = 0}) \Bigg] \nonumber  \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)] \nonumber \\
&amp; \phantom{=} + \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \sum_{i} \left.  \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0}(v_i - m_{v_i})] \nonumber \\
&amp; \phantom{=} + \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \sum_{j} \left.  \frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}\right|_{\beta = 0}(h_j - m_{h_j})] \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)] \nonumber \\ 
&amp; \phantom{=} + \sum_{i} \left.  \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0} (\underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[v_i]}_{= \mu_{0, m, \lambda^\ast(0)}(v_i) = m_{v_i}} - m_{v_i}) \nonumber \\
&amp; \phantom{=} + \sum_{j} \left.  \frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}\right|_{\beta = 0} (\underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[h_j]}_{= \mu_{0, m, \lambda^\ast(0)}(h_j) = m_{h_j}} - m_{h_j}) \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)] \nonumber \\
&amp; = \sum_i a_i  \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[v_i] + \sum_j b_j  \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[h_j] \nonumber \\
&amp; \phantom{=} + \sum_{i,j} w_{i,j}  \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[v_i]  \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[h_j] \label{expectation_minus_E} \\
&amp; = \sum_i a_i m_{v_i} + \sum_j b_j m_{h_j} + \sum_{i, j} w_{i,j} m_{v_i} m_{h_j} \label{eq:first_order_coef}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the equality at \eqref{expectation_minus_E} comes from the independency of marginal distributions as discussed in &lt;a href=&quot;#approximation_step1&quot;&gt;step 1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Proceeding the similar procedure for arriving at \eqref{eq:first_order_coef} to work out the second derivative of \eqref{eq:log_Z_hat_lambda_ast} and the coefficient ($\frac{1}{2!}$ is omitted for brevity) of the second-order term:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \left. \frac{\partial^2 log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta^2} \right|_{\beta = 0} \nonumber \\
&amp; = \left. \frac{\partial \frac{1}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \frac{\partial \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta}}{\partial \beta} \right|_{\beta = 0} \nonumber \\
&amp; = \left. \frac{ \hat{Z}(\beta, m, \lambda^\ast(\beta)) \frac{\partial^2 \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta^2} - (\frac{\partial \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta})^2}{(\hat{Z}(\beta, m, \lambda^\ast(\beta)))^2} \right|_{\beta = 0} \nonumber \\
&amp; = \left. \frac{\frac{\partial^2 \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta^2}}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \right|_{\beta = 0} \nonumber \\ 
&amp; \phantom{=} - \left. (\underbrace{\frac{1}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \frac{\partial \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta}}_{= \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta}})^2 \right|_{\beta = 0} \nonumber \\
&amp; = \left. \frac{\frac{\partial^2 \hat{Z}(\beta, m, \lambda^\ast(\beta))}{\partial \beta^2}}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \right|_{\beta = 0} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \nonumber \\
&amp; = \left. \sum_{v, h} \Big( \frac{exp(-\hat{E}(v, h, m, \beta, \lambda^\ast(\beta)))}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \right. \nonumber \\
&amp; \phantom{= \sum_{v, h}\Big(} \left. * \big(\frac{-\partial \hat{E}(v, h, m, \beta, \lambda^\ast(\beta))}{\partial \beta}\big)^2 \Big) \right|_{\beta = 0} \nonumber \\
&amp; \phantom{=} + \left. \sum_{v, h} \Big( \frac{exp(-\hat{E}(v, h, m, \beta, \lambda^\ast(\beta)))}{\hat{Z}(\beta, m, \lambda^\ast(\beta))} \right. \nonumber \\
&amp; \phantom{= + \sum_{v, h}\Big(} \left. * \frac{-\partial^2 \hat{E}(v, h, m, \beta, \lambda^\ast(\beta))}{\partial \beta^2} \Big) \right|_{\beta = 0} \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(-E(v, h) + \left.\frac{\partial -G(v, h, m, \lambda^\ast(\beta))}{\partial \beta} \right|_{\beta = 0} \big)^2 ] \nonumber \\
&amp; \phantom{=} + \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\sum_i \left. \frac{\partial^2 \lambda^\ast_{v_i}(\beta)}{\partial \beta^2}\right|_{\beta = 0}(v_i - m_{v_i})]}_{= 0} \nonumber \\
&amp; \phantom{=} + \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\sum_j \left. \frac{\partial^2 \lambda^\ast_{h_j}(\beta)}{\partial \beta^2}\right|_{\beta = 0}(h_j - m_{h_j})]}_{= 0} \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(-E(v, h) \nonumber \\ 
&amp; \phantom{= \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_i \left. \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0}(v_i - m_{v_i}) \nonumber \\
&amp; \phantom{= \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_j \left. \frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}\right|_{\beta = 0}(h_j - m_{h_j})\big)^2] \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2  \label{eq:second_order_coef_with_lambda_and_expectation} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This time $\frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}$ and $\frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}$ do not disappear, so they all need to be evaluated at $\beta = 0$, which can be done by the following trick: the fact that $\frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial m_{v_i}} = -\lambda^\ast_{v_i}(\beta)$ implies $\frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta} = -\frac{\partial \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial m_{v_i}}}{\partial \beta}$, and it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;\left. \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0} \\
&amp; = \left. -\frac{\partial \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial m_{v_i}}}{\partial \beta} \right|_{\beta = 0} \\
&amp; = -\frac{\partial \; \underbrace{\left. \frac{\partial log(\hat{Z}(\beta, m, \lambda^\ast(\beta)))}{\partial \beta} \right|_{\beta = 0}}_{= \eqref{eq:first_order_coef}} }{\partial m_{v_i}} \\
&amp; = -\frac{\partial (\sum_{i'} a_{i'} m_{v_{i'}} + \sum_j b_j m_{h_j} + \sum_{i, j} w_{i',j} m_{v_{i'}} m_{h_j})}{\partial m_{v_i}} \\
&amp; = - a_i - \sum_j w_{i, j} m_{h_j}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As the same idea also applies to $\frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}$, the conclusion below is reached:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{cases}
\left. \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0} = -a_i - \sum_j w_{i, j} m_{h_j} \\
\left. \frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}\right|_{\beta = 0} = -b_j - \sum_i w_{i, j} m_{v_i}
\end{cases}
\label{eq:evaluation_d_lambda_d_beta_at_0}
\end{equation}&lt;/script&gt;

&lt;p&gt;With \eqref{eq:evaluation_d_lambda_d_beta_at_0} at hand, \eqref{eq:second_order_coef_with_lambda_and_expectation} can now be expressed without involving anything relating to $\lambda^\ast(\beta)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(-E(v, h) \nonumber \\ 
&amp; \phantom{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_i \left. \frac{\partial \lambda^\ast_{v_i}(\beta)}{\partial \beta}\right|_{\beta = 0}(v_i - m_{v_i}) \nonumber \\
&amp; \phantom{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_j \left. \frac{\partial \lambda^\ast_{h_j}(\beta)}{\partial \beta}\right|_{\beta = 0}(h_j - m_{h_j})\big)^2] \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(-E(v, h) \nonumber \\
&amp; \phantom{= \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_i (-a_i - \sum_j w_{i, j} m_{h_j})(v_i - m_{v_i}) \nonumber \\
&amp; \phantom{= \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_j (-b_j - \sum_i w_{i, j} m_{v_i})(h_j - m_{h_j})\big)^2] \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \label{eq:second_order_coef_with_expectation} \\ 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For futher simplifying \eqref{eq:second_order_coef_with_expectation}, mainly the evaluation of the expectations, one might expand the quadratic terms in \eqref{eq:second_order_coef_with_expectation} and carefully deal with not only the first moment, but also the second moments of all $v_i$ and $h_j$, which is a lengthy and error-prone process; or adopt another trick developed by brilliant researchers who have found a shortcut for computing the expectation of such a form: given two random variables $A$ and $B$ with $\mathbb{E}[B] = 0$, let $C = A - \mathbb{E}[A] + B$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \mathbb{E}[C^2] \\
&amp; = \mathbb{E}[(A - \mathbb{E}[A])^2 + 2(A - \mathbb{E}[A])B + B^2] \\
&amp; = \mathbb{E}[A^2 - 2A\mathbb{E}[A] + \mathbb{E}[A]^2 + 2AB - 2\mathbb{E}[A]B + B^2] \\
&amp; = \mathbb{E}[A^2 + 2AB + B^2] - \mathbb{E}[A]^2 - 2\mathbb{E}[A]\underbrace{\mathbb{E}[B]}_{= 0} \\
&amp; = \mathbb{E}[(A + B)^2] - \mathbb{E}[A]^2
\end{aligned}
\label{eq:expectation_trick}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;in which \eqref{eq:second_order_coef_with_expectation} are perfectly fitted by viewing $-E(v, h)$ as $A$ and $\sum_i (-a_i - \sum_j w_{i, j} m_{h_j})(v_i - m_{v_i}) + \sum_j (-b_j - \sum_i w_{i, j} m_{v_i})(h_j - m_{h_j})$ as $B$. Therefore, $C$ in our case becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; -E(v, h) - \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)]  \\
&amp; \; + \sum_i (-a_i - \sum_j w_{i, j} m_{h_j})(v_i - m_{v_i}) \\
&amp; \; + \sum_j (-b_j - \sum_i w_{i, j} m_{v_i})(h_j - m_{h_j})  \\
&amp; = \sum_i a_i v_i + \sum_j b_j h_j + \sum_{i,j} w_{i,j} v_i h_j \\
&amp; \phantom{=} - \sum_i a_i m_{v_i} - \sum_j b_j m_{h_j} - \sum_{i,j} w_{i,j} m_{v_i} m_{h_j} \\
&amp; \phantom{=} - \sum_i a_i(v_i - m_{v_i}) - \sum_{i,j} w_{i, j} m_{h_j}(v_i - m_{v_i}) \\
&amp; \phantom{=} - \sum_j b_j(h_j - m_{h_j}) - \sum_{i,j} w_{i, j} m_{v_i}(h_j - m_{h_j}) \\
&amp; = \sum_{i,j} w_{i,j} (v_i h_j - m_{v_i} m_{h_j} - v_i m_{h_j} + m_{v_i} m_{h_j} - m_{v_i} h_j + m_{v_i} m_{h_j}) \\
&amp; = \sum_{i,j} w_{i,j} (v_i - m_{v_i})(h_j - m_{h_j})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and \eqref{eq:second_order_coef_with_expectation} is just equivalent to $\mathbb{E}[C^2]$, which means:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(-E(v, h) \nonumber \\
&amp; \phantom{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_i (-a_i - \sum_j w_{i, j} m_{h_j})(v_i - m_{v_i}) \nonumber \\
&amp; \phantom{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ \big(} + \sum_j (-b_j - \sum_i w_{i, j} m_{v_i})(h_j - m_{h_j}) \big)^2] \nonumber \\
&amp; \phantom{=} - (\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[-E(v, h)])^2 \nonumber \\ 
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\big(\sum_{i,j} w_{i,j} (v_i - m_{v_i})(h_j - m_{h_j})\big)^2] \nonumber \\
&amp; = \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\sum_{i,j} w^2_{i,j} (v_i - m_{v_i})^2(h_j - m_{h_j})^2] \nonumber \\
&amp; \phantom{=} + \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\sum_{\substack{i,j,k,l \\ (i, j) \neq (k, l)}} w_{i,j} w_{k,l} (v_i - m_{v_i})(h_j - m_{h_j})  \nonumber \\
&amp; \phantom{= + \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[\sum_{\substack{i,j,k,l \\ (i, j) \neq (k, l)}}} * (v_k - m_{v_k})(h_l - m_{h_l})] \nonumber \\
&amp; = \sum_{i,j} w^2_{i,j} \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[ (v_i - m_{v_i})^2]}_{= \text{ the variance of } v_i} \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_j - m_{h_j})^2]}_{= \text{ the variance of } h_j} \nonumber \\
&amp; \phantom{=} + \sum_{i \neq k, j \neq l} w_{i,j} w_{k,l} \Big(\underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(v_i - m_{v_i})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i \neq k, j \neq l} w_{i,j} w_{k,l} \Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_j - m_{h_j})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i \neq k, j \neq l} w_{i,j} w_{k,l} \Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(v_k - m_{v_k})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i \neq k, j \neq l} w_{i,j} w_{k,l} \Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_l - m_{h_l})]}_{= 0} \Big) \nonumber \\
&amp; \phantom{=} + \sum_{i, j \neq l} w_{i,j} w_{i,l} \Big( \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(v_i - m_{v_i})^2] \nonumber \\
&amp; \phantom{= + \sum_{i, j \neq l} w_{i,j} w_{i,l} \Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_j - m_{h_j})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i, j \neq l} w_{i,j} w_{i,l} Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_l - m_{h_l})]}_{= 0}\Big) \nonumber \\
&amp; \phantom{=} + \sum_{i \neq k, j} w_{i,j} w_{k,j} \Big(\underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(v_i - m_{v_i})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i \neq k, j} w_{i,j} w_{k,j} \Big(} * \underbrace{\mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(v_k - m_{v_k})]}_{= 0} \nonumber \\
&amp; \phantom{= + \sum_{i \neq k, j} w_{i,j} w_{k,j} \Big(} * \mathbb{E}_{P_{0, m, \lambda^\ast(0)}}[(h_j - m_{h_j})^2] \Big) \nonumber \\
&amp; = \sum_{i,j} w^2_{i,j} (m_{v_i} - m_{v_i}^2) (m_{h_j} - m_{h_j}^2) \label{eq:second_order_coef} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Surprisingly, the coefficent of the second-order term is just as compact as \eqref{eq:second_order_coef} is. Now that all the ingredients are ready, the complete expression of \eqref{eq:Taylor_expansion_with_unknown_coefficients} can be obtained by the substitution of \eqref{eq:zeroth_order_coef}, \eqref{eq:first_order_coef}, and \eqref{eq:second_order_coef}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; log(\hat{Z}(\beta, m, \lambda^\ast(\beta))) \\ 
&amp; \approx A(\beta, m) \\
&amp; \phantom{\approx} -\sum_i m_{v_i}log(m_{v_i}) + (1 - m_{v_i})log(1-m_{v_i}) \\
&amp; \phantom{\approx} - \sum_j m_{h_j}log(m_{h_j}) + (1 - m_{h_j})log(1-m_{h_j}) \\
&amp; \phantom{\approx} \; + \big(\sum_i a_i m_{v_i} + \sum_j b_j m_{h_j} + \sum_{i, j} w_{i,j} m_{v_i} m_{h_j}\big) \beta  \\
&amp; \phantom{\approx} \; + \big(\sum_{i,j} w^2_{i,j} (m_{v_i} - m_{v_i}^2) (m_{h_j} - m_{h_j}^2)\big) \frac{\beta^2}{2}
\end{aligned}
\label{eq:Taylor_expansion}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;step-3-&quot;&gt;Step 3 &lt;a name=&quot;approximation_step3&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;As stated above, the goal at &lt;a href=&quot;#approximation_step3&quot;&gt;step 3&lt;/a&gt; is to find the stationary points of the approximate function, $A(\beta, m)$, while $\beta$ is treated as a constant which is fixed to some value. Recall that at the beginning of the derivation the relaxation introduced has made $\beta$ a free variable instead of a constant 1; therefore to match the setting of the true objective, the particular choice of $\beta = 1$ is used. After $\beta$ is set, the remaining work is to take the derivatives of $A(\beta, m)$ with respect to each $m_{v_i}$ and $m_{h_j}$, which are the only variables left in $A$, and assigning them to 0 in an attempt to acquire a solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \frac{A(\beta = 1, m)}{\partial m_{v_i}} \\
&amp; = -(log(m_{v_i}) - log(1 - m_{v_i})) + a_i \\
&amp; \phantom{=} + \sum_j w_{i,j} m_{h_j} + \sum_j w_{i,j}^2 (\frac{1}{2} - m_{v_i})(m_{h_j} - m_{h_j}^2) = 0
\end{aligned}
\label{eq:d_A_d_m_vi}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \frac{A(\beta = 1, m)}{\partial m_{h_j}} \\
&amp; = -(log(m_{h_j}) - log(1 - m_{h_j})) + b_j \\
&amp; \phantom{=} + \sum_i w_{i,j} m_{v_i} + \sum_i w_{i,j}^2 (m_{v_i} - m_{v_i}^2) (\frac{1}{2} - m_{h_j}) = 0
\end{aligned}
\label{eq:d_A_d_m_hj}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Calling it an “attempt” is because, while \eqref{eq:d_A_d_m_vi} and \eqref{eq:d_A_d_m_hj} do not look very complicated, there is no closed-form solution for each $m_{v_i}$ and $m_{h_j}$; nevertheless, those equations still provide the information about what rules should be obeyed at the stationary point. For example, through some rearrangement of \eqref{eq:d_A_d_m_vi}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \frac{A(\beta = 1, m)}{\partial m_{v_i}} \nonumber \\
&amp; = -(log(m_{v_i}) - log(1 - m_{v_i})) + a_i \nonumber \\
&amp; \phantom{=} + \sum_j w_{i,j} m_{h_j} + \sum_j w_{i,j}^2 (\frac{1}{2} - m_{v_i})(m_{h_j} - m_{h_j}^2) = 0 \nonumber \\
&amp; \implies log(\frac{m_{v_i}}{1 - m_{v_i}}) = a_i + \sum_j w_{i,j} m_{h_j} \nonumber \\
&amp; \phantom{\implies log(\frac{m_{v_i}}{1 - m_{v_i}}) =}	+ \sum_j w_{i,j}^2 (\frac{1}{2} - m_{v_i})(m_{h_j} - m_{h_j}^2) \nonumber \\
&amp; \implies m_{v_i} = sigmoid\big(a_i + \sum_j w_{i,j} m_{h_j} \nonumber \\ 
&amp; \phantom{\implies m_{v_i} = sigmoid\big(} + \sum_j w_{i,j}^2 (\frac{1}{2} - m_{v_i})(m_{h_j} - m_{h_j}^2)\big) \label{eq:cc_m_vi} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;a constraint \eqref{eq:cc_m_vi}, which fits the role of $m_{v_i}$ as the mean of the marginal distribution of $v_i$, is established (constraints for all other $m$ variables can be derived similarly). It specifies that $m_{v_i}$ will stay unchanged, as though it converges, under the transformation involing all other $m_{h_j}$. The researchers who came up with this idea of approximation then drew an analogy between such constraints and the self-consistency rules in &lt;strong&gt;B&lt;/strong&gt;elief &lt;strong&gt;P&lt;/strong&gt;ropagation, based on which they proposed that one can update all $m_{v_i}$ and all $m_{h_j}$ alternatively and iteratively by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{cases}
\begin{aligned}
&amp; m_{v_i} = sigmoid\big(a_i + \sum_j w_{i,j} m_{h_j} \\
&amp; \phantom{m_{v_i} = sigmoid\big(} + \sum_j w_{i,j}^2 (\frac{1}{2} - m_{v_i})(m_{h_j} - m_{h_j}^2)\big) \\
\end{aligned} \\
\begin{aligned}
&amp; m_{h_j} = sigmoid\big(b_j + \sum_i w_{i,j} m_{v_i} \\
&amp; \phantom{m_{h_j} = sigmoid\big(} + \sum_i w_{i,j}^2 (m_{v_i} - m_{v_i}^2)(\frac{1}{2} - m_{h_j})\big) \\
\end{aligned}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;to reach a steady state where they stop changing (have converged), starting from randomly (or with some heuristics) initialised $m_{v_i}$ and $m_{h_j}$. Once $m$ variables converge, say to some $m^\ast$, $m^\ast$ will respect all contraints required for being a stationary point, and the whole process is finished by taking $m^\ast$ as a solution and evaluating $A(\beta = 1, m^\ast)$ to produce the approximate value for \eqref{eq:log_Z}.&lt;/p&gt;

&lt;h2 id=&quot;intuition&quot;&gt;Intuition&lt;/h2&gt;

&lt;p&gt;As other researchers have pointed out, this method can be interpreted as an extension of &lt;em&gt;mean-field approximation&lt;/em&gt;, with the factorised marginal distributions $P_f(v)$ and $P_f(h)$, which are Bernoulli in the example above, being defined by $m_{v_i}$ and $m_{h_j}$. To see that, considering the case where &lt;em&gt;Taylor expansion&lt;/em&gt; is conducted only up to the first-order term, the approximation function, named $A_1(\beta = 1, m)$, can be converted into an interesting form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; A_1(\beta = 1, m) \nonumber \\
&amp; = -\sum_i m_{v_i}log(m_{v_i}) + (1 - m_{v_i})log(1-m_{v_i}) \nonumber \\
&amp; \phantom{=} - \sum_j m_{h_j}log(m_{h_j}) + (1 - m_{h_j})log(1-m_{h_j})  \nonumber \\
&amp; \phantom{=} + \big(\sum_i a_i m_{v_i} + \sum_j b_j m_{h_j} + \sum_{i, j} w_{i,j} m_{v_i} m_{h_j}\big)  \nonumber \\
&amp; = - \sum_v P_f(v) log(P_f(v)) - \sum_h P_f(h) log(P_f(h)) \nonumber \\
&amp; \phantom{=} + \mathbb{E}_{P_f(v, h)}[\sum_i a_i v_i + \sum_j b_j h_j + \sum_{i, j} w_{i,j} v_i h_j]  \nonumber \\
&amp; = - \sum_{v, h} P_f(v) * P_f(h) log(P_f(v) * P_f(h)) \nonumber \\ 
&amp; \phantom{=} + \mathbb{E}_{P_f(v, h)}[log(exp(-E(v, h))) - log(Z)] + log(Z)  \nonumber \\
&amp; = \mathbb{E}_{P_f(v, h)}[log(\frac{P(v, h)}{P_f(v, h)})] + log(Z) \nonumber \\ 
&amp; = -KL[P_f(v, h)\|P(v, h)] + log(Z) \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus taking the derivatives of $A_1(\beta = 1, m)$ with respect to $m_{v_i}$ and $m_{h_j}$ and setting them to 0 is equivalent to doing it with $KL[P_f(v, h)|P(v, h)]$ (as log(Z) is a constant), and the latter one is exactly the step for finding the parameters in &lt;em&gt;mean-field approximation&lt;/em&gt;; such a link is also true when the &lt;em&gt;Taylor Expansion&lt;/em&gt; is taken to a higher order, where extra terms, which serve as correction of error from the perspective of &lt;em&gt;Taylor expansion&lt;/em&gt;, will be present together with $KL[P_f(v, h)|P(v, h)]$ to mount some tweaks and thus imporve the approximation result.&lt;/p&gt;

&lt;p&gt;This interpretation also shows a further insight into the roles which $m$ variables plays: they are the parameters needed to describe the marginal distributions of $v$ and $h$ when each of them is treated as being independent of one another. So if it is binary variables that are being modelled, there will be $m_{v_i}$ and $m_{h_j}$ introduced acting as means to specify each factorised Bernoulli distribution, just as seen in the example; on the other hand, if the energy function involves continuous variables, such as the &lt;em&gt;RBM&lt;/em&gt; with Gaussian visible units:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; E_g(v, h) = \sum_i \frac{(v_i - \mu_i)^2}{2\sigma_i^2} - \sum_j b_j h_j - \sum_{i,j} w_{i,j} \frac{v_i}{\sigma_i} h_j
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;then the helping function $G_g$ in this case will become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; G_g(v, h, m, \lambda(\beta)) \\
&amp; = -\sum_i \lambda_{v_{i_1}}(\beta) (v_i - m_{v_{i_1}}) + \sum_i \lambda_{v_{i_2}}(\beta) (v_i^2 - m_{v_{i_2}}) \\
&amp; \phantom{=} - \sum_j \lambda_{h_j}(\beta) (h_j - m_{h_j})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where an additional $\lambda_{v_{i_2}}(\beta) (v_i^2 - m_{v_{i_2}})$ is set up for each $v_i$ to parameterise their second moments, as defining a univariate Gaussian distribution takes a mean (the first moment) and a variance (the second moment - the first moment squared). In case you wonder how those extra terms works in forming Gaussian marginals, recall that when $\beta = 0$, $v$ and $h$ turn independent in the modified distribution $P_g$ described by $\hat{E}_g(\beta, v, h, m, \lambda(\beta)) = \beta E_g(v, h) + G_g(v, h, m, \lambda(\beta))$, from which the factorised marginals can be uncovered:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P_{g_{\beta = 0, m, \lambda(\beta = 0)}}(v_i) \\
&amp; = \frac{\int_{v_{\neg i}}\sum_h exp(-G_g(v, h, m, \lambda(0)))}{\int_v \sum_h exp(-G_g(v, h, m, \lambda(0)))} \\
&amp; = \frac{exp(\lambda_{v_{i_1}}(0) (v_i - m_{v_{i_1}}) - \lambda_{v_{i_2}}(0) (v_i^2 - m_{v_{i_2}}))}{\int_{v_i} exp(\lambda_{v_{i_1}}(0) (v_i - m_{v_{i_1}}) - \lambda_{v_{i_2}}(0) (v_i^2 - m_{v_{i_2}}))} \\
&amp; = \frac{exp(-\lambda_{v_{i_1}}(0) m_{v_{i_1}} + \lambda_{v_{i_2}}(0) m_{v_{i_2}})}{exp(-\lambda_{v_{i_1}}(0) m_{v_{i_1}} + \lambda_{v_{i_2}}(0) m_{v_{i_2}})} \\
&amp; \phantom{=} * \frac{exp(-\lambda_{v_{i_2}}(0)(v_i^2 - \frac{\lambda_{v_{i_1}}(0)}{\lambda_{v_{i_2}}(0)}v_i)) }{\int_{v_i} exp(-\lambda_{v_{i_2}}(0)(v_i^2 - \frac{\lambda_{v_{i_1}}(0)}{\lambda_{v_{i_2}}(0)}v_i))} \\
&amp; = \frac{exp(-\lambda_{v_{i_2}}(0)(v_i^2 - \frac{\lambda_{v_{i_1}}(0)}{\lambda_{v_{i_2}}(0)}v_i)) exp(-\frac{\lambda_{v_{i_1}}(0)^2}{4\lambda_{v_{i_2}}(0)} ) }{\int_{v_i} exp(-\lambda_{v_{i_2}}(0)(v_i^2 - \frac{\lambda_{v_{i_1}}(0)}{\lambda_{v_{i_2}}(0)}v_i)) exp(-\frac{\lambda_{v_{i_1}}(0)^2}{4\lambda_{v_{i_2}}(0)} )} \\
&amp; \propto exp(-\frac{1}{2} \frac{(v_i - \frac{\lambda_{v_{i_1}}(0)}{2\lambda_{v_{i_2}}(0)})^2}{\frac{1}{2\lambda_{v_{i_2}}(0)}}) \\
&amp; \implies v_i \sim N(\frac{\lambda_{v_{i_1}}(0)}{2\lambda_{v_{i_2}}(0)}, \frac{1}{2\lambda_{v_{i_2}}(0)})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;See, it is Gaussian!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02914&quot;&gt;Marylou Gabrie, Eric W Tramel, and Florent Krzakala. “Training Restricted Boltzmann Ma-chine via the Thouless-Anderson-Palmer free energy”. In:Advances in Neural InformationProcessing Systems. Ed. by C. Cortes et al. Vol. 28. Curran Associates, Inc., 2015, pp. 640–648&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nerdwisdom.files.wordpress.com/2007/10/ja910924.pdf&quot;&gt;A Georges and J S Yedidia. “How to expand around mean-field theory using high-temperatureexpansions”. In:Journal of Physics A: Mathematical and General24.9 (May 1991), pp. 2173–2192.doi:10.1088/0305-4470/24/9/024&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 25 Oct 2020 00:00:00 +0800</pubDate>
        <link>/Blogs/note/ThermalExpansion.html</link>
        <guid isPermaLink="true">/Blogs/note/ThermalExpansion.html</guid>
        
        <category>Probability</category>
        
        <category>Calculus</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Useful Facts about Gaussian Distributions</title>
        <description>&lt;p&gt;Whenever there is a need to conduct a literature review in the field of machine learning, it seems inevitable to revisit some theorems or algorithms involving Gaussian distributions at some point. This is not only due to the fact that data in many real-world cases do tend to demonstrate themselves (approximately) in this way, but there is also an advantage in that it is analytically friendly, which makes the derivation of closed-form solutions easy. That analytical friendliness, however, sometimes does not hold for me because my lack of knowledge about several handy facts related to Gaussian distributions established by the ingenious minds of mathematicians. Therefore, to stand on the shoulders of giants, it was decided to go through some of them in detail, and this post records my informal and incomplete compilation of &lt;em&gt;Useful Facts about Gaussian Distributions&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h2&gt;
&lt;p&gt;Before delving into the main contents, recall that the probability density function (PDF) of a Gaussian distribution $N(\mu, \Sigma)$ for a random vector $X$ of a size $d$ by $1$ (so in multivariate form) is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(X = x) = \frac{1}{\sqrt{2 \pi * det(\Sigma)}} exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu)) \\ 
\end{aligned}
\label{eq:PDFOfGaussian}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;with the mean vector $\mu$ of a size $d$ by $1$ and the covariance matrix $\Sigma$ of a size $d$ by $d$. Since the integral of a PDF must be equal to 1, integrating \eqref{eq:PDFOfGaussian} with respect to $x$ would gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \int_x \frac{1}{\sqrt{2 \pi * det(\Sigma)}} * exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu)) dx \\
&amp; = \frac{1}{\sqrt{2 \pi * det(\Sigma)}} * \int_x exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu)) dx \\
&amp; = 1
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and that implies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \int_x exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu)) dx = \sqrt{det(2 \pi * \Sigma)}
\end{aligned}
\label{eq:integralOfUnnormalisedPDFOfGaussian}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;So the term $\sqrt{det(2 \pi * \Sigma)}$ actually plays the role of the normaliser in this PDF, and by removing it the unnormalised version of \eqref{eq:PDFOfGaussian} can be obtained:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(X = x) \propto exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu))
\end{aligned}
\label{eq:unnormalisedPDFOfGaussian}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that \eqref{eq:unnormalisedPDFOfGaussian} provides a convenient way of determining whether there is a Gaussian distribution: if a PDF of a random vector $X$ is said to be proportional to some function $f(x)$ for all $X = x$, where $f(x)$ is of the form of $exp(-\frac{1}{2} (x - \mu)^{T} \Sigma^{-1} (x - \mu))$, then without knowing the exact expression, what distribution does that PDF describe? $N(\mu, \Sigma)$!!&lt;/p&gt;

&lt;h2 id=&quot;facts&quot;&gt;Facts&lt;/h2&gt;

&lt;h4 id=&quot;1-the-marginals-and-conditionals-of-a-joint-guassian-distribution-are-both-gaussian-distributions&quot;&gt;1. The marginals and conditionals of a joint Guassian distribution are both Gaussian distributions&lt;/h4&gt;

&lt;p&gt;Specifically, given a random column vector $X$ composed of two sub vectors $Y$ and $Z$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; X = \begin{bmatrix}
Y \\
Z
\end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and $X \sim N(\mu, \Sigma)$, the unnormalised version of Gaussian PDF, $f(x) \propto P(X = x)$, can be written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; f(x) = exp \big( -\frac{1}{2} (
\begin{bmatrix}
y \\
z
\end{bmatrix}- \mu)^{T} \Sigma^{-1} (
\begin{bmatrix}
y \\
z
\end{bmatrix}- \mu) \big)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;in which $x$ is split into $\begin{bmatrix}y \\ z \end{bmatrix}$.&lt;/p&gt;

&lt;p&gt;That quadratic term can be further expanded using block multiplication:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; f(x) = exp \big( -\frac{1}{2} 
\begin{bmatrix}
y - \mu_Y \\
z - \mu_Z
\end{bmatrix} 
^{T} 
\begin{bmatrix}
M_{YY} &amp; M_{YZ} \\
M_{ZY} &amp; M_{ZZ}
\end{bmatrix}
\begin{bmatrix}
y - \mu_Y \\
z - \mu_Z
\end{bmatrix} \big) \\
&amp; \phantom{f(x)} = exp \Big(-\frac{1}{2} \big( y_c^T M_{YY} y_c + y_c^T M_{YZ} z_c \\ 
&amp; \phantom{f(x) = exp \Big(-\frac{1}{2}} + z_c^T M_{ZY} y_c + z_c^T M_{ZZ} z_c \big) \Big)
\end{aligned}
\label{eq:blockMultiplicationOfF_x}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \mu = \begin{bmatrix}
\mu_Y \\
\mu_Z
\end{bmatrix} \\
&amp; y_c = y - \mu_Y \\
&amp; z_c = z - \mu_Z \\
&amp; \Sigma^{-1} =
\begin{bmatrix}
M_{YY} &amp; M_{YZ} \\
M_{ZY} &amp; M_{ZZ}
\end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;To be able to continue the derivation, a trick devised by smart mathematicians needs to introduced here, which states if the symmetric matrix $\Sigma$ is also put into the block form similarly:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \Sigma = 
\begin{bmatrix}
\Sigma_{YY} &amp; \Sigma_{YZ} \\
\Sigma_{ZY} &amp; \Sigma_{ZZ}
\end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; M_{YY} = \Sigma_{YY}^{-1} + \Sigma_{YY}^{-1} \Sigma_{YZ} ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} \Sigma_{ZY} \Sigma_{YY}^{-1} \\
&amp; M_{YZ} = -\Sigma_{YY}^{-1} \Sigma_{YZ} ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} \\
&amp; M_{ZY} = M_{YZ}^T = - ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} \Sigma_{ZY} \Sigma_{YY}^{-1} \\
&amp; M_{ZZ} = ( \Sigma_{ZZ}^{-1} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;That statement is actually not hard to prove (although it is quite amazing to me that mathematicians could come up with this block form in the first place):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;One can set up a matrix $M$ as described, and do matrix multiplication to show $M \Sigma = I$&lt;/li&gt;
  &lt;li&gt;Since $\Sigma$ is symmetric, $(M \Sigma)^T = \Sigma M^T = I$, which implies $M = M (\Sigma M^T) = (M \Sigma) M^T = M^T$ ($M$ is symmetric)&lt;/li&gt;
  &lt;li&gt;As $M \Sigma = \Sigma M = I$, $M$ is $\Sigma^{-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this trick (and substituting $y_c^T M_{YZ} z_c$ for $z_c^T M_{ZY} y_c$), \eqref{eq:blockMultiplicationOfF_x} can be expanded to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; f(x) \nonumber \\
&amp; = exp \Big( -\frac{1}{2} \big( \nonumber \\
&amp; \phantom{=} y_c^T \Sigma_{YY}^{-1} y_c + y_c^T \Sigma_{YY}^{-1} \Sigma_{YZ} ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} \Sigma_{ZY} \Sigma_{YY}^{-1} y_c \nonumber \\
&amp; \phantom{=} - 2 * y_c^T \Sigma_{YY}^{-1} \Sigma_{YZ} ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} z_c \nonumber \\
&amp; \phantom{=} + z_c^T ( \Sigma_{ZZ}^{-1} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} z_c \nonumber \\
&amp; \phantom{=} \big) \Big) \nonumber \\
&amp; = exp \Big( \nonumber \\
&amp; \phantom{=} -\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c \nonumber \\
&amp; \phantom{=} - \frac{1}{2} \big( (y_c^T \Sigma_{YY}^{-1} \Sigma_{YZ}) ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} (\Sigma_{ZY} \Sigma_{YY}^{-1} y_c) \nonumber \\
&amp; \phantom{=} - 2 * (y_c^T \Sigma_{YY}^{-1} \Sigma_{YZ}) ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} z_c \nonumber \\
&amp; \phantom{=} + z_c^T ( \Sigma_{ZZ}^{-1} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} z_c \big) \nonumber \\ 
&amp; \phantom{=} \Big) \nonumber \\
&amp; = exp( -\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c ) * exp(g(z_c, y_c)) \label{eq:twoTermsOfF_x}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; g(z_c, y_c) \\
&amp; = -\frac{1}{2} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c)^T ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that in \eqref{eq:twoTermsOfF_x}, $f(x)$ is rearranged into two terms, only one of which ($exp(g(z_c, y_c))$) involves $z$. That gives an advantage of simplifying the integration when $z$ is marginalised out of the PDF $P(X = x) \propto f(x)$ to obtain $P(Y = y)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(Y = y) = \int_z P(X = x) dz \\
&amp; \phantom{P(y)} \propto \int_z f(x) dz = exp(-\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c) \int_z exp( g(z_c, y_c) ) dz
\end{aligned}
\label{eq:unsimplifiedPDF_Y}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The integral term in \eqref{eq:unsimplifiedPDF_Y}, if expanded, shows the exactly the same form as discussed in \eqref{eq:integralOfUnnormalisedPDFOfGaussian}, and therefore the result of that integration is known immediately:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \int_z exp( g(z_c, y_c) ) dz \\
&amp; = \int_z exp \big( \\
&amp; \phantom{\int_z} -\frac{1}{2} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c)^T ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c) \\
&amp; \phantom{\int_z exp} \big) dz \\
&amp; = \int_z exp(-\frac{1}{2} (z - \mu_{ZY} )^T ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} (z - \mu_{ZY})) dz \\
&amp; = \sqrt{det(2 \pi (\Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} ))} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mu_{ZY} = \mu_Z + \Sigma_{ZY} \Sigma_{YY}^{-1} y_c$. It is surprising that after integration, not only does $z$ disappear (which is just the purpose of marginalisation), but terms related to $y$ are also gone. So the integral turns out to be a constant with respect to $y$, which allows of further simpliying \eqref{eq:unsimplifiedPDF_Y} to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(Y = y) \\
&amp; \propto exp(-\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c) \sqrt{det(2 \pi (\Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} ))} \\
&amp; \propto exp(-\frac{1}{2} (y - \mu_Y)^T \Sigma_{YY}^{-1} (y - \mu_Y))
\end{aligned}
\label{eq:PDF_Y}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Seeing something familiar? Yes, the proportionality in \eqref{eq:PDF_Y} corresponds to the PDF of $N(\mu_Y, \Sigma_{YY})$, as per \eqref{eq:unnormalisedPDFOfGaussian}. Since $Y$ can be any vector whose entries are collected from $X$, that demonstrates any marginal $P(Y)$ of a Gaussian distribution $P(X)$ is also a Gaussian distribution, with a mean $\mu_Y$ and a covariance matrix $\Sigma_{YY}$.&lt;/p&gt;

&lt;p&gt;How about conditionals? Well, with $P(Y, Z) = P(X)$ and $P(Y)$ being defined, the probability of $Z$ given $Y$ can be derived from the definition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P(Z = z|Y = y) = \frac{P(Y = y, Z = z)}{P(Y = y)} = \frac{P(X = x = 
\begin{bmatrix}
y \\
z
\end{bmatrix}
)}{P(Y = y)} \\
&amp; \propto \frac{f(x)}{exp(-\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c)} \\
&amp; = \frac{exp(-\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c + g(z_c, y_c))}{exp(-\frac{1}{2} y_c^T \Sigma_{YY}^{-1} y_c)} \\
&amp; = exp( g(z_c, y_c) ) \\
&amp; = exp\big( \\
&amp; \phantom{\propto} -\frac{1}{2} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c)^T ( \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ} )^{-1} (z_c - \Sigma_{ZY} \Sigma_{YY}^{-1} y_c)\\
&amp; \phantom{\propto} \big) \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let $\mu_{Z|y} = \mu_Z + \Sigma_{ZY} \Sigma_{YY}^{-1}y_c = \mu_Z + \Sigma_{ZY} \Sigma_{YY}^{-1}(y - \mu_Y)$ and $\Sigma_{Z|Y} = \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ}$; $P(Z|Y)$ can be expressed in a neater way as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(Z = z|Y = y) \propto exp(-\frac{1}{2} (z - \mu_{Z|y})^T \Sigma_{Z|Y}^{-1}(z - \mu_{Z|y}))
\end{aligned}
\label{eq:PDF_ZGivenY}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is clear that the proportionality relating to Gaussian PDF emerges again, and hence it can be concluded that $Z|Y \sim N(\mu_{Z|y}, \Sigma_{Z|Y})$ for $Z$ and $Y$ consisting of $X$; in other words, any conditional $P(Z|Y)$ which has a joint Gaussian distribution $P(Y, Z) = P(X)$ is also a Gaussian distribution, with a mean $\mu_{Z|y} = \mu_Z + \Sigma_{ZY} \Sigma_{YY}^{-1} (y - \mu_Y)$ and a covariance matrix $\Sigma_{Z|Y} = \Sigma_{ZZ} - \Sigma_{ZY} \Sigma_{YY}^{-1} \Sigma_{YZ}$.&lt;/p&gt;

&lt;h4 id=&quot;-2-the-multiplication-of-a-random-vector-of-a-guassian-distribution-by-a-constant-not-equal-to-0-yields-a-random-vector-which-has-a-gaussian-distribution&quot;&gt;&lt;a name=&quot;fact2&quot;&gt;&lt;/a&gt; 2. The multiplication of a random vector of a Guassian distribution by a constant (not equal to 0) yields a random vector which has a Gaussian distribution&lt;/h4&gt;

&lt;p&gt;Suppose $X$ is a random vector of a Gaussian distribution $N(\mu, \Sigma)$. Multiplying $X$ by a constant $s \neq 0$ gives a new random vector $W = s*X$; or reversely one can map $W$ to $X$ with the relation $X = W / s$. As a result, $P(W)$ can be derived from $P(X)$ using the technique of &lt;strong&gt;Change of Variables&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(W = w) = P(X = x) * \lvert det(\frac{dx}{ds}) \rvert \\
&amp; \propto exp((\frac{w}{s} - \mu)^T \Sigma^{-1} (\frac{w}{s} - \mu)) * \lvert \frac{1}{s} \rvert^D \\
&amp; \propto exp((w - \frac{\mu}{s})^T \frac{1}{s^2}\Sigma^{-1} (w - \frac{\mu}{s})) * \lvert \frac{1}{s} \rvert^D
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $D$ stands for the number of entries in $x$ (or $s$). Since $s$ is a constant with respect to $w$, $\lvert \frac{1}{s} \rvert^D$ can be omitted in the proportionality, which yields:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; P(w = w) \propto exp((w - \frac{\mu}{s})^T \frac{1}{s^2}\Sigma^{-1} (w - \frac{\mu}{s}))
\end{aligned}
\label{eq:unnormalisedPDF_w}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here we go again! \eqref{eq:unnormalisedPDF_w} indicates $W \sim N(\frac{\mu}{s}, s^2 \Sigma)$, which shows that the multiplication of a random vector $X \sim N(\mu, \Sigma)$ by a constant $s \neq 0$ results in a new random vector whose PDF describes another Gaussian distribution, with a mean $\mu / s$ and a covariance matrix $s^2 \Sigma$.&lt;/p&gt;

&lt;h4 id=&quot;-3-the-sum-of-two-independent-random-vectors-of-guassian-distributions-yields-a-random-vector-which-also-has-a-gaussian-distribution&quot;&gt;&lt;a name=&quot;fact3&quot;&gt;&lt;/a&gt; 3. The sum of two independent random vectors of Guassian distributions yields a random vector which also has a Gaussian distribution&lt;/h4&gt;

&lt;p&gt;Let $X_1 \sim N(\mu_1, \Sigma_1)$ and $X_2 \sim N(\mu_2, \Sigma_2)$. The probability density of the sum of them, $W = X_1 + X_2$, can be interpreted as $P(W = w) = \int_{x_1} P(X_1 = x_1) * P(X_2 = w - x_1|X_1 = x_1) dx_1$. The independence between $X_1$ and $X_2$ suggests $P(X_2 = w - x_1 | X_1 = x_1) = P(X_2 = w - x_1)$, and hence the PDF of $W$ can be written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(W = w) \nonumber \\ 
&amp; = \int_{x_1} P(X_1 = x_1) * P(X_2 = w - x_1) dx_1 \nonumber \\
&amp; \propto \int_{x_1} exp(-\frac{1}{2} (x_1 - \mu_1)^T \Sigma_1^{-1} (x_1 - \mu_1)) \nonumber \\
&amp; \phantom{\propto \int_{x_1}} * exp(-\frac{1}{2} (w - x_1 - \mu_2)^T \Sigma_2^{-1} (w - x_1 - \mu_2)) dx_1 \nonumber \\ 
&amp; \propto \int_{x_1} exp(-\frac{1}{2} (x_1 - \mu_1)^T \Sigma_1^{-1} (x_1 - \mu_1)) \nonumber \\
&amp; \phantom{\propto \int_{x_1}} * exp(-\frac{1}{2} (x_1 - (w - \mu_2))^T \Sigma_2^{-1} (x_1 - (w - \mu_2))) dx_1 \label{eq:unnormalisedPDF_W_1}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The next step is to rearrage terms in \eqref{eq:unnormalisedPDF_W_1}, so that those relating to $x_1$ are put together for integration. Note that terms which are constants with respect to $w$ can be safely omitted, as they do not affect the proportionality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(W = w) \nonumber \\
&amp; \propto \int_{x_1} exp\Big( -\frac{1}{2} \big( \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} x_1^T \Sigma_1^{-1} x_1 + x_1^T \Sigma_2^{-1} x_1 \nonumber \\ 
&amp; \phantom{\propto \int_{x_1} exp(} - 2 x_1^T \Sigma_1^{-1} \mu_1  - 2 x_1^T \Sigma_2^{-1} (w - \mu_2) \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} \big) \Big) dx_1 \nonumber \\
&amp; \phantom{\propto} \; * exp(-\frac{1}{2} ( w - \mu_2 )^T \Sigma_2^{-1} ( w - \mu_2 )) \nonumber \\
&amp; = \int_{x_1} exp\Big( -\frac{1}{2} \big( \nonumber \\
&amp; \phantom{= \int_{x_1} exp(} x_1^T \Sigma_3^{-1} x_1 - 2 x_1^T \Sigma_3^{-1} \Sigma_3 \mu_3 + \mu_3^T \Sigma_3 \Sigma_3^{-1} \Sigma_3 \mu_3 \nonumber \\
&amp; \phantom{= \int_{x_1} exp(} \big) \Big) dx_1 \nonumber \\
&amp; \phantom{=} \; * exp(-\frac{1}{2} (-1) * (\mu_3^T \Sigma_3 \Sigma_3^{-1} \Sigma_3 \mu_3)) \nonumber \\
&amp; \phantom{=} \; * exp(-\frac{1}{2} ( w - \mu_2 )^T \Sigma_2^{-1} ( w - \mu_2 )) \nonumber \\
&amp; = \int_{x_1} exp( -\frac{1}{2} (x_1 - \Sigma_3 \mu_3)^T \Sigma_3^{-1} (x_1 - \Sigma_3 \mu_3)) dx_1 \nonumber \\
&amp; \phantom{=} \; * exp(-\frac{1}{2} (( w - \mu_2 )^T \Sigma_2^{-1} ( w - \mu_2 ) - \mu_3^T \Sigma_3 \mu_3)) \label{eq:unnormalisedPDF_W_2}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\Sigma_3^{-1} = \Sigma_1^{-1} + \Sigma_2^{-1}$, $\mu_3 = \Sigma_1^{-1} \mu_1 + \Sigma_2^{-1} (w - \mu_2)$. Now the integral with respect to $x_1$, as noted in \eqref{eq:integralOfUnnormalisedPDFOfGaussian}, can be read out directly as $\sqrt{det(2 \pi \Sigma_3)}$, which has nothing to do with $w$. Also the expansion of $\mu_3^T \Sigma_3 \mu_3$ gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \mu_3^T \Sigma_3 \mu_3 \\
&amp; = (\Sigma_1^{-1} \mu_1 + \Sigma_2^{-1} (w - \mu_2))^T \Sigma_3  (\Sigma_1^{-1} \mu_1 + \Sigma_2^{-1} (w - \mu_2)) \\ 
&amp; = (\Sigma_2^{-1} w - (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1))^T \Sigma_3  (\Sigma_2^{-1} w - (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1)) \\
&amp; = w^T \Sigma_2^{-1} \Sigma_3  \Sigma_2^{-1} w \\ 
&amp; \phantom{=} - 2 w^T \Sigma_2^{-1} \Sigma_3 (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1)) \\
&amp; \phantom{=} + (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1))^T \Sigma_3 (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1)) 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;in which the last bulk contains no $w$. By removing terms not involving $w$ in \eqref{eq:unnormalisedPDF_W_2}, the proportionality can be further simplified to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; P(W = w) \nonumber \\
&amp; \propto exp(-\frac{1}{2} (( w - \mu_2 )^T \Sigma_2^{-1} ( w - \mu_2 ) - \mu_3^T \Sigma_3 \mu_3)) \nonumber \\
&amp; \propto exp\Big(-\frac{1}{2} \big( \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} w^T \Sigma_2^{-1} w - 2 w^T \Sigma_2^{-1} \mu_2 \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} - w^T \Sigma_2^{-1} \Sigma_3  \Sigma_2^{-1} w \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} + 2 w^T \Sigma_2^{-1} \Sigma_3 (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1) \nonumber \\
&amp; \phantom{\propto \int_{x_1} exp(} \big) \Big) \nonumber \\
&amp; = exp(-\frac{1}{2} (w^T \Sigma_{sum}^{-1} w - 2 w^T \Sigma_{sum}^{-1} \mu_{sum})) \label{eq:unnormalisedPDF_W_3}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \Sigma_{sum}^{-1} \\
&amp; = \Sigma_2^{-1} - \Sigma_2^{-1} \Sigma_3  \Sigma_2^{-1} \\
&amp; = \Sigma_2^{-1} (I - (\Sigma_1^{-1} + \Sigma_2^{-1})^{-1} \Sigma_2^{-1}) \\
&amp; = \Sigma_2^{-1}(\Sigma_1^{-1} + \Sigma_2^{-1})^{-1} ((\Sigma_1^{-1} + \Sigma_2^{-1}) - \Sigma_2^{-1}) \\
&amp; = \Sigma_2^{-1}(\Sigma_1^{-1} + \Sigma_2^{-1})^{-1} \Sigma_1^{-1} \\
&amp; = (\Sigma_1 (\Sigma_1^{-1} + \Sigma_2^{-1}) \Sigma_2)^{-1} \\
&amp; = (\Sigma_1 + \Sigma_2)^{-1}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \mu_{sum} \\
&amp; = \Sigma_{sum} (\Sigma_2^{-1} \mu_2 - \Sigma_2^{-1} \Sigma_3 (\Sigma_2^{-1} \mu_2 - \Sigma_1^{-1} \mu_1)) \\
&amp; = \Sigma_{sum} (\Sigma_2^{-1} \mu_2 - \Sigma_2^{-1} \Sigma_3 \Sigma_2^{-1} \mu_2 + \Sigma_2^{-1} \Sigma_3 \Sigma_1^{-1} \mu_1) \\
&amp; = \Sigma_{sum} ((\Sigma_2^{-1} - \Sigma_2^{-1} \Sigma_3 \Sigma_2^{-1}) \mu_2 + \Sigma_2^{-1} \Sigma_3 \Sigma_1^{-1} \mu_1) \\
&amp; = \Sigma_{sum} (\Sigma_{sum}^{-1} \mu_2 + \Sigma_2^{-1} (\Sigma_1^{-1} + \Sigma_2^{-1})^{-1} \Sigma_1^{-1} \mu_1) \\
&amp; = \Sigma_{sum} (\Sigma_{sum}^{-1} \mu_2 + (\Sigma_1 (\Sigma_1^{-1} + \Sigma_2^{-1}) \Sigma_2)^{-1} \mu_1) \\
&amp; = \Sigma_{sum} (\Sigma_{sum}^{-1} \mu_2 + \Sigma_{sum}^{-1} \mu_1) \\
&amp; = \mu_2 + \mu_1
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is not a coincidence that the derivation arrives at $\Sigma_{sum} = \Sigma_1 + \Sigma_2$ and $\mu_{sum} = \mu_2 + \mu_1$, because this is what the covariance and mean should be for the sum of any two independent random vectors regardless of what distributions they represent! Finaly, since $\Sigma_{sum}^{-1}$ and $\mu_{sum}$ are constant with respect to $w$, \eqref{eq:unnormalisedPDF_W_3} can be tweaked slightly to demonstrate the form similar to \eqref{eq:unnormalisedPDFOfGaussian}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P(W = w) \\
&amp; \propto exp(-\frac{1}{2} (w^T \Sigma_{sum}^{-1} w - 2 w^T \Sigma_{sum}^{-1} \mu_{sum})) \\
&amp; \propto exp(-\frac{1}{2} (w^T \Sigma_{sum}^{-1} w - 2 w^T \Sigma_{sum}^{-1} \mu_{sum} + \mu_{sum}^T \Sigma_{sum}^{-1} \mu_{sum})) \\
&amp; = exp(-\frac{1}{2} (w - \mu_{sum})^T \Sigma_{sum}^{-1} (w - \mu_{sum}))
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which implies $W \sim N(\mu_{sum}, \Sigma_{sum})$. That just proves the sum of two independent Gaussian random vectors $X_1 \sim N(\mu_1, \Sigma_1)$ and $X_2 \sim N(\mu_2, \Sigma_2)$ is indeed a random vector of a Gaussian distribution; and moreover, its mean is $\mu_1 + \mu_2$ and its covariance matrix is $\Sigma_1 + \Sigma_2$.&lt;/p&gt;

&lt;h4 id=&quot;4--the-linear-combination-of-independent-random-vectors-of-guassian-distributions-yields-a-random-vector-which-also-has-a-gaussian-distribution&quot;&gt;4. &lt;a name=&quot;fact4&quot;&gt;&lt;/a&gt; The linear combination of independent random vectors of Guassian distributions yields a random vector which also has a Gaussian distribution&lt;/h4&gt;

&lt;p&gt;This is the immediate result from &lt;a href=&quot;#fact2&quot;&gt;fact 2&lt;/a&gt; and &lt;a href=&quot;#fact3&quot;&gt;fact 3&lt;/a&gt;, as every linear combination can been viewed as a sequence of operations involving constant multiplication and sum only; every operation conducted yields a new Gaussian random vector, which is independent of the rest of vectors not involved in that operation, and is used as the operand to the following operator to create another Gaussian random vector; consequently, &lt;a href=&quot;#fact2&quot;&gt;fact 2&lt;/a&gt; and &lt;a href=&quot;#fact3&quot;&gt;fact 3&lt;/a&gt; always hold through out the sequence and the the last random vector produced will be of a Gaussian distribution.&lt;/p&gt;

&lt;p&gt;On a side note, given $n$ independent Gaussian random column vectors $X_1, \dotsc, X_n$ and the combination weights $s_1, \dotsc, s_n$, such a linear combination can be expressed as (in terms of matrix multiplication):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; 
\begin{bmatrix}
X_1, \dotsc, X_n
\end{bmatrix}
\begin{bmatrix}
s_1 \\
\vdots \\
s_n
\end{bmatrix}
\end{aligned}
\label{eq:linearCombination}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;5-the-linear-transformation-of-a-random-vector-of-a-guassian-distribution-by-a-invertible-mapping-yields-a-random-vector-which-also-has-a-gaussian-distribution&quot;&gt;5. The linear transformation of a random vector of a Guassian distribution by a invertible mapping yields a random vector which also has a Gaussian distribution&lt;/h4&gt;

&lt;p&gt;Equivalently, this fact states that the left multiplication of a Gaussian random vector $X$ of a size $d * 1$ by a invertible matrix $A$ of a size $d * d$ produces another Gaussian random vector $U = AX$ (note that do not be confused with \eqref{eq:linearCombination} in &lt;a href=&quot;#fact4&quot;&gt;fact 4&lt;/a&gt;). Since the invertibility of $A$ implies $X = A^{-1}U$, with &lt;strong&gt;Change of Variables&lt;/strong&gt; one can derive the PDF of $U$ from the PDF of $X \sim N(\mu_X, \Sigma_X)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P(U = u) = P(X = x) * \lvert det(\frac{dx}{du}) \rvert \\
&amp; = P(X = x) * \lvert \frac{1}{det(A)} \rvert \\
&amp; \propto exp(-\frac{1}{2} (x - \mu_X)^T \Sigma_X^{-1} (x - \mu_X)) \\
&amp; \propto exp(-\frac{1}{2} (A^{-1}u - \mu)^T \Sigma_X^{-1} (A^{-1}u - \mu_X)) \\
&amp; \propto exp(-\frac{1}{2} (u - A \mu_X)^T {A^{-1}}^T \Sigma_X^{-1} A^{-1} (u - A \mu_X)) \\
&amp; \propto exp(-\frac{1}{2} (u - \mu_U)^T \Sigma_U^{-1} (u - \mu_U))
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mu_U = A \mu_X$ and $\Sigma_U = A \Sigma_X A^T$. Again by \eqref{eq:unnormalisedPDFOfGaussian}, $U$ has a Gaussian distribution $N(\mu_U, \Sigma_U)$, which leads to the conclusion that the random vector induced by some invertible linear transformation (defined by a square matrix $A$) on a random vector of $N(\mu_X, \Sigma_X)$ is still Gaussian, but with a new mean $A \mu_X$ and a new covariance matrix $A \Sigma_X A^T$.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html&quot;&gt;http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs229.stanford.edu/summer2020/more_on_gaussians.pdf&quot;&gt;http://cs229.stanford.edu/summer2020/more_on_gaussians.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 07 Aug 2020 00:00:00 +0800</pubDate>
        <link>/Blogs/note/Gaussian-Distribution.html</link>
        <guid isPermaLink="true">/Blogs/note/Gaussian-Distribution.html</guid>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Looking into Label Smoothing</title>
        <description>&lt;p&gt;I have forgotten where I obtained this impression about label smoothing: “Since data could be mislablled, to reflect the possibility of errors, labels are weighted to reduce our confidence about them.” While that description seems to present a straightforward intuition, it does not tell the full story, which I came to realise a few days ago after revisited this technique. So here are the missing pieces I have found.&lt;/p&gt;

&lt;h2 id=&quot;regularisation&quot;&gt;Regularisation&lt;/h2&gt;
&lt;p&gt;Given a label $L$ represented as a one-hot vector $L_{\text{one-hot}}$ and the total number of classes $K$, the authors of label smoothing defined the smoothed vector $L_{\text{ls}}$ by a small proportion $\alpha$ as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; L_{\text{ls}}[k] = (1 - \alpha) * L_{\text{one-hot}} [k] + \frac{\alpha}{K} \\
&amp; where\,\,L_{\text{one-hot}}[k] =
\begin{cases}
1,&amp; L = k \\
0,&amp; otherwise
\end{cases}
\end{aligned}
\label{eq:labelSmoothing}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Most probability models fitting categorical distributions are optimised towards a lower expected cross entropy between model estimations and labels. Suppose $P$ is a vector of probabilities for different classes outputted by a model; with the notations from \eqref{eq:labelSmoothing}, the cross entropy $E_{\text{one-hot}}$ between $L_{\text{one-hot}}$ and $P$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; E_{\text{one-hot}} = - \sum_{i=0}^K L_{\text{one-hot}} [i] * log(P[i]) = log(P[k]), L = k
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the one-hot version of labels is replaced with the smoothed one, a different version of cross entropy, $E_{\text{ls}}$ can be obtained:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; E_{\text{ls}} = - \sum_{i=0}^K L_{\text{ls}} [i] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = - \sum_{i=0}^K [(1 - \alpha) * L_{\text{one-hot}} [i] + \frac{\alpha}{K}] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * (-1) * \sum_{i=0}^K L_{\text{one-hot}} [i] * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}} = } - \alpha * \sum_{i=0}^K \frac{1}{K} * log(P[i]) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * E_{\text{one-hot}} - \alpha * \sum_{i=0}^K \frac{1}{K} * (log(P[i]) - log(\frac{1}{K})) \\
&amp; \phantom{ E_{\text{ls}} = } - \alpha * \sum_{i=0}^K \frac{1}{K} log(\frac{1}{K}) \\
&amp; \phantom{ E_{\text{ls}}} = (1 - \alpha) * E_{\text{one-hot}} + \alpha * KL[\text{Uniform}_K \lVert P] \\
&amp; \phantom{ E_{\text{ls}} = } + \alpha * Entropy(\text{Uniform}_K)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $Entropy(\text{Uniform}_K)$ is a constant, the minimisation of $E_{\text{ls}}$ for a model effectively achieves two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Minimise $E_{\text{one-hot}}$, which is the original optimisation objective of the model, with a weight of $ (1 - \alpha)$.&lt;/li&gt;
  &lt;li&gt;Minimise $KL[\text{Uniform}_K \lVert P]$, which serves as a &lt;strong&gt;regularisor&lt;/strong&gt; to bring the distribution of $P$ towards the one of $\text{Uniform}_K$, with a weight of $\alpha$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So “reduce confidence” can be seen as the result of regularisation which causes models to produce flatter distributions.&lt;/p&gt;

&lt;h2 id=&quot;calibration&quot;&gt;Calibration&lt;/h2&gt;

&lt;p&gt;When doing classification, one will usually use softmax as the final step to produce a vector of the length equal to the number of classes. Despite the fact that all entries in that vector add up to 1, it does not necessarily mean that vector (approximately) describes a distribution of classes. For example, if the result of its two-class classification is $[0.9, 0.1]$, in the case of &lt;strong&gt;uncalibrated models&lt;/strong&gt; that only implies the first class receives a higher (normalised) score, and hence has a higher rank, than the second class does. As for the purpose of mere classification, ranking does do the job; nonetheless, there are some scenarios where distributions of classes are required, so that the accuracy of decisions (the probability that a chosen class is correct) can be estimated, or samples following the same distributions can be generated. To that end, we need to &lt;strong&gt;calibrate&lt;/strong&gt; our model, and one way to achieve that is to apply label smoothing during training.&lt;/p&gt;

&lt;p&gt;But, why does label smoothing have such power? A recent paper has presented an interesting insight into the penultimate layer, right before softmax, of classification models, and I would like to add my own interpretation based on their finding to offer some intuition.&lt;/p&gt;

&lt;p&gt;First, in most cases, a penultimate layer only involves a linear transformation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
y_k(x) = x^T \odot w_k
\end{aligned}&lt;/script&gt;

&lt;p&gt;where $x$ is the input to the penultimate layer, and $w_k$ describes the linear mapping which produces the logit for class $k$. The softmax output for class $k$ given $x$, $s_k(x)$, can then be expressed in terms of $x^T \odot w_k$ (assuming the total number of classes is $K$).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\begin{aligned}
s_k(x) = \frac{exp(x^T \odot w_k)}{\sum_{i=1}^{K} exp(x^T \odot w_i)}
\end{aligned}
\label{eq:softmax}
\end{equation}&lt;/script&gt;

&lt;p&gt;Since the multiplication of both numerator and denominator in \eqref{eq:softmax} by the same constant factors will not change the ratio, \eqref{eq:softmax} can be further derived as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; s_k(x) = \frac{exp(-0.5 \lVert x \rVert_2^2) * exp(-0.5 \lVert w_{max} \rVert_2^2)}{exp(-0.5 \lVert x \rVert_2^2) * exp(-0.5 \lVert w_{max} \rVert_2^2)} \\
&amp; \phantom{s_k(x) =} * \frac{exp(x^T \odot w_k)}{\sum_{i=1}^{K} exp(x^T \odot w_i)} \\
&amp; \phantom{s_k(x)} = \frac{exp(-0.5 \lVert x \rVert_2^2 + x^T \odot w_k - 0.5 \lVert w_{max} \rVert_2^2)}{\sum_{i=1}^{K} exp(-0.5 \lVert x \rVert_2^2 + x^T \odot w_i – 0.5 \lVert w_{max} \rVert_2^2)} \\
&amp; \phantom{s_k(x)} = \frac{exp(-0.5 * (\lVert x \rVert_2^2 - 2 * x^T \odot w_k + \lVert w_{max} \rVert_2^2))}{\sum_{i=1}^{K} exp(-0.5 * (\lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{max} \rVert_2^2))} \\
&amp; \phantom{s_k(x)} = \frac{exp(-0.5 * z(x, w_k))}{\sum_{i=1}^{K} exp(-0.5 * z(x, w_i))}
\end{aligned}
\label{eq:softmax2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $w_{max}$ refers to the vector with the largest length among all $w_i$, $i \in \{ j \lvert j \in \mathbb{Z}, 0 \leq j \leq K \}$; also the last equality is put in for simplification, by letting $z(x, w) = \lVert x \rVert_2^2 - 2 * x^T \odot w + \lVert w_{max} \rVert_2^2$.&lt;/p&gt;

&lt;p&gt;One might notice a high degree of similarity between the terms in the function $z$ and the ones which would have been involved in a L2 norm. In effect, they can be linked through the following inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; z(x, w_i) = \lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{max} \rVert_2^2 \\
&amp; \phantom{z(x, w_i)} \geq \lVert x \rVert_2^2 - 2 * x^T \odot w_i + \lVert w_{i} \rVert_2^2 \\
&amp; \phantom{z(x, w_i)} = \lVert x - w_i \rVert_2^2 \geq 0
\end{aligned}
\label{eq:L2Inequality}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Meanwhile, \eqref{eq:softmax2} also suggests that a model trained with some example from class $k$ of which the penultimate representation is $x$ will produce small $z(x, w_k)$ and large $z(x, w_l)$ for all other classes $l \neq k$, so that the distinct differences between them can result in dominant $s_k(x)$ with tiny $s_l(x)$ to reflect the labelling. Since according to \eqref{eq:L2Inequality}, the function $z$ describes an upper bound of the L2 norm squared of the difference between its two input arguments, it can be further inferred that $\lVert x - w_k \rVert_2^2$ should be small while there is room for $\lVert x - w_l \rVert_2^2$ to grow. From a geometric perspective, if $x$ and $w_k$/$w_l$ are viewed as points in a hyperspace, the relation, in terms of bounds of L2 norms, between those points loosely describes a form of clustering, with cluster centres being $w_k$/$w_l$.&lt;/p&gt;

&lt;figure&gt;
	
	&lt;img src=&quot;/Blogs/assets/images/2020-03-09-LabelSmoothing/PenultimateClustering.png&quot; alt=&quot;A schematic diagram for the geometric view of $x$ and $w_k$/$w_l$&quot; /&gt;
	&lt;figcaption&gt;
	A schematic diagram for the geometric view of $x$ and $w_k$/$w_l$
	&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This phenomenon of clustering is one of the discoveries presented in the aforementioned paper; besides, the authors also explored the effect of label smoothing to the widths of clusters, on which my interpretation is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When a model is trained given labels in one-hot encoding, $z(x, w_l)$ can become quite large as $s_l(x)$ is approaching 0. In fact, there is no theoretical upper bound for it; at the same time, rising $z(x, w_l)$ will also boost $s_k(x)$, so $z(x, w_k)$ does not have to be very small to generate a dominant response of softmax for class $k$. That potential increase in $z(x, w_k)$, which corresponds to the upper bounds of the distances between $x$ and $w_k$, suggests the possible presence of broad clusters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The use of label smoothing essentially limits the growth of $z(x, w_l)$ so that the resulting $s_l(x)$ can account for the slight probability assigned to class $l$. That also makes minimisation of $z(x, w_k)$ more necessary as $s_k(x)$ cannot be further increased by unboundedly large $z(x, w_l)$. Therefore, in contrast to the case where one-hot encoding is directly employed, clusters are more likely to be tighter due to a smaller distance bound $z(x, w_k)$ under the effect of label smoothing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the advantanges of connecting penultimate layers with clustering is that it offers a different way to understand the subsequent softmax. The equivalent form of the softmax, \eqref{eq:softmax2}, can now be seen an approximation of the posterior probabilities of classes given data under a Gaussian Mixture model, with the assumption that the covariance matrix of every Gaussian is $I * \sigma^2$, $\sigma^2 = 1$, and the prior for classes is a uniform distribution. That assumption, $\sigma^2 = 1$ in particular, implies the probability model is not suitable for data coming from broad clusters, since it cannot estimate their posterior probabilities very well, or to put it differently, is &lt;strong&gt;uncalibrated&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, without label smoothing, the clusters formed by the inputs to the penultimate layer could end up with large widths. In that case, bad calibration occurs because the distribution of those inputs cannot be fitted properly under the model assumption of $\sigma^2 = 1$; this analysis might also explain why previous researches have found temperature scaling, which divides the logits to the softmax function by a constant named temperature $T$, can help to calibrate models:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
s_k^{\text{TS}}(x) = \frac{exp(\frac{x^T \odot w_k}{T})}{\sum_{i=1}^{K} exp(\frac{x^T \odot w_i}{T})}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Since it is equivalent to dividing the function $z$ by $T$ in \eqref{eq:softmax2}, the scaling in effect adjusts $\sigma^2$ so that the sizes of clusters can be better described. Similarly, label smoothing provides the effect of improving the degree of calibration by matching the probability model to the underlying data as temperature scaling does, but it achieves that through encouraging tighter clusters, and hence increasing the validity of $\sigma^2 = 1$.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.02629&quot;&gt;Rafael Müller, Simon Kornblith, and Geoffrey E.Hinton. “When Does Label Smoothing Help?” In:CoRRabs/1906.02629 (2019). arXiv:1906.02629.url:http://arxiv.org/abs/1906.02629&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 09 Mar 2020 00:00:00 +0800</pubDate>
        <link>/Blogs/note/LabelSmoothing.html</link>
        <guid isPermaLink="true">/Blogs/note/LabelSmoothing.html</guid>
        
        <category>MachineLearning</category>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>The Probability of Winning a Lottery</title>
        <description>&lt;p&gt;For some time I have been reading an interesting book, in which the author analysed real-world events in our daily life from the perspective of statistics and probability. One of its topics, which no doubt relates to the two subjects, is gambling, and there is a section discussing about the strategy of playing a lottery. I have always heard that the chance of winning the top prize of a lottery is less the one of being struck by lightning (at least for the lottery held in Taiwan), so I have not bothered to pay even slight attention to it. Nonetheless, after having read that section describing a statistically reasonable approach to decide the timing of buying lottery tickets, I started wondering maybe winning a prize does not have to be all on the off chance.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;statistically reasonable approach&lt;/em&gt; is actually pretty simple: it is to calculate the expectation of money returned per ticket. Taking one of the major lotteries for example, the game is to pick 6 different numbers from 1 to 49, and based on how many your numbers match the randomly-drawn winner numbers, different prizes will be awarded. The smallest prize in that game is a fixed reward of 400 NTDs, which requires exactly 3 matches on your ticket. The probability of winning that reward is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;P(\text{Smallest Reward})= \frac{Count(\text{winning cases})}{Count(\text{all outcomes})} \nonumber \\
&amp;\phantom{P(\text{Smallest Reward})} = \frac{C_{3}^{6} * C_{3}^{49 - 6}}{C_{6}^{49}} \nonumber \\
&amp;\phantom{P(\text{Smallest Reward})} = 0.01765
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since each ticket costs 50 NTDs, given that &lt;strong&gt;only the smallest prize is offered&lt;/strong&gt;, the expectation of money returned for every ticket bought will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Smallest Reward}(\text{Money returned}) \nonumber \\
&amp; = P(\text{Smallest Reward}) * 400\, – 1 * 50 \nonumber \\
&amp; = -42.94
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;That expectation indicates on average players are destined to lose 42.94 NTDs for every ticket they buy, and there is nothing we can do to change our fate because every factor in the calculation is fixed. However, it does not mean no silver linings can be found when playing lotteries, and they lie in the prizes of which the size can grow.&lt;/p&gt;

&lt;p&gt;For example, in the lottery mentioned above, the size of the top prize is determined without an upper bound by a certain portion of the total wager collected from selling tickets. As it will be accumulated from one draw to another if winners of it are not found, the total amount of money can occasionally become huge.&lt;/p&gt;

&lt;p&gt;To win that top prize, one needs to have all six numbers matching the drawing result, and that makes the probability drop to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;P(\text{Top prize})= \frac{Count(\text{winning cases})}{Count(\text{all outcomes})} \nonumber \\
&amp;\phantom{P(\text{Top prize})} = \frac{1}{C_{6}^{49}} \nonumber \\
&amp;\phantom{P(\text{Top prize})} = \frac{1}{13983816}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let X be the variable representing the changeble amount of money, then the same calculation of expectation can be applied &lt;strong&gt;as far as the top prize is concerned&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Top prize}(\text{Money returned}) \nonumber \\
&amp; = P(\text{Top prize}) * X\, – 1 * 50 \nonumber \\
&amp; = \frac{X}{13983816} – 1 * 50
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now that X can grow unlimitedly, seeing that expectation go above 0 becomes possible. So how large X has to be? It can be answered by plugging the expression above into the following inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;E_\text{Top prize}(\text{Money returned}) &gt; 0 \nonumber \\
&amp;\Rightarrow \frac{X}{13983816} – 1 * 50 &gt; 0 \nonumber \\
&amp;\Rightarrow X &gt; 50 * 13983816 = 699190800
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;So in this specific case, when the top prize is over the threshold of 699,190,800 NTDs, statistically there would be a certain amount of money gain for every ticket bought. A good timing to try one’s luck.&lt;/p&gt;

&lt;p&gt;Finally, one thing needs to be added is that the analysis discussed above only focuses on winning one particular type of prize. In terms of the &lt;em&gt;true&lt;/em&gt; money returned on average, the calculation has to incorporate all possible prizes. To illustrate that, below is the table containing, alongside the probabilities, the prize detail copied from one of history records of the aforementioned lottery.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Prize&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Money (NTDs)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Probability&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;400&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.017650403866870102&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;400&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.012314235255955885&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1,000&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.0012314235255955885&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2,000&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0.000968619724401408&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;11,856&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4.505208020471665e-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;60,026&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.84498995124077e-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;7&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3,065,656&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4.29067430521111e-07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Top&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;468,588,622&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;7.151123842018516e-08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;where prize names not in bold indicate the sizes of the corresponding prizes are fixed.&lt;/p&gt;

&lt;p&gt;Interestingly, the top prize only reached 468,588,622 NTDs at that time, but the expected money returned over all prizes already amounted to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
E(\text{Money returned}) = 51.62 - 50 = 1.62
\end{equation}&lt;/script&gt;

&lt;p&gt;That implies it is actually easier to see a lottery game of a positive expectation of money returned than the threshold suggests. While it could be quite bothersome to constantly keep track of the change in the size of all prizes so as to make better estimation, it should probably not be a hindrance to those assiduous and avid players, unlike me.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Bruce Frey, “Statistics Hacks: Tips &amp;amp; Tools for Measuring the World and Beating the Odds”, O’Reilly Media, Inc. ©2006, ISBN:0596101643&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 +0800</pubDate>
        <link>/Blogs/note/WinningLotteries.html</link>
        <guid isPermaLink="true">/Blogs/note/WinningLotteries.html</guid>
        
        <category>Fun</category>
        
        <category>Probability</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Intuitions behind Wasserstein GAN</title>
        <description>&lt;p&gt;Since the appearance of Generative Adversarial Network, aka GAN, a large number of variants have been proposed in attempts to improve the training dynamics, and Wasserstein GAN (WGAN) is one of them. The reason I would like to make a note of it is because in my last project, this type of GAN, specifically, WGAN-GP, was the only one working among several types and architectures I had tried. Honestly, most of time I just pick a model and try it to see if it works; but since WGAN has proven its success in my own case, I think it is time to deepen my understanding about it. There have already been abundant discussions and explanations about WGAN on the Internet, so here I simply write down some informal interpretation of my own.&lt;/p&gt;

&lt;h2 id=&quot;wasserstein-distance&quot;&gt;Wasserstein Distance&lt;/h2&gt;
&lt;p&gt;The central idea of WGAN is to replace the means of measuring the similarity between two distributions with the one called Wasserstein Distance (WD), which is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned} 
&amp; WD_2[P, Q] = \inf_{\pi} \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi(x, y) dx dy \\
&amp; where
\begin{cases}
P(x) = \int_{y \in \text{support}_Q} \pi(x, y) dy \\
Q(y) = \int_{x \in \text{support}_P} \pi(x, y) dx
\end{cases}
\end{aligned}
\label{eq:WD2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the distance between every pair of $x$ and $y$ in \eqref{eq:WD2} is defined via 2-norm. The use of other metrics is also possible, but for the consistency with later discussion 2-norm is specifically adopted.&lt;/p&gt;

&lt;p&gt;$\pi$ can be viewed as a transport plan which depicts how one can shift around the densities in one distribution so that the resultant distribution matches the other one. This interpretation comes from the fact that $\pi$ satisfies the &lt;em&gt;where&lt;/em&gt; condition: for any point $x’ \in \text{support}_P$, if from every point $y’ \in \text{support}_Q$ the amount of density described by $\pi(x = x’, y = y’)$ is moved to $x’$, then the density of $P(x = x’)$ can be obtained. Hence, by applying such a transport plan the entire distribution $P$ can be constructed from the distribution $Q$, and vice versa.&lt;/p&gt;

&lt;p&gt;There are many possible transport plans to meet the &lt;em&gt;where&lt;/em&gt; condition, so in order to tell which one is preferable, a cost is designed to serve the purpose. From my point of view, the best way to interpret this cost is to follow the idea of &lt;a href=&quot;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&quot;&gt;Earth mover’s distance&lt;/a&gt;: considering there are several piles of dirt at different locations, and certain amounts of dirt from each pile is going to be moved to other locations, how can we quantify the effort required to complete this job? Well, one way to summarise the overall effort, or cost, of such transport is to calculate the sum of each amount of dirt to be moved times the corresponding distance to be travelled. By analogy, $\pi(x, y)$ describes amounts of density to be moved between every pair of points $x$ and $y$; if the definition of distance between two points is set to be 2-norm, then \eqref{eq:WD2} is equivalent to computing such costs across all transport plans and finding the lowest one. As a result, WD between two distributions $P$ and $Q$ indicates the minimum effort demanded to make them identical; and the larger this value is, the more dissimilar these two distributions are.&lt;/p&gt;

&lt;p&gt;It might look like WD is just another criterion for comparing distributions, but it indeed stands out from other distance measures due to its smoothness property. In the paper where WGAN is introduced, the authors used a simple example to illustrate this advantage. Supposed two distributions $P$ and $Q_{\theta}$ are defined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; P(x, y) =
\begin{cases}
1,&amp; x = 0, 0 \leq y \leq 1 \\
0,&amp; otherwise
\end{cases} \\
&amp; Q_{\theta}(x, y) =
\begin{cases}
1,&amp; x = \theta, 0 \leq y \leq 1 \\
0,&amp; otherwise
\end{cases} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $Q_{\theta}$ is a parametric distribution with $\theta$ being its parameter. In that case, Kullback Leibler divergence (KL), reverse KL, and Jensen-Shannon divergence (JS) can be explicted computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
KL[P \lVert Q_{\theta}] &amp; = \int P(x, y) ln(\frac{P(x, y)}{Q_{\theta}(x, y)}) dx dy \\
&amp; = [\int P(x, y) ln(P(x, y)) dx dy] \\
&amp; \phantom{=} - [\int P(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; = 
\begin{cases}
0,&amp; \theta = 0 \\
\infty,&amp; \theta \neq 0
\end{cases} \\
KL[Q_{\theta} \lVert P] &amp; = \int Q_{\theta}(x, y) ln(\frac{Q_{\theta}(x, y)}{P(x, y)}) dx dy \\
&amp; = [\int Q_{\theta}(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; \phantom{=} - [\int Q_{\theta}(x, y) ln(P(x, y)) dx dy] \\
&amp; = 
\begin{cases}
0,&amp; \theta = 0 \\
\infty,&amp; \theta \neq 0
\end{cases} \\
JS[P \lVert Q_{\theta}] &amp; = 0.5 * KL[P \lVert 0.5 * (P + Q) ] \\
&amp; \phantom{=} + 0.5 * KL[Q \lVert 0.5 * (P + Q)] \\
&amp; = 0.5 * [\int P(x, y) ln(P(x, y)) dx dy] \\ 
&amp; \phantom{=} - 0.5 * [\int P(x, y) ln(0.5 * (P(x, y) + Q_{\theta}(x, y))) dx dy] \\
&amp; \phantom{=} + 0.5 * [\int Q_{\theta}(x, y) ln(Q_{\theta}(x, y)) dx dy] \\
&amp; \phantom{=} - 0.5 * [\int Q_{\theta}(x, y) ln(0.5 * (P(x, y) + Q_{\theta}(x, y))) dx dy] \\
&amp; = -0.5 * (-ln(2)) \\
&amp; \phantom{=} - 0.5 * \int P(x, y) * ln(P(x, y) + Q_{\theta}(x, y)) dx dy \\
&amp; \phantom{=} -0.5 * (-ln(2)) \\
&amp; \phantom{=} - 0.5 * \int Q_{\theta}(x, y) * ln((P(x, y) + Q_{\theta}(x, y))) dx dy \\
&amp; =
\begin{cases}
ln(2) - ln(2) = 0, &amp; \theta = 0 \\
ln(2),&amp; \theta \neq 0
\end{cases} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;All three distance measures lead to a discontinuous drop at $\theta = 0$; moreover , KL and reverse KL give a numerically unstable result of $\infty$ when $\theta \neq 0$.&lt;/p&gt;

&lt;p&gt;On the other hand, with the definition of $WD_2$, the minimum cost of adjusting $Q_{\theta}$ to match $P$ can be achived by moving the distribution $Q_{\theta}$ a distance of $\lvert \theta \rvert$ along x-axis towards $0$, and that results in:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;WD_2[P, Q_{\theta}] = \lvert \theta \rvert * 1 = \lvert \theta \rvert&lt;/script&gt;

&lt;p&gt;Clearly, $WD_2$ demonstrates better smoothness and continuousness. Note that this property is rather important when such distance measures are used as objectives of optimisation via gradient descent. For example, in the particular case above the respective gradients of 4 distances w.r.t $\theta$ are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \nabla_{\theta} KL[P \lVert Q_{\theta}] = 
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} KL[Q_{\theta} \lVert P] =
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} JS[P \lVert Q_{\theta}] =
\begin{cases}
undefined,&amp; \theta = 0\\
0,&amp; \theta \neq 0
\end{cases} \\
&amp; \nabla_{\theta} WD_2[P, Q_{\theta}]
\begin{cases}
undefined,&amp; \theta = 0\\
-1,&amp; \theta &lt; 0 \\
1,&amp; \theta &gt; 0 
\end{cases}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;It can been seen that optimisation via gradient descent in that case will not work with KL, reverse KL and JS due to lack of gradient information; but it is still possible to reach the optimum $\theta$ through the gradient of $WD_2$.&lt;/p&gt;

&lt;h2 id=&quot;wgan&quot;&gt;WGAN&lt;/h2&gt;
&lt;p&gt;Standard GANs are known for its unstable and difficult training process, and some researches have suggested that it might be due to the objective it uses, which leads to the minimisation of JS distance between model distribution and data distribution. As pointed out in the previous &lt;a href=&quot;#dir0&quot;&gt;section&lt;/a&gt;, JS sometime can be problematic in problems of optimisation, while other measures of good properties, such as WD, might be better choices for construction of GAN. Therefore, the inventors of WGAN tried to bring WD (particularly, $WD_2$) into and GAN architecture, resulting in a new objective (given $Q$ is a model distribution and $P$ is the data distribution which needs to be modelled):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \min_{Q} WD_2[P, Q] = \\
&amp; \phantom{min_Q} \min_{Q} \inf_{\pi} \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi(x, y) dx dy \\
&amp; \phantom{min_Q} where
\begin{cases}
P(x) = \int_{y \in \text{support}_Q} \pi(x, y) dy \\
Q(y) = \int_{x \in \text{support}_P} \pi(x, y) dx
\end{cases}
\end{aligned}
\label{eq:WGANGoal}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;In contrast to implicit minimisation of the JS distance between $P$ and $Q$ in standard GANs, \eqref{eq:WGANGoal} explicitly expresses the goal of reducing $WD_2$. That seems to make the objective quite different from the one of standard GANs, since it does not directly correspond to a minimax game anymore; in addition, the calculation of \eqref{eq:WGANGoal} involves finding a $\pi$ which satisfies both infimum and the &lt;em&gt;where&lt;/em&gt; condition, which makes the whole equation looks rather intimidating. Fortunately, it turns out that \eqref{eq:WGANGoal} can be transformed into its dual form according to the theorem of Kantorovich-Rubinstein Duality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
\min_{Q} WD_2[P, Q] = \min_{Q} \sup_{f, \lVert f \rVert_L \le 1} E_{x \sim P}[f(x)] - E_{y \sim Q}[f(y)] 
\label{eq:WGANDualGoal}
\end{equation}&lt;/script&gt;

&lt;p&gt;where $\lVert f \rVert_L$ stands for a Lipschitz constant of $f$ under norm-2 metric. If a neural network of enough capacity with parameter set $\phi$ is used to model $f$ such that supremum can be reached; and samples of $Q$ are generated via mapping each sample $z$ from a known distribution $Z$ with another neural network $g$ parameterized by $\theta$, then \eqref{eq:WGANDualGoal} can be rewritten as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\min_{Q} WD_2[P, Q] = \min_{\theta} \max_{\phi} E_{x \sim P}[f_{\phi}(x)] - E_{z \sim Z}[f_{\phi}(g_\theta(z))] 
\label{eq:WGANPraticalGoal}
\end{equation}&lt;/script&gt;

&lt;p&gt;It can be seen from \eqref{eq:WGANPraticalGoal} that a new minimax game between a generator of $g_{\theta}$ and a critic $f_{\phi}$ emerges: the generator is optimised to produce samples from a better Q which reduce $WD_2$, while the job of the critic is to compute the $WD_2$ of the current $Q$ and the data distribution $P$ through maximisation of the objective.&lt;/p&gt;

&lt;p&gt;Arguably, the most important part which allows us to be able to perform minimax optimisation is the conversion from the original problem into its dual form, with the help of Kantorovich-Rubinstein Duality. While one could go through a solid mathematical derivation to explain how this conversion works, here I would simply like to provide some intuitions based on my understanding about this dual form from the aspect of Lipschitz continuity.&lt;/p&gt;

&lt;p&gt;As expounded in &lt;a href=&quot;https://en.wikipedia.org/wiki/Lipschitz_continuity&quot;&gt;here&lt;/a&gt;, Lipschitz continuity is basically a notion to represent how fast a function $f:X \to W$ can change, described by a number $K \ge 0$ called Lipschitz constant which satisfies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; d_W(f(x_1) - f(x_2)) \le K * d_X(x_1, x_2), \\
&amp; \forall x_1, x_2 \in X
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $d_X$ and $d_W$ are two metrics for the space $X$ and $W$ respectively. With that definition, the $\lVert f \rVert_L \le 1$ appearing in \eqref{eq:WGANDualGoal} can be expressed more precisely as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \lVert f(x) - f(y) \rVert_2 = \lvert f(x) - f(y) \vert \le 1 * \lVert x - y \rVert_2 \\
&amp; \implies -\lVert x - y \rVert_2 \le f(x) - f(y) \le \lVert x - y \rVert_2, \\
&amp; \forall x, y \in \text{support}_Q \cup \text{support}_P
\end{aligned}
\label{eq:lipschitz1Norm2}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first equality in \eqref{eq:lipschitz1Norm2} comes from the fact that $f$ is a real-valued function.&lt;/p&gt;

&lt;p&gt;Now back to the definition of $WD_2$. Supposing an optimal transport plan $\pi^*$, which is one of the solutions to \eqref{eq:WD2}, is given, then it can be rewritten as (&lt;em&gt;where&lt;/em&gt; condition is omitted):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
WD_2[P, Q] = \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy
\label{eq:WD2OptimalPI}
\end{equation}&lt;/script&gt;

&lt;p&gt;The combination of \eqref{eq:lipschitz1Norm2} and \eqref{eq:WD2OptimalPI} yields the following inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \int_{x \in \text{support}_P, y \in \text{support}_Q} (f(x) - f(y)) \pi^*(x, y) dx dy \nonumber \\
&amp; \le \int_{x \in \text{support}_P, y \in \text{support}_Q} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy \nonumber \\ 
&amp; \implies \int_{x \in \text{support}_P, y \in \text{support}_Q} (f(x) - f(y)) \pi^*(x, y) dx dy \nonumber \\
&amp; \phantom{\implies} \le WD_2[P, Q] \label{eq:WD2LowerBound}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that \eqref{eq:WD2LowerBound} has suggested when the two sides of the equation are equal: if some function $f^*$ satisfies $f^*(x) - f^*(y) = \lVert x - y \rVert_{2}$ for every pair $x$, $y$ in the transport plan described by $\pi^*$, i.e. $\pi^*(x, y) \ne 0$, then it can be concluded that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \int_{x \in \text{support}_P, y \in \text{support}_Q} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; = \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) \ne 0} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; \phantom{=} + \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) = 0} (f^*(x) - f^*(y)) \pi^*(x, y) dx dy \\
&amp; = \int_{x \in \text{support}_P, y \in \text{support}_Q, \pi^*(x, y) \ne 0} \lVert x - y \rVert_{2} \pi^*(x, y) dx dy + 0 \\
&amp; = WD_2[P, Q]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $WD_2[P, Q]$ represents the upper bound of \eqref{eq:WD2LowerBound}, and it has been shown that such upper bound can be reached with $f^*$. That implies the following equation holds under the assumption that the parameterised form of $f$, $f_{\phi}$, can express any f whose $\lVert f \rVert_L \le 1$, including $f^*$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; WD_2[P, Q] \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P, y \in \text{support}_Q} (f_{\phi}(x) - f_{\phi}(y)) \pi^*(x, y) dx dy
\end{aligned}
\label{eq:maximisationWRTPhi}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Interestingly, \eqref{eq:maximisationWRTPhi} can be further simplified through the derivation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; WD_2[P, Q] \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P, y \in \text{support}_Q} f_{\phi}(x) * \pi^*(x, y) dx dy \\
&amp; \phantom{= \max_{\phi}} - \int_{x \in \text{support}_P, y \in \text{support}_Q} f_{\phi}(y) * \pi^*(x, y) dx dy \\
&amp; = \max_{\phi} \int_{x \in \text{support}_P} f_{\phi}(x) * P(x) dx \\
&amp; \phantom{= \max_{\phi}} - \int_{y \in \text{support}_Q} f_{\phi}(y) * Q(y) dy \\
&amp; = \max_{\phi} E_{x \sim P}[f_{\phi}(x)] - E_{y \sim Q}[f_{\phi}(y)]
\end{aligned}
\label{eq:WD2DualForm}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out that there is no need to know what the exact $\pi^*$ is in order to compute $WD_2[P, Q]$, and that is why the minimisation of \eqref{eq:WGANGoal} can be achieved by solving \eqref{eq:WGANDualGoal}.&lt;/p&gt;

&lt;h2 id=&quot;geometric-interpretation-of-eqrefeqwd2dualform&quot;&gt;Geometric Interpretation of \eqref{eq:WD2DualForm}&lt;/h2&gt;
&lt;p&gt;Inspired by the idea of high-dimensional data visualisation, I have found it helpful to approach \eqref{eq:WD2DualForm} with this geometric interpretation. Although I have to emphasise that this is only my personal understanding, which is rather informal and might be theoretically inaccurate.&lt;/p&gt;

&lt;p&gt;The essence of Wasserstein Distance is to describe how far two distributions are separate. If a set of samples are used to represent the corresponding distribution from which they are generated, such a distance can also be viewed as the degree of separation between two groups of sample points on average. One way to estimate this average distance is to compute it directly in the space of supports of distributions, as seen in \eqref{eq:WD2}; the other way is to project samples onto a space of few dimensions, and then do the estimation in that reduced space. \eqref{eq:WD2DualForm}, where $f$ can be viewed as a projection function brining every data point onto the real line, is in a sense analogous to the latter approach.&lt;/p&gt;

&lt;p&gt;The use of projection is rather common in tasks such as high-dimensional data visualisation. The type of projection is specifically selected so that information of interest is retained after projection, and hence the result lie in a more meaningful and representative space. So what information is preserved through the projection introduced in \eqref{eq:WD2DualForm}? Well, note that that projection is described by some function $f^*$ which can maximise \eqref{eq:WD2DualForm} and satisfies ($\pi^*$ is the joint distribution describing the optimal transport plan corresponding to $f^*$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}
\lVert f^* \rVert_L \le 1 \\
f^*(x) - f^*(y) = \lVert x - y \rVert_{2}\text{, if } \pi^*(x, y) \ne 0  \\
f^*(x) - f^*(y) \le \lVert x - y \rVert_{2}\text{, if } \pi^*(x, y) = 0 
\end{cases}&lt;/script&gt;

&lt;p&gt;That suggests that $f^*$ preserves the distances of pairs of points in the optimal transport plan, while distances between other points are allowed to decrease (resulting in information loss to some extent). Since pairs in the plan most likely contain data points from two respective distributions for the purpose of density matching, such preservation of distance information leads to a phenomenon where projected points from one distribution tend to stay the same distances away from those coming from the other distribution; in other words, the projection gives rise to two clusters with similiar degree of separation as the original distributions have, and each cluster corresponds to one distribution, respectively.&lt;/p&gt;

&lt;figure&gt;
	
	&lt;img src=&quot;/Blogs/assets/images/2019-06-11-WassersteinGAN/GeometricInterpretationOfWD2.png&quot; alt=&quot;An illustration of the projection $f^*$&quot; /&gt;
	&lt;figcaption&gt;
	An illustration of the projection $f^*$
	&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Just as what projection benefits in tasks of high-dimensional data visualisation, $f^*$ also gives advantage in terms of the calculation of $WD_2$. It can be seen from \eqref{eq:WD2DualForm} that $WD_2$ becomes the distance between the means of two resulting clusters of projected points. That implies distributions being compared are in a sense better described after the projection, as one single piece of statistics (mean) is representative enough to reflect the relative location of an entire cluster.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Lipschitz_continuity&quot;&gt;https://en.wikipedia.org/wiki/Lipschitz_continuity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&quot;&gt;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v70/arjovsky17a.html&quot;&gt;Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Networks. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. International Convention Centre, Sydney, Australia: PMLR, June 2017, pp. 214223.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 11 Jun 2019 00:00:00 +0800</pubDate>
        <link>/Blogs/note/WassersteinGAN.html</link>
        <guid isPermaLink="true">/Blogs/note/WassersteinGAN.html</guid>
        
        <category>MachineLearning</category>
        
        <category>GAN</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Jekyll Markdown Syntax Reference</title>
        <description>&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[link](url)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/roachhd/779fa77e9b90fe945b0c&quot;&gt;Reference link&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 27 May 2019 00:00:00 +0800</pubDate>
        <link>/Blogs/note/JekyllMarkdownSyntaxReference.html</link>
        <guid isPermaLink="true">/Blogs/note/JekyllMarkdownSyntaxReference.html</guid>
        
        <category>Jekyll</category>
        
        <category>Markdown</category>
        
        
        <category>Note</category>
        
      </item>
    
  </channel>
</rss>
