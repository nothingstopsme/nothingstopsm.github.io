<!DOCTYPE html>
<html>
  <head>
      
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
	<script type="text/javascript"
	        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true},
		jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
		extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
		TeX: {
		extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
		equationNumbers: {
		autoNumber: "AMS"
		}
		}
		});
		</script>
	

<!-- CSS -->

  <link rel="stylesheet" href="/Blogs/assets/css/main.css">
  <link rel="canonical" href="/Blogs/note/Variational-Diffusion-Model.html">
  <link rel="alternate" type="application/rss+xml" title="" href="/Blogs/feed.xml">

<!-- Google font -->

  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Noto Sans">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open Sans">

<!-- font awesome -->

	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
	
	<title>From VAE, Hierarchical VAE, to Variational Diffusion Model</title>

	


</head>


  


  </head>

  <body>


  <div class="wrapper">
          <header class="post-header">

    <center><div class="post-title" itemprop="name headline">From VAE, Hierarchical VAE, to Variational Diffusion Model</div>

		<div class="post-meta"><i class="fa fa-calendar-o"></i> <time datetime="20 Jan 2023" itemprop="datePublished">Jan 20 2023</time>

		&nbsp;&nbsp;•&nbsp;&nbsp;<i class="fa fa-user-secret"></i> <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Chao</span>
        
		<br>
		<!--<i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-pulse"></i></span>˚C</span>-->
	</div>

        
        <div class="post-tags">
        
		<a class="post-tags-item" href="/Blogs/tags/">Probability</a>
        
		<a class="post-tags-item" href="/Blogs/tags/">MachineLearning</a>
        
	</div>
    </center>
    
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
<div class="post-content">
    <center>
	
    </center>
</div>




<div class="post-content" itemprop="articleBody">
    <p><strong>V</strong>ariational <strong>D</strong>iffusion <strong>M</strong>odels, a relatively new design of generative models, have demonstrated its competitiveness against other state-of-the-art methods (and in some cases outperformed). By the look of its name, one (OK, I mean me) might think it must derive from a ground-breaking and sophisticated mathematical theory in order to grant it such power; but a tutorial I read recently has completely broken down my prejudice: it can be interpreted as simple as a form of <strong>H</strong>ierarchical <strong>VAE</strong>. This post will cover that particular perspective which I think make <strong>VDM</strong> become really straightforward and understandable to me.</p>

<h2 id="vae-and-hvae-"><strong>VAE</strong> and <strong>HVAE</strong> <a name="VAE_and_HVAE"></a></h2>
<p>Starting with these two types of models, the main objective of <strong>VAE</strong> is to estimate data distribution on the principle of maximum likelihood: given a data set $D_X$ of the size $|D_X|$, we want to find a distribution $p(x)$ that can maximise $\sum_{d_x \in D_X} \frac{1}{|D_X|} log\;p(x = d_x)$. However, for real-world data $p(x)$ is usually unknown or intractable, and thus $log\;p(x)$ cannot be evaluated, let alone to solve that maximisation problem. To work around that, the inventors of <strong>VAE</strong> derived the <strong>E</strong>vidence <strong>L</strong>ower <strong>BO</strong>und of $log\;p(x)$ by introducing a latent variable $z$ (note that $x$ is the observed variable) and a auxiliary distribution $q(z|x)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& log\;p(x) \nonumber \\
& = log \big( \int q(z|x) \frac{p(x, z)}{q(z|x)} dz \big) \nonumber \\
& \ge \int q(z|x) log \big( \frac{p(x,z)}{q(z|x)} \big) dz \,\,\, \text{(Applying Jensen’s Inequality)} \nonumber \\
& = E_{g} [ log \; p(x|z) ] + E_{g} [ log \big( \frac{p(z)}{q(z|x)} \big) ] \nonumber \\
& = E_{g} [ log \; p(x|z) ] - KL[ q(z|x) \| p(z) ]  \label{eq:VAE_ELBO_1} \\
& = E_{g} [ log \big( \frac{p(x, z)}{p(z|x)} \big) ] + E_{g} [ log \big( \frac{p(z|x)}{q(z|x)} \big) ] \nonumber \\
& = E_{g} [ log \; p(x) ] - KL[ q(z|x) \| p(z|x) ]  \nonumber \\
& = log \; p(x) - KL[ q(z|x) \| p(z|x) ]  \label{eq:VAE_ELBO_2}
\end{align} %]]></script>

<p>The two equivalent forms of <strong>ELBO</strong> (\eqref{eq:VAE_ELBO_1} and \eqref{eq:VAE_ELBO_2}) reveal the fact that the maximisation of \eqref{eq:VAE_ELBO_1} will lead to the maximisation of $log\;p(x)$ and the minimisation of $KL[ q(z|x) \| p(z|x) ]$; and when $KL[ q(z|x) \| p(z|x) ] = 0$, i.e. $q(z|x)$ and $p(z|x)$ are identical, maximising \eqref{eq:VAE_ELBO_1} is just the same as maximising $log\;p(x)$. That is the reason why $q(z|x)$ of this particular form is brought in as the auxiliary distribution in the first place, because we want that \eqref{eq:VAE_ELBO_2} as close to $log\;p(x)$ as possible, so that one can use \eqref{eq:VAE_ELBO_1} which only involves $q(z|x)$, $p(x|z)$, and $p(z)$ as a proxy objective to indirectly search a $p$ that can be the (approximate) solution to the original maximisation problem. Being able to use these three alternative distributions has the advantage in that, the two conditionals can be defined by function mappings (such as neural networks) taking given conditions as input to produce the description of output distributions, while $p(z)$ might be of any form as long as it matches how model designers think the latent variable should be distributed. Therefore, a typical <strong>VAE</strong> normally contains one mapping (as an encoder) to describe $q(z|x)$; another (as a decoder) to describe $p(x|z)$, and a tractable distribution (as a prior) choosen by model designers for $p(z)$. Note that if both $p(x|z)$ and $p(z)$ are set up to be some distributions from which we know how to sample, then we can actually generate new data points with <strong>VAE</strong> through first sampling $z$ and then given the instance of $z$ sampling $x$. That property opens up the potential of <strong>VAE</strong> being a generative model.</p>

<figure>
	
  <a name="included_image_Fig1"></a>
	
	<img src="/Blogs/assets/images/2023-01-20-Variational-Diffusion-Model/graphical_model_VAE.png" alt="Fig1: The graphical model for &lt;strong&gt;VAE&lt;/strong&gt;" />
	<figcaption>
	Fig1: The graphical model for <strong>VAE</strong>
	</figcaption>
</figure>

<p>As for <strong>HVAE</strong>, to put it simply it is a generalised form of <strong>VAE</strong> which allows multiple latent variables; hence for latent variables $z_1 \dots z_T$, the <strong>ELBO</strong> becomes:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& log\;p(x) \\
& = log \big( \int q(z_1, \cdots, z_T|x) \frac{p(x, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|x)} dz_1 \dots dz_T \big) \\
& \ge \int q(z_1, \cdots, z_T |x) log \big( \frac{p(x,z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|x)} \big) dz_1 \dots dz_T \\
& = \int q(z_1 |x) \prod_{i=2}^T q(z_i | z_{<i}, x)  \\
& \phantom{= \int} log \big( \frac{p(x | z_1) \big( \prod_{j=1}^{T-1} p(z_j | z_{>j}) \big) p(z_T)}{q(z_1 |x) \prod_{i=2}^T q(z_i | z_{<i}, x)} \big) dz_1 \dots dz_T \\
\end{aligned}
\label{eq:HVAE_ELBO}
\end{equation} %]]></script>

<p>where the conditional dependency across latent and observed variables shows a hierarchical structure, as illustrated in <a href="#included_image_Fig2">Fig2</a>. Likewise, we can use $p(x | z_1) \big( \prod_{j=1}^{T-1} p(z_j | z_{&gt;j}) \big) p(z_T)$ to generate new data samples once these conditionals that maximise \eqref{eq:HVAE_ELBO} are obtained.</p>

<figure>
	
  <a name="included_image_Fig2"></a>
	
	<img src="/Blogs/assets/images/2023-01-20-Variational-Diffusion-Model/graphical_model_HVAE_4nodes.png" alt="Fig2: The graphical model for &lt;strong&gt;HVAE&lt;/strong&gt; with T = 3" />
	<figcaption>
	Fig2: The graphical model for <strong>HVAE</strong> with T = 3
	</figcaption>
</figure>

<h2 id="reducing-the-complexity-of-hvae-">Reducing the Complexity of <strong>HVAE</strong> <a name="Complexity_Reduction"></a></h2>
<p>The fully conditional dependency formulated in \eqref{eq:HVAE_ELBO} has the highest capacity for representing the joints, but it also means the cost of modelling grows exponentially as the number of latent variables increases. One way to alleviate these overhands is to introduce the assumption of conditional independency, such as (for brevity, $z_0$ is substituted for $x$):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{cases}
& \begin{aligned}
& p(x, z_1, \cdots, z_T) = p(z_0, z_1, \cdots, z_T) \\
& = p(z_0 | z_1) \big( \prod_{i = 2}^{T} p(z_{i-1}|z_i) \big) p(z_T)
\end{aligned} \\
& \begin{aligned}
& q(z_1, \cdots, z_T|x) = q(z_1, \cdots, z_T| z_0) \\
& = q(z_1 | z_0) \prod_{i = 2}^{T} q(z_i|z_{i-1}, z_0) \\
& = q(z_1 | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0) \frac{q(z_i|z_0)}{q(z_{i-1}|z_0)} \\
& = q(z_1 | z_0) \big( \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0) \big) \frac{q(z_T|z_0)}{q(z_1|z_0)} \\
& = q(z_T | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0)
\end{aligned}
\end{cases}
\label{eq:pq_decomposition}
\end{equation} %]]></script>

<figure>
	
  <a name="included_image_Fig3"></a>
	
	<img src="/Blogs/assets/images/2023-01-20-Variational-Diffusion-Model/graphical_model_HVAE_4nodes_cind.png" alt="Fig3: The graphical model for &lt;strong&gt;HVAE&lt;/strong&gt; with T = 3, under the assumption of the conditional independency set out in \eqref{eq:pq_decomposition}" />
	<figcaption>
	Fig3: The graphical model for <strong>HVAE</strong> with T = 3, under the assumption of the conditional independency set out in \eqref{eq:pq_decomposition}
	</figcaption>
</figure>

<p>Then the decomposition described in \eqref{eq:pq_decomposition} gives rise to a different <strong>ELBO</strong>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& log\;p(x) \nonumber \\
& = log\;p(z_0) \nonumber \\
& \ge \int q(z_1, \cdots, z_T |z_0) log \big( \frac{p(z_0,z_1, \cdots, z_T)}{q(z_1, \cdots, z_T|z_0)} \big) dz_1 \dots dz_T \nonumber \\
& = \int q(z_1, \cdots, z_T |z_0) \nonumber \\
& \phantom{= \int}log \big( \frac{p(z_0 | z_1) \big( \prod_{i = 2}^{T} p(z_{i-1}|z_i) \big) p(z_T)}{q(z_T | z_0) \prod_{i = 2}^{T} q(z_{i-1}|z_i, z_0)} \big) dz_1 \dots dz_T \nonumber \\
& = E_{q(z_1, \cdots, z_T |z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} + E_{q(z_1, \cdots, z_T |z_0)} [ log \big( \frac{p(z_T)}{q(z_T|z_0)} \big) ] \nonumber \\
& \phantom{=} + E_{q(z_1, \cdots, z_T |z_0)} [ log \big( \prod_{i=2}^T \frac{p(z_{i-1}|z_i)}{q(z_{i-1}|z_i, z_0)} \big)] \nonumber \\
& = E_{q(z_1 | z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} + E_{q(z_T |z_0)} [ log \big( \frac{p(z_T)}{q(z_T|z_0)} \big) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_T | z_0) \prod_{j = 2, j \ne i}^{T} q(z_{j-1}| z_j, z_0)} \big[ E_{q(z_{i-1}|z_i, z_0)}[ log \big( \frac{p(z_{i-1}|z_i)}{q(z_{i-1}|z_i, z_0)} \big) ] \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_T | z_0) \prod_{j = 2, j \ne i}^{T} q(z_{j-1}| z_j, z_0)} \big[ \underbrace{KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ]}_{\text{constant with respect to } z_{i-1}} \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_1, \cdots, z_T |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \nonumber \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \nonumber \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \nonumber \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \label{eq:HVAE_CIND_ELBO} \\

\end{align} %]]></script>

<p>Notice that when $T=1$, \eqref{eq:HVAE_CIND_ELBO} falls back to:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \\
& \phantom{=} - KL[q(z_1|z_0) \| p(z_1) ] \\
& \phantom{=} - \underbrace{\sum_{i=2}^{1} E_{q(z_i | z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big]}_{\text{summation over nothing}} \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] - KL[q(z_1|z_0) \| p(z_1)] \\
& = E_{q(z_1|x)} [ log \; p(x|z_1) ] - KL[q(z_1|x) \| p(z_1)]

\end{aligned} %]]></script>

<p>which is exactly the same as \eqref{eq:VAE_ELBO_1} of <strong>VAE</strong>; thus this summation $\sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big]$ can be viewed as additional criteria to makes sure that for every newly introduced latent variable $z_{i-1}$, its conditionals ($q(z_{i-1}|z_i, z_0)$ and $p(z_{i-1}|z_i)$) found for maximising \eqref{eq:HVAE_CIND_ELBO} are similar on average.</p>

<h2 id="vdm"><strong>VDM</strong></h2>
<p>The key of <strong>VDM</strong> is the concept of <a href="https://en.wikipedia.org/wiki/Diffusion">diffusion processes</a>, in which the substance of interest gradually and randomly spreads out. Just like particle diffusion in physics, in <strong>VDM</strong> data representation is set to be the one undergoing a diffusion process over time, and at the end of the process, one would expect it becomes completely randomised; in other words, the diffusion process of <strong>VDM</strong> will turn data into noise.</p>

<p>For continuous data, such a diffusion process can be formulated as the addition of various degrees of standard gaussian noise to the observed variable $z_0$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& z_i \\
& = \sqrt{\bar{\alpha}_i} z_0 + \sqrt{1 - \bar{\alpha}_i} \epsilon_i \\
& \sim N(\sqrt{\bar{\alpha}_i} z_0, (1 - \bar{\alpha}_i) I ) \\
& = q(z_i | z_0)
\end{aligned}
\label{eq:gaussian_noise_addition}
\end{equation} %]]></script>

<p>where $z_i$ is the random variable for the outcomes at the $i$-th discrete time step in this diffusion process; $\epsilon_i \sim N(0, I)$; and $\bar{\alpha}_i$ is its parameter controlling the strength of noise being added such that</p>
<ol>
  <li>$\bar{\alpha}_i &lt; \bar{\alpha}_j$ for every $j &lt; i$. This makes the distribution of $z_i$ closer to $N(0, I)$ than others preceding it.</li>
  <li>Given a total of $T$ time steps, $\bar{\alpha}_T = 0$ and therefore $z_T \sim N(0, I)$ (the results at the last step follows a standard guassian).</li>
</ol>

<p>OK, now we have this particular diffusion process defined, and we know how to directly sample at each time step; but what is the use of it? Well, smart researchers come up with this idea: if $p(z_i | z_{&gt;i})$ for all $0 \le i &lt; T$ and $p(z_T)$ are known, one can in a sense create a reverse process which generates data (outcomes of $z_0$) from samples of $z_T$. To that end, we want to find those conditionals and marginal which are consistent with \eqref{eq:gaussian_noise_addition} and the data distribution, and the consistency is imposed by the following KL divergence (given $q(z_0) = \frac{1}{| D_x |}$ being the empirical data distribution of a data set $D_x$, and $q(z_1, \cdots, z_T | z_0) $ being the joint distribution of all $z_i$ over the course of the diffusion process):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& KL[q \| p] \\
& = \int q(z_0, z_1, \cdots, z_T) \\
& \phantom{=} log \big(\frac{p(z_0, z_1, \cdots, z_T)}{q(z_0, z_1, \cdots, z_T)} \big) dz_0 \dots dz_T  \\
& = \int q(z_1, \cdots, z_T | z_0) q(z_0) \\
& \phantom{=} log \big(\frac{p(z_0, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T | z_0) q(z_0)} \big) dz_0 \dots dz_T  \\
& = E_{q(z_0)}[\int q(z_1, \cdots, z_T | z_0) \\
& \phantom{= E_{q(z_0)}[\int} log \big(\frac{\prod_{i=1}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T] \\
& \phantom{=} - E_{q(z_0)} [ log \big( q(z_0) \big) ] \\
& = E_{q(z_0)} [\int q(z_1, \cdots, z_T | z_0) \\
& \phantom{= E_{q(z_0)} [ \int} log \big(\frac{\prod_{i=1}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T ] \\
& \phantom{=} + entropy(q(z_0))

\end{aligned}
\label{eq:KL_for_consistency_1}
\end{equation} %]]></script>

<p>Since $entropy(q(z_0))$ is constant with respect to all $p(z_i | z_{&gt;i})$ and $p(z_T)$, it can be safely omitted without changing the consistency criterion, resulting in:</p>

<script type="math/tex; mode=display">% <![CDATA[
\require{cancel}
\begin{equation}
\begin{aligned}
& E_{q(z_0)} [ \int q(z_1, \cdots, z_T | z_0) \\
& \phantom{= E_{q(z_0)} [ \int} log \big(\frac{\prod_{i=1}^{T-1} p(z_i | z_{>i}) p(z_T)}{q(z_1, \cdots, z_T | z_0)} \big) dz_1 \dots dz_T ] \\
& \phantom{=} + \cancel{entropy(q(z_0))} \\
& = \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( \int q(z_1, \cdots, z_T | z_0 = d_x) \\
& \phantom{= \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( \int} log \big(\frac{p(z_0 = d_x, z_1, \cdots, z_T)}{q(z_1, \cdots, z_T | z_0 = d_x)} \big) dz_1 \dots dz_T \Big) \\
& \le \sum_{d_x \in D_x} \frac{1}{| D_x |} \Big( \int log \big( p(z_0 = d_x, z_1, \cdots, z_T) dz_1 \dots dz_T \big) \Big) \\
& \sum_{d_x \in D_x} \frac{1}{| D_x |} log \big( p(z_0 = d_x) \big) \\
\end{aligned}
\label{eq:KL_for_consistency_2}
\end{equation} %]]></script>

<p>Looking familiar, doesn’t it? \eqref{eq:KL_for_consistency_2} is exactly the same as the expectation of \eqref{eq:HVAE_ELBO} over the data distribution with $z_0$ in place of $x$; therefore it is as though building a <strong>HVAE</strong> given a fixed auxiliary distribution $q$ derived from the diffusion process according to \eqref{eq:gaussian_noise_addition}, and the same trick of complexity reduction discussed in this <a href="#Complexity_Reduction">section</a> can be applied immediately to give us the final objective $\sum_{d_x \in D_x} \frac{1}{| D_x |} Obj_{\text{VDM}}(z_0 = d_x)$, where</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{VDM}}(z_0) \\
& = E_{q(z_1|z_0)} [ log \; p(z_0|z_1) ] \\
& \phantom{=} - KL [ q(z_T|z_0) \| p(z_T) ] \\
& \phantom{=} - \sum_{i=2}^T E_{q(z_i |z_0)} \big[ KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \big] \\
\end{aligned}
\label{eq:Obj_VDM}
\end{equation} %]]></script>

<p>is copied from \eqref{eq:HVAE_CIND_ELBO}. Note that \eqref{eq:Obj_VDM} goes particularly well with our diffusion process, because:</p>
<ul>
  <li>We can easily sample from $q(z_i | z_0)$ for any $1 \le i \le T$, so in practice all expectations appearing in \eqref{eq:Obj_VDM} can be estimated with low variance via <em>Monte Carlo estimate</em>.</li>
  <li>When choosing the distribution of $p(z_T)$ (just as picking the prior in <strong>VAE</strong>), if $p(z_T)$ is set to be a standard gaussian, then $KL [ q(z_T|z_0) \| p(z_T) ] = 0$ (due to the fact that $q(z_T|z_0)$ is also $N(0, I)$)</li>
</ul>

<p>Nonetheless, there are still three missing pieces before we can use \eqref{eq:Obj_VDM} to start to learn a <strong>VDM</strong>:</p>

<h5 id="1-what-distribution-is-qz_i-1z_i-z_0">1. What distribution is $q(z_{i-1}|z_i, z_0)$?</h5>
<p>It is indeed obscure as we can not read it out directly from \eqref{eq:gaussian_noise_addition}, but research has demonstrated that, given $q(z_T | z_0) = N(\sqrt{\bar{\alpha}_T} z_0, (1 - \bar{\alpha}_T) I )$, the following gaussian matches our diffusion process</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& q(z_{i-1} |z_i, z_0) \\
& = N(\underbrace{\sqrt{\bar{\alpha}_{i-1}} z_0 + \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2} \frac{z_i - \sqrt{\bar{\alpha}_i} z_0}{\sqrt{1 - \bar{\alpha}_i}}}_{M_i}, \sigma_i^2 I)
\end{aligned}
\label{eq:q_z_i_1_given_z_i_z_0}
\end{equation} %]]></script>

<p>with $\sigma_i^2$ being the magnitude of the isotropic variance at the time step $i$. To show that is true, assuming that $q(z_i | z_0) = N(\sqrt{\bar{\alpha}_i} z_0, (1 - \bar{\alpha}_i) I )$, then</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& q(z_{i-1} | z_0) \\
& = \int q(z_{i-1} |z_i, z_0) q(z_i | z_0) dz_i \\
& \propto \int exp \Big(-\frac{1}{2} \big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} - 2 \frac{z_{i-1}^T M_i}{\sigma_i^2} + \frac{M_i^T M_i}{\sigma_i^2} \big) \Big) \\
& \phantom{\propto \int } exp\Big(-\frac{1}{2} \big( \frac{z_i^T z_i}{1 - \bar{\alpha}_i} - 2 \frac{\sqrt{\bar{\alpha}_i} z_i^T z_0 }{1 - \bar{\alpha}_i}  +  \frac{\bar{\alpha}_i z_0^T z_0}{1 - \bar{\alpha}_i} \big) \Big) dz_i \\
& = \underbrace{f(z_0, z_{i-1})}_{\text{terms not involving } z_i} \int \underbrace{g(z_0, z_{i-1}, z_i)}_{\text{terms involving } z_i} dz_i
\end{aligned}
\label{eq:q_z_minus_1_given_z_0}
\end{equation} %]]></script>

<p>Since</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& \frac{z_{i-1}^T M_i}{\sigma_i^2} \\
& = \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 + \frac{\sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} (z_{i-1}^T z_i - \sqrt{\bar{\alpha}_i} z_{i-1}^T z_0)
\end{aligned} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& \frac{M_i^T M_i}{\sigma_i^2} \\
& = \frac{\bar{\alpha}_{i-1}}{\sigma_i^2} z_0^T z_0 + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} (z_0^T z_i - \sqrt{\bar{\alpha}_i} z_0^T z_0)\\
& \phantom{=} \frac{1-\bar{\alpha}_{i-1} - \sigma_i^2}{\sigma_i^2 (1 - \bar{\alpha}_i)} (z_i^T z_i - 2\sqrt{\bar{\alpha}_i} z_i^T z_0 + \bar{\alpha}_i z_0^T z_0)

\end{aligned} %]]></script>

<p>therefore</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& g(z_0, z_{i-1}, z_i) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1}{1 - \bar{\alpha}_i} z_i^T z_i + \frac{1-\bar{\alpha}_{i-1} - \sigma_i^2}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T z_i \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{ \sqrt{\bar{\alpha}_i}}{1 - \bar{\alpha}_i} z_i^T z_0 \\ 
& \phantom{= exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} \sqrt{1-\bar    {\alpha}_{i-1} - \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} z_i^T z_0 \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{(1-\bar{\alpha}_{i-1} - \sigma_i^2) \sqrt{\bar{\alpha}_i}}{\sigma_i^2 (1 - \bar{\alpha}_i)}) z_i^T z_0 \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{1-\bar{\alpha}_{i-1} -     \sigma_i^2}}{\sigma_i^2 \sqrt{1 - \bar{\alpha}_i}} z_i^T z_{i-1} \\
& \phantom{= exp} \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T z_i \\
& \phantom{= exp\Big(-\frac{1}{2}\big(} - 2 \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} z_i^T \nu(z_0, z_{i-1}) \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \| z_i - \nu(z_0, z_{i-1}) \|^2_2 \big) \Big) \\
& \phantom{= exp} exp\Big(\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big)

\end{aligned}
\label{eq:g_of_z_0_z_i_minus_1_z_i}
\end{equation} %]]></script>

<p>where</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& \nu(z_0, z_{i-1}) \\
& = (\sqrt{\bar{\alpha}_i} - \frac{\sqrt{(1-\bar{\alpha}_i)\bar{\alpha}_{i-1}(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{1-\bar{\alpha}_{i-1}}) z_0 \\ 
& \phantom{=} + \frac{\sqrt{(1-\bar{\alpha}_i)(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{1-\bar{\alpha}_{i-1}} z_{i-1}
\end{aligned} %]]></script>

<p>\eqref{eq:g_of_z_0_z_i_minus_1_z_i} suggests that (based on the form of the normaliser for any multivariate gaussian) $\int g(z_0, z_{i-1}, z_i) dz_i$ is just $\sqrt{det(2 \pi \frac{\sigma_i^2 (1 - \bar{\alpha}_i)}{1 - \bar{\alpha}_{i-1}}I)} exp\Big(\frac{1}{2}\big( \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big)$, which allows us to further simplify \eqref{eq:q_z_minus_1_given_z_0}:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& q(z_{i-1} | z_0) \\
& \propto f(z_0, z_{i-1}) \int g(z_0, z_{i-1}, z_i) dz_i \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_i(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_i}} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - \frac{1-\bar{\alpha}_{i-1}}{\sigma_i^2 (1 - \bar{\alpha}_i)} \nu(z_0, z_{i-1})^T \nu(z_0, z_{i-1}) \big) \Big) \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{z_{i-1}^T z_{i-1}}{\sigma_i^2} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{\sigma_i^2} z_{i-1}^T z_0 \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} \cancel {+ 2 \frac{\sqrt{\bar{\alpha}_i(1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_i}} z_{i-1}^T z_0} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} - \frac{1-\bar{\alpha}_{i-1}-\sigma_i^2}{\sigma_i^2 (1-\bar{\alpha}_{i-1})} z_{i-1}^T z_{z_i} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} \cancel{- 2 \frac{\sqrt{\bar{\alpha}_i (1-\bar{\alpha}_{i-1}-\sigma_i^2)}}{\sigma_i^2 \sqrt{1-\bar{\alpha}_{i}}} z_{i-1}^T z_0} \\
& \phantom{\propto exp\Big(-\frac{1}{2}\big(} + 2 \frac{\sqrt{\bar{\alpha}_{i-1}} (1-\bar{\alpha}_{i-1}-\sigma_i^2)}{\sigma_i^2 (1-\bar{\alpha}_{i-1})} z_{i-1}^T z_0 \big) \Big) \\
& = exp\Big(-\frac{1}{2}\big( \frac{1}{1-\bar{\alpha}_{i-1}} z_{i-1}^T z_{i-1} - 2 \frac{\sqrt{\bar{\alpha}_{i-1}}}{1-\bar{\alpha}_{i-1}} z_{i-1}^T z_0 \big) \Big) \\
& \propto exp\Big(-\frac{1}{2}\big( \frac{\|z_{i-1} - \sqrt{\bar{\alpha_{i-1}}}z_0 \|^2_2}{1-\bar{\alpha_{i-1}}} \big) \Big) \\
& \propto N(\sqrt{\bar{\alpha_{i-1}}}z_0, (1-\bar{\alpha_{i-1}}) I)
\end{aligned} %]]></script>

<p>Hence by induction, all $q(z_1 | z_0), \cdots, q(z_T | z_0)$ match the diffusion process defined by \eqref{eq:gaussian_noise_addition}.</p>

<h5 id="2-how-to-parameterise-pz_i-1z_i-for-2-le-i-le-t">2. How to parameterise $p(z_{i-1}|z_i)$ for $2 \le i \le T$?</h5>
<p>The answer to this one becomes more obvious after \eqref{eq:q_z_i_1_given_z_i_z_0} is established: since we want to minimise $KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ]$ for maximising \eqref{eq:Obj_VDM}, the distribution of $p(z_{i-1}|z_i)$ should be able to be shaped as close to $q(z_{i-1}|z_i, z_0)$ as possible. Note that $q(z_{i-1}|z_i, z_0)$ itself is actually a guassian parameterised by a fixed $\sigma_i^2$ and a variable $M_i$ which is in effect a function of $z_0, z_i$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& M_i(z_0, z_i) \\
& = \sqrt{\bar{\alpha}_{i-1}} z_0 + \sqrt{1-\bar{\alpha}_{i-1} - \sigma_i^2} \frac{z_i - \sqrt{\bar{\alpha}_i} z_0}{\sqrt{1 - \bar{\alpha}_i}} \\
& = \underbrace{\frac{\sqrt{\bar{\alpha}_{i-1}(1-\bar{\alpha}_i)} - \sqrt{\bar{\alpha}_i (1 - \bar{\alpha}_{i-1}- \sigma_i^2)}}{\sqrt{1-\bar{\alpha}_i}}}_{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_0 \\
& \phantom{=} + \underbrace{\frac{\sqrt{1 - \bar{\alpha}_{i-1} - \sigma_i^2}}{\sqrt{1 - \bar{\alpha}_i}}}_{w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_i

\end{aligned}
\label{eq:M_i_z_0_z_i}
\end{equation} %]]></script>

<p>So why not parameterise $p(z_{i-1}|z_i)$ in the same way as a guassian?</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& p(z_{i-1}|z_i) \\
& = N(w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) f(i, z_i) + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i, \sigma_i^2 I)

\end{aligned}
\label{eq:p_z_i_minus_1_given_z_i}
\end{equation} %]]></script>

<p>with $f$ being the function to be learnt, shared across time steps, taking time index $i$ and $z_i$ as input to in a sense “predict” $z_0$ (as $p(z_{i-1}|z_i)$ does not condition on $z_0$). In addition to similarity, another reason of \eqref{eq:p_z_i_minus_1_given_z_i} being a good choice is that, the target KL will be computed over two gaussians, and thus we have a closed form for it:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \\
& = \frac{1}{2} \Big( log \big(\frac{det(\sigma_i^2 I)}{det(\sigma_i^2 I)} \big) - trace(I) + trace(\frac{1}{\sigma_i^2} I \sigma_i^2 I) \\ 
& \phantom{= \frac{1}{2} \Big(} + \frac{1}{\sigma_i^2} \|(w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) f(i, z_i) + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i) \\ 
& \phantom{= \frac{1}{2} \Big( + \frac{1}{\sigma_i^2} \|} - (w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_0 + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i)\|^2_2 \Big) \\
& = \frac{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| f(i, z_i) - z_0 \|_2^2

\end{aligned}
\label{eq:KL_q_p_simplified}
\end{equation} %]]></script>

<p>It is also worth mentioning that there is an equivalent formulation where \eqref{eq:M_i_z_0_z_i}, \eqref{eq:p_z_i_minus_1_given_z_i}, and \eqref{eq:KL_q_p_simplified} are rewritten using $z_0 = \frac{z_i - \sqrt{1-\bar{\alpha}_i}\epsilon_i}{\sqrt{\bar{\alpha}_i}}$, the fact derived from \eqref{eq:gaussian_noise_addition}, as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& M_i(z_0, z_i) \\
& = w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) \frac{z_i - \sqrt{1-\bar{\alpha}_i}\epsilon_i}{\sqrt{\bar{\alpha}_i}} + w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i \\
& = \underbrace{\frac{-w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)\sqrt{1-\bar{\alpha}_i}}{\sqrt{\bar{\alpha}_i}}}_{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} \epsilon_i + \underbrace{\frac{w_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)w_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)}{\sqrt{\bar{\alpha}_i}}}_{\hat{w}_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i)} z_i \\
& \; \\
& p(z_{i-1}|z_i) \\
& = N(\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i) e(i, z_i) + \hat{w}_2(\bar{\alpha}_{i-1}, \bar{\alpha}_i) z_i, \sigma_i^2 I) \\
& \; \\
& KL[ q(z_{i-1}|z_i, z_0) \| p(z_{i-1}|z_i) ] \\
& = \frac{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| e(i, z_i) - \epsilon_i \|_2^2
\end{aligned}

\label{eq:equivalent}
\end{equation} %]]></script>

<p>Compared with $f(i, z_i)$ from the original form, the function $e(i, z_i)$ in the equivalent has a different meaning in that it is to estimate the noise present in $z_i$, and this change reportedly result in a better model.</p>

<h5 id="3-how-about-pz_0z_1">3. How about $p(z_0|z_1)$?</h5>
<p>Now that we have $f(i, z_i)$/$e(i, z_i)$ defined, which can be used to estimate $z_0$, it seems intuitively straightforward to share the same function and set $p(z_0|z_1)$ to:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& p(z_0|z_1) \\
& = N(f(1, z_1), \sigma^2 I) \\
& = N(\frac{z_1 - \sqrt{1-\bar{\alpha}_1}e(1, z_1)}{\sqrt{\bar{\alpha}_1}}, \sigma_1^2 I) 
\end{aligned}
\label{eq:p_z_0_z_1}
\end{equation} %]]></script>

<p>Lastly, while the exact results from the theoretical analysis above should be plugged into \eqref{eq:Obj_VDM} directly to complete the objective, in practice we often turn to the reweighted version of them (related to the idea of reweighted <strong>ELBO</strong>) for performace improvement. For example, in the case where $e(i, z_i)$ is adopted, rather than</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
& Obj_{\text{VDM}}(z_0) \\
& = \underbrace{E_{\epsilon_1 \sim N(0, I)} [ - log (\sqrt{det(2\pi\sigma_1^2 I)}) ]}_{\text{a constant}} \\
& \phantom{=} - E_{\epsilon_1 \sim N(0, I)} [ \frac{1}{2\sigma_1^2} \frac{1-\bar{\alpha}_1}{\bar{\alpha}_1} \| e(1, z_1) - \epsilon_1 \|_2^2 ] \\
& \phantom{=} - \underbrace{KL [ q(z_T|z_0) \| p(z_T) ]}_{= 0 \text{ by design}} \\
& \phantom{=} - \sum_{i=2}^T E_{\epsilon_i \sim N(0, I)} [ \frac{\hat{w}_1(\bar{\alpha}_{i-1}, \bar{\alpha}_i)^2}{2 \sigma_i^2} \| e(i, z_i) - \epsilon_i \|_2^2 ]
\end{aligned} %]]></script>

<p>the heuristic below is more commonly used for <strong>VDM</strong> training:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{heuristic VDM}}(z_0) \\
& = - E_{\epsilon_1 \sim N(0, I)} [ \| e(1, z_1) - \epsilon_1 \|_2^2 ] \\
& \phantom{=} - \sum_{i=2}^T E_{\epsilon_i \sim N(0, I)} [ \| e(i, z_i) - \epsilon_i \|_2^2 ] \\
& = - \sum_{i=1}^T E_{\epsilon_i \sim N(0, I)} [ \| e(i, z_i) - \epsilon_i \|_2^2 ] 
\end{aligned}
\label{eq:Obj_heuristic_VDM}
\end{equation} %]]></script>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2208.11970">Calvin Luo. Understanding Diffusion Models: A Unified Perspective. 2022.</a></li>
  <li><a href="https://arxiv.org/abs/2010.02502">Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. 2020.</a></li>
</ul>

</div>


</article>



<foot>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Derictory -->

	<script src="http://code.jquery.com/jquery-1.7.2.min.js"></script>
  <script src="http://yandex.st/highlightjs/6.2/highlight.min.js"></script>


<!-- Directory -->

<script src="/Blogs/assets/js/main.js"></script>

</foot>



  </div>

</body>


<footer class="site-footer">


	<center>
			
	<a class="tab-link" href="/Blogs/archive/">Archive</a> /
	<a class="tab-link" href="/Blogs/category/">Category</a> / 
	<a class="tab-link" href="/Blogs/tags/">Tags</a> 

	
	</center>
    

</footer>



</html>
