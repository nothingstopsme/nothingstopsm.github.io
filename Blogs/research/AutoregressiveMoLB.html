<!DOCTYPE html>
<html>
  <head>
      
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
	<script type="text/javascript"
	        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true},
		jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
		extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
		TeX: {
		extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
		equationNumbers: {
		autoNumber: "AMS"
		}
		}
		});
		</script>
	

<!-- CSS -->

  <link rel="stylesheet" href="/Blogs/assets/css/main.css">
  <link rel="canonical" href="/Blogs/research/AutoregressiveMoLB.html">
  <link rel="alternate" type="application/rss+xml" title="" href="/Blogs/feed.xml">

<!-- Google font -->

  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Noto Sans">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open Sans">

<!-- font awesome -->

	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
	
	<title>Priors That Unlock the Potential of VBVAE</title>

	


</head>


  


  </head>

  <body>


  <div class="wrapper">
          <header class="post-header">

    <center><div class="post-title" itemprop="name headline">Priors That Unlock the Potential of VBVAE</div>

		<div class="post-meta"><i class="fa fa-calendar-o"></i> <time datetime="15 Jan 2022" itemprop="datePublished">Jan 15 2022</time>

		&nbsp;&nbsp;•&nbsp;&nbsp;<i class="fa fa-user-secret"></i> <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Chao</span>
        
		<br>
		<!--<i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-pulse"></i></span>˚C</span>-->
	</div>

        
        <div class="post-tags">
        
		<a class="post-tags-item" href="/Blogs/tags/">Probability</a>
        
		<a class="post-tags-item" href="/Blogs/tags/">MachineLearning</a>
        
	</div>
    </center>
    
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
<div class="post-content">
    <center>
	
    </center>
</div>




<div class="post-content" itemprop="articleBody">
    <p>In my earlier post, a new variant of <strong>V</strong>ector <strong>Q</strong>uantised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder was proposed in an attempt to improve for better gradient estimation and efficient discrete coding. The essential difference is the use of bit-strings rather than integer codes to index vectors in a latent space, and therefore its name <strong>V</strong>ector <strong>B</strong>inarised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder was coined to reflect that distinction. Just as <strong>VQVAE</strong>, obtaining discrete representation is only the first half of the story, and this post is to complete the full picture by introducing a simple modification to autoregressive priors commonly used with <strong>VQVAE</strong>, and accordingly establishing similar generative models which takes advantage of <strong>VBVAE</strong> in handling a huge code space.</p>

<h2 id="recap-vbvae">Recap: <strong>VBVAE</strong></h2>
<p>The idea of <strong>VBVAE</strong> originated from the fact that the entire quantisation process (vectors $\implies$ codes $\implies$ embeddings) in <strong>VQVAE</strong> can be viewed as one full Gibbs step at which samples of the highest probabilities are always picked, and the sampling distributions are governed by a <strong>R</strong>estricted <strong>B</strong>oltzmann <strong>M</strong>achines whose conditional of hidden and visible variables given the other are a categorical distribution (under one-hot encoding) and a gaussian with identity covariance, respectively. Therefore by generalising the configuration of the <strong>RBM</strong> to have the following energy function $E_{\text{bi}}(v, h)$, as well as the resulting conditionals for $h$ (a random column vector of $K$ binary hidden variables) and $v$ (a random column vector of continuous visible variables):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
& E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h \\
\end{aligned} \\
\begin{aligned}
& P_{\text{bi}}(h|v) = \prod_{i=1}^{K} \frac{exp(S(v)[i] * h[i])}{\sum_{\hat{h}[i] \in \{0, 1\}} exp(S(v)[i] * \hat{h}[i])} \\
& \phantom{P_{\text{bi}}(h|v)} \propto \prod_{i=1}^{K} \sigma(S(v)[i])
\end{aligned}\\
\begin{aligned}
& P_{\text{bi}}(v|h) \propto N(\mu + \Sigma^{0.5} C h, \Sigma)
\end{aligned}
\end{cases}
\label{eq:RGBM}
\end{equation} %]]></script>

<p>with $\mu$ being the mean bias vector of $v$; $\Sigma$ being the diagonal covariance matrix of $v$; $C$ being a matrix of which each column corresponds to an embedding vector; $\sigma$ being the entrywise sigmoid operator; and $S(v)$ defined as $v^T \Sigma^{-0.5} C + a^T$ given the bias vector $a$ of $h$. Then based on the entrywise floor operator $\lfloor\;\rfloor$ a new quantisation function outputting discrete codes in the form of bit-strings, denoted by $Qn_{\text{bi}}$, can be attained:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& Qn_{\text{bi}}(v) \nonumber \\
& = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber \\
& = \mu + \Sigma^{0.5} C \lfloor \sigma(S(v)) + 0.5 \rfloor \label{eq:Qn_bi}
\end{align} %]]></script>

<p>For more detailed explanations and discussions please check this <a href="./VBVAE.html">post</a>.</p>

<h2 id="sampling-bit-strings-">Sampling Bit-strings <a name="sampling_bit-strings"></a></h2>
<p>One of the most exciting ways of utilising the codes generated from <strong>VQVAE</strong> is to fit a probabilistic model (such as autoregressive ones) over that discrete space as an external prior, and set up a generative process via first sampling codes from the learned distribution, and then decoding them with the decoder of <strong>VQVAE</strong> to obtain target samples. While this is still the case for <strong>VBVAE</strong>, the discrete space is now encoded in terms of bit-strings as opposed to integer codes, which means our probalistic models need to be capable of producing samples in that form. How can we do that?</p>

<p>Indeed, one can always treat bit-strings as some integer codes in its binary form, then with a quick conversion between these two discrete descriptions the same modelling techniques for codes from <strong>VQVAE</strong> can immediately apply; but ironically, that seemingly straightforward approach is rather backward from the perspective of <strong>VBVAE</strong>, as the return of integer codes brings back the limit on the size of the code space that can be manageable, restricting the power of <strong>VBVAE</strong> in exploiting the expressiveness of a greater number of codes (in the form of bit-string).</p>

<p>Thus for making the most out of <strong>VBVAE</strong> and its bit-string coding, an alternative, named “autoregressive <strong>M</strong>ixtures of <strong>L</strong>ogistic distributions for <strong>B</strong>it-strings”, is proposed based on existing autoregressive models over sequences of codes with a few modifications that turn them into ones over sequences of bit-strings. These changes include:</p>

<h5 id="normalisation-">Normalisation <a name="normalisation"></a></h5>
<p>As the state of a bit ($0$ or $1$) in a bit-string produced by <strong>VBVAE</strong> is determined by how the input reacts to the associated embedding in terms of dot-product, it can be regarded as a quantised intensity in response to the particular embedding as a filter. That perspective allows us to adopt a common practice of normalisation to transform $h$ to $z$ by shifting and rescaling bit values from a discrete space of $\{0, 1\}$ into a continuous one of $[-1, 1]$, which is often used to prepare quantised intensities like RGB colours for a more balanced value distribution across a desired range</p>

<h5 id="bit-strings-to-embeddings">Bit-strings to Embeddings</h5>
<p>The intesity view given <a href="#normalisation">above</a> also implies that entries of bit-strings can be interpreted as channels related to different abstract features; therefore one or several fully-connected layers taking as input normalised bit-strings would be a reasonable choice for embedding them into a continuous vector space, and by doing so to avoid the need of a code book listing every possible code for looking-up.</p>

<h5 id="parameterisation-for-molb">Parameterisation for <strong>MoLB</strong></h5>
<p>In order to target bit-strings directly, the last layer of autoregressive models for parameterising categorical distributions is replaced with the one for <strong>MoLB</strong>, a special case of the discretised logistic mixture likelihood introduced in <em>PixelCNN++</em> (see <a href="#included_image_Fig1">Fig1</a>). The density function of <strong>MoLB</strong> with $M$ components given a normalised bit-string $z$ is defined as follows (denoting half of the uniform distance between every two consecutive discrete outcomes by $d$, which in this case is $1$):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& P(z[i]|\pi, l[i], s[i]) = \sum_{j = 0}^{M} \pi_j P_j(z[i]|l_j[i], s_j[i]) \\
& P_j(z[i]|l_j[i], s_j[i]) = \begin{cases}
\sigma(\frac{-1 + d - l_j[i]}{s_j[i]}),\; if\;z[i] < -1 + d \\
1 - \sigma(\frac{1 - d - l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
& \phantom{P_j(z[i]|l_j[i], s_j[i]) } = \begin{cases}
\sigma(\frac{- l_j[i]}{s_j[i]}),\; if\;z[i] < 0 \\
1 - \sigma(\frac{- l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
& \phantom{P_j(z[i]|l_j[i], s_j[i]) } = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;z[i] < 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
& l_j[i] = w_j[i,i] + \sum_{1 \le k < i} w_j[i,k] * z[k]
\end{aligned}
\label{eq:MoLB}
\end{equation} %]]></script>

<p>where $\pi$ is a vector of mixture probabilities for the $M$ component; $l[i]$ and $s[i]$ are the mean and the scale parameter which describes the logistic distribution for $z[i]$; the subscript $j$ is used to indicate which component a parameter is associated with; and $w_j[i,k]$ (subscripted for the mixture component $j$) is the weight of $z[k]$ used in computing $l_j[i]$, the result of a linear combination of all entries preceding $z[i]$, plus a bias term $w_j[i,i]$. Note that if $\pi$, $w$, and $s$ are set to come from the underlying autoregressive model (can be of any type such as <em>PixelCNN</em> or <em>Transformer</em>) in an autoregressive <strong>MoLB</strong> architecture, each bit will also depend on all bit-strings positioned before the one in which that bit resides, and consequently the entire model can been seen acting autoregressive at bit-level.</p>

<figure>
	
  <a name="included_image_Fig1"></a>
	
	<img src="/Blogs/assets/images/2022-01-15-AutoregressiveMoLB/comparison_autoregressive_categorical_dist_vs_autoregressive_MoLB.png" alt="Fig1: The architecture comparison between an autoregressive model parameterised based on categorical distribution (left), and the one based on MoLB (right)" />
	<figcaption>
	Fig1: The architecture comparison between an autoregressive model parameterised based on categorical distribution (left), and the one based on MoLB (right)
	</figcaption>
</figure>

<h2 id="further-improvement-incorporating-bit-dependency-into-vbvae">Further Improvement: Incorporating Bit Dependency into VBVAE</h2>
<p>Maybe clever readers like you have already noticed that there is a discrepancy between the way bits in a bit-string are treated in \eqref{eq:MoLB} and in <strong>VBVAE</strong>: while autoregressive dependency is assumed present between these bits, they are actually generated by random variables designed to be (conditionally) independent! If our external prior model is to be parameterised in congruence with \eqref{eq:MoLB}, why not making the assumption more valid by constructing <strong>VBVAE</strong> to generate bits accordingly in the first place?</p>

<p>To that end, let us first derive $P_j(h[i]|l_j[i], s_j[i])$ from \eqref{eq:MoLB} by assigning the probability of $z[i] &lt; 0$ to $h[i] = 0$ and the one of $z[i] &gt;= 0$ to $h[i] = 1$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& P_j(h[i]|l_j[i], s_j[i]) = \begin{cases}
P_j(z[i] < 0|l_j[i], s_j[i]) if\;h[i] = 0 \\
P_j(z[i] >= 0|l_j[i], s_j[i]) if\;h[i] = 1 \\
\end{cases} \\
& \phantom{P_j(h[i]|l_j[i], s_j[i])} = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;h[i] = 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; if\;h[i] = 1
\end{cases} \\
& \phantom{P_j(h[i]|l_j[i], s_j[i])} \propto \sigma(\frac{l_j[i]}{s_j[i]}) \\
& \phantom{P_j(h[i]|l_j[i], s_j[i])} = \sigma(\underbrace{\frac{w_j[i,i]}{s_j[i]}}_{\hat{w}_j[i,i]} + \sum_{1 \le k < i} \underbrace{\frac{w_j[i,k]}{s_j[i]}}_{\hat{w}_j[i,k]} * z[k]) \\
& \phantom{P_j(h[i]|l_j[i], s_j[i])} = \sigma(\hat{w}_j[i,i] + \sum_{1 \le k < i} \hat{w}_j[i,k] * f_{\text{map}}(h[k])) \\
\end{aligned}
\label{eq:conditional_P_j_of_h_i}
\end{equation} %]]></script>

<p>which describes a logistic model for $h[i]$ taking as features its preceding bits with the feature mapping $f_{\text{map}}$. Based on the nature of \eqref{eq:conditional_P_j_of_h_i}, an adaptation named <strong>A</strong>utoregressive <strong>VBVAE</strong> is introduced, featuring this tweaked energy function $E_{\text{abi}}(v, h)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& E_{\text{abi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h \\
& \phantom{E_{\text{abi}}(v, h) =} - \sum_i h[i] \big( A[i, i] + \sum_{1 \le k < i} A[i,k] f_{\text{map}}(h[k]) \big)
\end{aligned}
\label{eq:E_abi}
\end{equation} %]]></script>

<p>where $A[i, k]$ is the weight associated with the pair ($h[i]$, $h[k]$), and $A[i, i]$ is substituted for the i-th entry of original bias vector $a$. Since there are no new terms involving $v$ added in \eqref{eq:E_abi}, the resulting conditional for $v$ remains unchanged ($P_{\text{abi}}(v|h) \propto N(\mu + \Sigma^{0.5} C h, \Sigma)$); however, its conditional for $h$ turns into a form similar to what is seen in \eqref{eq:conditional_P_j_of_h_i}:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
& P_{\text{abi}}(h|v) = P_{\text{abi}}(h[1]|v) \\
& \phantom{P_{\text{abi}}(h|v) =} * \prod_{i=2} P_{\text{abi}}(h[i]|v, \underbrace{h[1], \dotsc, h[i-1]}_{h_{<i}})
\end{aligned} \\
\begin{aligned}
& P_{\text{abi}}(h[1]|v) \propto exp\Big(h[1] * (v^T \Sigma^{-0.5} C)[1] + h[1] * A[1, 1] \Big) \\
& \phantom{P_{\text{abi}}(h[1]|v)} = exp\Big(h[1] * \big(\underbrace{(v^T \Sigma^{-0.5} C)[1] + A[1, 0]}_{S(v)[1]} \big) \Big) \\
& \phantom{P_{\text{abi}}(h[1]|v)} \propto \sigma\big(S(v)[1]\big)
\end{aligned} \\
\begin{aligned}
& P_{\text{abi}}(h[i]|v, h_{<i}) \\
& \propto exp\Big(h[i] * (v^T \Sigma^{-0.5} C)[i] \\ 
& \phantom{\, \propto exp\Big(} + h[i] * \big( A[i, i] + \sum_{1 \le k < i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
& = exp\Big(h[i] \\ 
& \phantom{= exp\Big(} * \big( \underbrace{(v^T \Sigma^{-0.5} C)[i] + A[i, i]}_{S(v)[i]} + \sum_{1 \le k < i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
& = exp\Big(h[i] * \big(S(v)[i] + \sum_{1 \le k < i} A[i,k] f_{\text{map}}(h[k]) \big) \Big) \\
& \propto \sigma\big(S(v)[i] + \sum_{1 \le k < i} A[i,k] f_{\text{map}}(h[k])\big) 
\end{aligned} \\
\end{cases}
\label{eq:conditional_P_abi_of_h}
\end{equation} %]]></script>

<p>which leads to $Qn_{\text{abi}}(v)$, the autoregressive version of \eqref{eq:Qn_bi}:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Qn_{\text{abi}}(v) \\
& = \mu \\
& \phantom{=} + \sum_{i=1}^{K} Qn_{\text{abi}}(v)_i \\ 
& = \mu \\
& \phantom{=} + \Sigma^{0.5} C \; one\_hot(1) * \lfloor \sigma\big(\underbrace{\hat{S}(v)[1] + A[1,1]}_{S(v)[1]}\big) + 0.5 \rfloor \\
& \phantom{=} + \dotsb \\
& \phantom{=} + \Sigma^{0.5} C \; one\_hot(K) \\ 
& \phantom{= +} * \lfloor \sigma\big(\underbrace{\hat{S}(v)[k] + A[K,K]}_{S(v)[K]} + \sum_{1 \le k < K} A[K,k] f_{\text{map}}(h[k])\big) + 0.5 \rfloor
\end{aligned}
\label{eq:Qn_abi}
\end{equation} %]]></script>

<p>where $one\_hot(i)$ stands for a column vector of which the only non-zero entry is the i-th one, which contains a value $1$. It should be pointed out that unlike <strong>VQVAE</strong> and <strong>VBVAE</strong>, \eqref{eq:Qn_abi} in <strong>AVBVAE</strong> does not necessarily produce samples with the highest probability (they are only locally optimal); nonetheless, it is still a valid function describing a deterministic way of quantisation, and thus <strong>AVBVAE</strong> exhibits the same attracting properties possessed by the two predecessors, such as less noise and a simplified training loss owing to the constant <strong>KL</strong> term.</p>

<h2 id="better-resource-efficiency-parameter-sharing-enabled-by-avbvae">Better Resource Efficiency: Parameter Sharing Enabled by AVBVAE</h2>
<p>In addition to modelling consistency, there is a bonus benefit when a <strong>AVBVAE</strong> and a autoregressive <strong>MoLB</strong> prior are used together. Recall that all the parameters involved in our quantisation functions are designed to be global; that means the autoregressive relationship in every bit-string generated by a <strong>AVBVAE</strong>, no matter where they are located in a sequence or what context the are conditioned on, will be overseen by a single dependency model, and hence it should be well sufficient to share autoregressive weights to decrease the number of parameters, and in turns the memory cost of the <strong>MoLB</strong> part, which would normally takes $O(N * M * K^2)$ for such a prior of $M$ components over sequences of a size ($N$).</p>

<p>Moreover, this parameter sharing is not just limited within autoregressive <strong>MoLB</strong> itself; it is also possible to reuse the autoregressive weights from <strong>AVBVAE</strong> and realise a memory complexity of $O(N * M * K)$! That reduction is based on the observation that the actual code distribution of a <strong>AVBVAE</strong>, which can be explicitly written out decomposed as follows (let $I(x, y) = 1$ when $x = y$ and $I(x, y) = 0$ when $x \neq y$):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& P_{\text{abi}}(h) \\
& = \int_v P_{\text{abi}}(\underbrace{\hat{S}(v)}_{=r}) \\ 
& \phantom{=} * P_{\text{abi}}(h[1]| \underbrace{\hat{S}(v)[1]}_{r[1]}) * \dotsb * P_{\text{abi}}(h[K]| \underbrace{\hat{S}(v)[K]}_{r[K]}, h_{<K}) \\
& = \int_r P_{\text{abi}}(r) \\ 
& \phantom{=} * I\Big(h[1], \lfloor \sigma\big(r[1] + A[1,1]\big) + 0.5 \rfloor\Big) \\
& \phantom{=} * \dotsb \\
& \phantom{=} * I\Big(h[K], \\
& \phantom{= * I\Big(} \lfloor \sigma\big(r[K] + A[K,K] + \sum_{1 \le k < K} A[K,k] f_{\text{map}}(h[k])\big) + 0.5 \rfloor\Big) \\
\end{aligned}
\label{P_code_of_h}
\end{equation} %]]></script>

<p>suggests that one only needs to model the contribution of uncertainty from $P(r)$ for the reconstruction of the code distribution; consequently, \eqref{eq:MoLB} might be reformulated as such:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& P(z[i]|\pi, l[i], s[i]) = \sum_{j = 0}^{M} \pi_j P_j(z[i]|l_j[i], s_j[i]) \\
& P_j(z[i]|l_j[i], s_j[i]) = \begin{cases}
1 - \sigma(\frac{l_j[i]}{s_j[i]}),\; if\;z[i] < 0 \\
\sigma(\frac{l_j[i]}{s_j[i]}),\; otherwise
\end{cases} \\
& l_j[i] = r_j[i] + A[i,i] + \sum_{1 \le k < i} A[i,k] * z[i]
\end{aligned}
\label{eq:MoLB_lite}
\end{equation} %]]></script>

<p>to approximate the distribution of $r$ through a categorical distribution $\pi$ for mixture components, while reuse the same linear model (which is fixed in \eqref{eq:MoLB_lite}) to allow the computation of losses and the learning process to be conducted in terms of $z$/$h$.</p>

<figure>
	
  <a name="included_image_Fig2"></a>
	
	<img src="/Blogs/assets/images/2022-01-15-AutoregressiveMoLB/autoregressive_MoLB_lite.png" alt="Fig2: Autoregressive MoLB lite, a low-memory-consumption version of autoregressive MoLB through parameter sharing" />
	<figcaption>
	Fig2: Autoregressive MoLB lite, a low-memory-consumption version of autoregressive MoLB through parameter sharing
	</figcaption>
</figure>

<h2 id="the-advantages">The Advantages</h2>
<p>As mentioned earlier, the motivation of autoregressive <strong>MoLB</strong> is to allow the direct distribution modelling over sequences of bits-strings generated by <strong>VBVAE</strong>, and circumvent the use of code books as well as categorical distributions; hence as the expressiveness of the discrete representation increases, i.e. more bits involved, training an external prior can still be manageble, without being overwhelmed by the demand for memory resources in handling a large code space; moreover, the likely memory requirement of <strong>MoLB</strong> itself can also be relaxed with the introduction of <strong>AVBVAE</strong>, where further parameter reduction to a linear complexity with respect to the number of bits is achievable. In other words, when it comes to building a <strong>VBVAE</strong>/<strong>AVBVAE</strong>-based generative model, the expressive potential of <strong>VBVAE</strong>/<strong>AVBVAE</strong> is better fulfilled via the memory efficiency of autoregressive <strong>MoLB</strong>.</p>

<p>Besides, as a special case of the discretised logistic mixture likelihood proposed in <em>PixelCNN++</em>, <strong>MoLB</strong> also enjoys the fact that its likelihood function produces denser gradients. That means the learning of a autoregressive <strong>MoLB</strong> could benefit from stronger training signals, which leads to possibly faster convergence.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1711.00937">Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete RepresentationLearning”. In:CoRR abs/1711.00937 (2017)</a></li>
  <li><a href="https://arxiv.org/abs/1701.05517">Tim Salimans et al. “PixelCNN++: Improving the PixelCNN with Discretized Logistic Mix-ture Likelihood and Other Modifications”. In:CoRRabs/1701.05517 (2017)</a></li>
</ul>

</div>


</article>



<foot>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Derictory -->

	<script src="http://code.jquery.com/jquery-1.7.2.min.js"></script>
  <script src="http://yandex.st/highlightjs/6.2/highlight.min.js"></script>


<!-- Directory -->

<script src="/Blogs/assets/js/main.js"></script>

</foot>



  </div>

</body>


<footer class="site-footer">


	<center>
			
	<a class="tab-link" href="/Blogs/archive/">Archive</a> /
	<a class="tab-link" href="/Blogs/category/">Category</a> / 
	<a class="tab-link" href="/Blogs/tags/">Tags</a> 

	
	</center>
    

</footer>



</html>
