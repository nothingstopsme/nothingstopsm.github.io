<!DOCTYPE html>
<html>
  <head>
      
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
	<script type="text/javascript"
	        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']],
		processEscapes: true},
		jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
		extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
		TeX: {
		extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
		equationNumbers: {
		autoNumber: "AMS"
		}
		}
		});
		</script>
	

<!-- CSS -->

  <link rel="stylesheet" href="/Blogs/assets/css/main.css">
  <link rel="canonical" href="/Blogs/research/VBVAE.html">
  <link rel="alternate" type="application/rss+xml" title="" href="/Blogs/feed.xml">

<!-- Google font -->

  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Noto Sans">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open Sans">

<!-- font awesome -->

	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
	
	<title>From VQVAE to VBVAE</title>

	


</head>


  


  </head>

  <body>


  <div class="wrapper">
          <header class="post-header">

    <center><div class="post-title" itemprop="name headline">From VQVAE to VBVAE</div>

		<div class="post-meta"><i class="fa fa-calendar-o"></i> <time datetime="18 Jan 2021" itemprop="datePublished">Jan 18 2021</time>

		&nbsp;&nbsp;•&nbsp;&nbsp;<i class="fa fa-user-secret"></i> <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Chao</span>
        
		<br>
		<!--<i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-pulse"></i></span>˚C</span>-->
	</div>

        
        <div class="post-tags">
        
		<a class="post-tags-item" href="/Blogs/tags/">Probability,</a>
        
		<a class="post-tags-item" href="/Blogs/tags/">MachineLearning</a>
        
	</div>
    </center>
    
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
<div class="post-content">
    <center>
	
    </center>
</div>




<div class="post-content" itemprop="articleBody">
    <p>Living in the Digital Age, we have witnessed how important that the notion of “digitalisation” is in the advance of modern technologies. Arguably the main advantange brought by digitalisation, or quantisation from the perspective of digital signal processing, is the transformation of complicated signals into a form which can be not only processed and stored efficiently, but also robust to noise. Lately, such an idea has also be deployed into the field of machinge learning, and led to a new model of neural networks called <strong>V</strong>ector <strong>Q</strong>uantised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder. While the modelling method has been proven successful in many applications, issues have been observed in my attempt to incorporate it into my recent project, where <strong>VQVAE</strong> have had difficulty to deal with a large code space. Therefore, a new variant named <strong>V</strong>ector <strong>B</strong>inarised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder, which retains the spirit and benefit of quantisation but does not suffer from the shortcomings of the original <strong>VQVAE</strong>, is proposed, and in this post the derivation from <strong>VQVAE</strong> to <strong>VBVAE</strong> as well as the intuitions behind it are presented. Note that no thorough and comprehensive experiments have been conducted to support this new concept, so please read with a huge grain of salt. ; )</p>

<h2 id="review-vqvae">Review: <strong>VQVAE</strong></h2>
<p>For a data point $x$, the objective (or called <strong>E</strong>vidence <strong>L</strong>ower <strong>BO</strong>und in some literature) of a <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder to be maximised is normally written as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{VAE}}(x) = E_{z \sim q(z|x)}[log(p(x|z))] \\ 
& \phantom{Obj_{\text{VAE}}(x) =\;} + E_{z \sim q(z|x)}[log(\frac{p(z)}{q(z|x)})]
\end{aligned}
\label{eq:ELBO_VAE}
\end{equation} %]]></script>

<p>where $z$ is a point in the latent space; $q(z|x)$, $p(x|z)$, and $p(z)$ are the approximate posterior distribution, likelihood, and prior distribution over the joint space of $x$ and $z$, respectively. In the common setup of <strong>VAE</strong>, $q(z|x)$ is usually described by some parametric distribution, whose parameters are generated by a neural network (referred to as encoder $En$) taking $x$ as input. Such an arrangement is also adopted in <strong>VQVAE</strong>, excpet that the authors have designed a special distribution for $q(z|x)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& q(z = e_k|x) \\
& = \begin{cases}
1 \;\;\; \text{if }e_k = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| En(x) - e_i \|_2 \\
0 \;\;\; \text{otherwise}
\end{cases}
\end{aligned}
\label{eq:q_VQVAE}
\end{equation} %]]></script>

<p>with $\{e_1,\dotsc,e_K\}$ being a codebook, a set of $K$ codes of which the dimensionnality is the same as the one of $En(x)$. This particular choice turns the process of sampling from $q(z|x)$ into the one which simply maps $En(x)$ to one of the codes via nearest-neighbour-search, and hence the latent space is effectively constrained to contain only $K$ instead of an infinite number of points; in other words, under \eqref{eq:q_VQVAE} the whole latent space can be considered quantised into $K$ discrete values, and $z$ is just the quantised result of $En(x)$ (formulated as $z = Qn(En(x))$). Note that by assuming the prior is a uniform distribution, i.e. $p(z) = \frac{1}{K}, z \in \{e_1,\dotsc,e_K\}$, \eqref{eq:ELBO_VAE} in this case can be further simplified to:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{Qn}}(x) = log(p(x|Qn(En(x))) + log(\underbrace{p(Qn(En(x)))}_{= \frac{1}{K}}) \\
& \phantom{Obj_{Qn}(x) =} + \underbrace{Entropy(q(z|x))}_{= 0} \\
& \phantom{Obj_{Qn}(x)} = log(p(x|Qn(En(x))) + log(\frac{1}{K})
\end{aligned}
\label{eq:ELBO_Qn}
\end{equation} %]]></script>

<p>which implies for the purpose of maximisation, the effective objective is simply $log(p(x|Qn(En(x)))$. Practically, the direct optimisation of $log(p(x|Qn(En(x)))$ via gradient ascent is impossible, since there is no gradient defined for the quantisation function $Qn$; as a result, the authors of <strong>VQVAE</strong> proposed an alternative where an straight-through gradient estimator is used to pass directly to $En(x)$ the gradients that are supposed into $Qn$, and this bypassing operation can be realised by the stop-gradient operator (denoted as $SG$):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{VQVAE}}(x) \\ 
& = log(p(x| SG(Qn(En(x)) - En(x)) + En(x))) \\ 
& \phantom{=\;}+ \beta \| En(x) - SG(Qn(En(x))) \|_2^2 \\
\end{aligned}
\label{eq:ELBO_VQVAE}
\end{equation} %]]></script>

<p>Note that the second term in \eqref{eq:ELBO_VQVAE} is purposely introduced to serve as a regulariser weighted by the hyper-parameter $\beta$. It corresponds to the fact that other objectives, which will differ from the goal described by \eqref{eq:ELBO_Qn}, are required to provide training signals for $Qn$, as they no longer receive any gradients under \eqref{eq:ELBO_VQVAE}. Once additional optimisation targets dedicated to $Qn$ are incorporated, there will be two disjoint gradient flows (one for parameters involved in the first term of \eqref{eq:ELBO_VQVAE}, and the other for the codebook involved in $Qn$) present in the training dynamic; therefore it needs some form of regularisation, such as the one introduced here, to keep their updates synchronised and make sure the output space of $En$ is learned at the same pace as the codeboock is.</p>

<p>In relation to providing training signals for updating the codebook, one popular strategy is to regard the training of the codebook used in nearest-neighbour-search as K-mean learning, in which codes (or mean vectors from the perspective of K-mean) are updated via <em>on-line EM</em> with the following rules:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
& N_i^{(t)} =  N_i^{(t-1)} * \gamma + n_i^{(t)} * (1-\gamma)
\end{aligned}\\
\begin{aligned}
& e_i^{(t)} = e_i^{(t-1)} * \gamma + \sum_{\substack{j \\ Qn(En(x_j)) = e_i^{(t-1)}}} \frac{En(x_j)}{N_i^{(t)}} * (1 - \gamma)
\end{aligned}
\end{cases}
\label{eq:K_Mean_online_EM}
\end{equation} %]]></script>

<p>where $N_i^{(t)}$ is the average use count for code $i$ up to iteration $t$; $n_i^{(t)}$ is the recorded usage count for code $i$ at iteration $t$; and $\gamma$ is the decaying rate. Despite being effective in many scenarios, training the codebook in this way when the number of codes becomes large could suffer from <em>index collapse</em>, where the utilisation of codebook capacity is inefficient due to the increasing difficulty of exploring and updating the entire code space under nearest-neighbour-search.</p>

<h2 id="alternative-interpretation-of-quantisation-in-vqvae-">Alternative Interpretation of Quantisation in <strong>VQVAE</strong> <a name="alternative_interpretatino"></a></h2>
<p>In the previous section, the quantisation function $Qn$ was introduced to neatly describe the action of sampling from the distribution \eqref{eq:q_VQVAE}, but there was no explicit definition given. Now it is time to make up for it by expressing the nearest-neighbour-search with the condition from \eqref{eq:q_VQVAE}:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Qn(v) \\
& = argmin_{e_i \in \{e_1,\dotsc,e_K\}} \| v - e_i \|_2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \| v - e_i \|_2^2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} -\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2 \\
& = argmax_{e_i \in \{e_1,\dotsc,e_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v \odot e_i - \frac{1}{2} \| e_i \|_2^2)
\end{aligned}
\label{eq:Qn_rewriting}
\end{equation} %]]></script>

<p>Next, let $C$ denote a matrix where the i-th column corresponds to the code $e_i$, $b$ denote a column vector where the i-th entry is equal to $-\frac{1}{2} \| e_i \|_2^2$, and $h_i$ denote a one-hot column vector where the i-th entry is 1; the last equation of \eqref{eq:Qn_rewriting} can be rewritten in the following way:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Qn(v) \\
& = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)) \\
& = C (argmax_{h_i \in \{h_1,\dotsc,h_K\}} \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{Z}) \\
& = C h_{max} = e_{max}
\end{aligned}
\label{eq:Qn_special_RBM}
\end{equation} %]]></script>

<p>where $h_{max}$ and $e_{max}$ are used to refer to the corresponding vectors picked under the $argmax$ operator; and $Z$ is a constant with respect to all $h_i$. If $Z$ is deliberately chosen to be $\sum_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)$, then $h_{max}$ can be viewed as the result of picking the most likely outcome from the categorical distribution $P(h_i|v)$, which is defined as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& P(h_i|v) \\
& = \frac{exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(-\frac{1}{2} \|v \|_2^2 + v^T C h_j + b^T h_j)} \\
& = \frac{exp(v^T C h_i + b^T h_i)}{\sum\limits_{h_j \in \{h_1,\dotsc,h_K\}} exp(v^T C h_j + b^T h_j)} \\
\end{aligned}
\label{eq:P_h_i}
\end{equation} %]]></script>

<p>and that is exactly the expression one would derive for the conditional distribution (hidden given visible) from a special case of <strong>R</strong>estricted <strong>B</strong>oltzmann <strong>M</strong>achines, in which binary hidden variables are grouped together to represent a categorical one; visible variables are set to be independent gaussian with 0 mean and 1 standard deviation; and the corresponding energy function is $E(v, h_i) = \frac{1}{2} \|v \|_2^2 - v^T C h_i - b^T h_i$. Moreover, as such a <strong>RBM</strong> also suggests $P(v|h_i) = N(C h_i, I)$, \eqref{eq:Qn_special_RBM} essentially depicts $Qn(v)$ as taking one Gibbs step starting from $v$, except that there is no randomness involved: samples at the first half of the step ($v \to h_{max}$) are always the outcomes with highest probability, while samples at the second half of the step ($h_{max} \to e_{max}$) are simply the distribution mean.</p>

<h2 id="generalising-to-vbvae">Generalising to VBVAE</h2>
<p>The <strong>RBM</strong> interpretation not only sheds light on how the quantisation function in a <strong>VQVAE</strong> works, but also suggests that very component needed to compose a quantisation function: A <strong>RBM</strong> with discrete hidden variables and continuous visible variables. So it is possible to define other forms of qunantisation by introducing different configurations of <strong>RBM</strong>s which provide the similar functionality, for example, the generic one with independent binary hidden variables and independent guassian visible variables (denoted by $h$ and $v$ respectively as in <a href="#alternative_interpretatino">the last section</a>), whose energy function and conditionals are:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{cases}
\begin{aligned}
& E_{\text{bi}}(v, h) = \frac{1}{2} (v - \mu)^T \Sigma^{-1} (v - \mu) - v^T \Sigma^{-0.5} C h - a^T h \\
\end{aligned} \\
\begin{aligned}
& P_{\text{bi}}(h|v) = \frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}}exp(-E_{\text{bi}}(v, \hat{h}))} \\
& \phantom{P_{\text{bi}}(h|v)} = \frac{exp(v^T \Sigma^{-0.5} C h + a^T h)}{\sum_{\hat{h}} exp(v^T \Sigma^{-0.5} C \hat{h} + a^T \hat{h})} \\
& \phantom{P_{\text{bi}}(h|v)} = \frac{exp((\overbrace{v^T \Sigma^{-0.5} C + a^T}^{= S(v)^T}) h)}{\sum_{\hat{h}} exp((\underbrace{v^T \Sigma^{-0.5} C + a^T}_{= S(v)^T}) \hat{h})} \\
& \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} \frac{exp(S(v)[m] * h[m])}{\sum_{\hat{h}[m] \in \{0, 1\}} exp(S(v)[m] * \hat{h}[m])} \\
& \phantom{P_{\text{bi}}(h|v)} = \prod_{m=1}^{K} P_{\text{bi}}(h[m]|v)
\end{aligned}\\
\begin{aligned}
& P_{\text{bi}}(v|h) = \frac{exp(-E_{\text{bi}}(v, h))}{\int_{\hat{v}}exp(-E_{\text{bi}}(\hat{v}, h))} \\ 
& \phantom{P_{\text{bi}}(v|h)} = N(\mu + \Sigma^{0.5} C h, \Sigma)
\end{aligned}
\end{cases}
\label{eq:RGBM}
\end{equation} %]]></script>

<p>where $\mu$ is the mean vector; $\Sigma$ is the covariance matrix with all off-diagnal entries equal to 0; $C$ is the same matrix as defined previously; $a$ is the bias vector (which is a free parameter not related to the length of codes, unlike $b$ in \eqref{eq:Qn_special_RBM}); and $S(v) = C \Sigma^{-0.5} v + a$. Also note that under this <strong>RBM</strong>, $h$ is no longer a one-hot vector but a bit string of which each entry independently contains a bit 1 or a bit 0. With \eqref{eq:RGBM}, a new quantisation function can be defined accordingly:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& Qn_{\text{bi}}(v) \nonumber \\
& = \mu + \Sigma^{0.5} C h_{max} \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} exp(-E_{\text{bi}}(v, h))) \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} \underbrace{\frac{exp(-E_{\text{bi}}(v, h))}{\sum_{\hat{h}} exp(-E_{\text{bi}}(v, h))}}_{= P_{\text{bi}}(h|v)}) \nonumber \\
& = \mu + \Sigma^{0.5} C (argmax_{h} \prod_m P_{\text{bi}}(h[m]|v)) \nonumber \\
& = \mu + \Sigma^{0.5} C 
\begin{bmatrix} 
argmax_{h[1]} P_{\text{bi}}(h[1]|v) \\
\vdots \\
argmax_{h[K]} P_{\text{bi}}(h[K]|v)
\end{bmatrix} \nonumber \\
& = \mu + \Sigma^{0.5} C
\begin{bmatrix}
\lfloor \sigma(S(v)[1]) + 0.5 \rfloor \\
\vdots \\
\lfloor \sigma(S(v)[K]) + 0.5 \rfloor
\end{bmatrix} \nonumber \\
& = \mu + \Sigma^{0.5} C \lfloor \sigma(S(v)) + 0.5 \rfloor \label{eq:Qn_bi} \\
& = e_{max} \nonumber
\end{align} %]]></script>

<p>with $\sigma(.)$ denoting entrywise the sigmoid function and $\lfloor . \rfloor$ representing entrywise the floor operator. Compared with \eqref{eq:Qn_special_RBM}, the troubling $argmax$ operators, which are the cause of undefined gradients for $Qn$, has disappeared in \eqref{eq:Qn_bi}. While the introduction of floor operators induces another blockage of gradient flows, the same straight-through gradient estimator can still be utilised to send training signals over; in fact, such an estimation becomes more effective and accurate when applied to \eqref{eq:Qn_bi} (see <a href="#included_image_Fig1">Fig1</a>), as not only does the number of nodes along the gradient path affected by the rerouting is reduced, but no parameters are excluded due to the bypass, which means all parameters can receive the training signals generated with respect to the true objective.</p>

<figure>
	
  <a name="included_image_Fig1"></a>
	
	<img src="/Blogs/assets/images/2021-01-18-VBVAE/Qn_vs_Qn_bi.png" alt="Fig1: The comparison between the straight-through gradient estimator for $Qn(v)$ (left) and the one for $Qn_{\text{bi}}(v)$ (right)" />
	<figcaption>
	Fig1: The comparison between the straight-through gradient estimator for $Qn(v)$ (left) and the one for $Qn_{\text{bi}}(v)$ (right)
	</figcaption>
</figure>

<p>Finally, by the substitution of $Qn_{\text{bi}}$ for $Qn$ in \eqref{eq:ELBO_Qn}, an new variant of <strong>VQVAE</strong>, named <strong>V</strong>ector <strong>B</strong>inarised <strong>V</strong>ariational <strong>A</strong>uto<strong>E</strong>ncoder, is established. The notion of binarisation, as the obvious distinction between them, comes from the fact that under $Qn_{\text{bi}}$, discrete codes are indexed by $h$ of the form of bit strings instead of one-hot vectors; as a result, picking a representing code in this quantisation process can be viewed as though deciding whether each bit in $h$ should be on or off. Besides, the objective for <strong>VBVAE</strong> highlights another difference:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
& Obj_{\text{VBVAE}}(x) \\ 
& = log\Bigg(p\bigg(x| SG\Big( \lfloor \sigma(S(En(x))) + 0.5 \rfloor - \sigma(S(En(x))) \Big) \\ 
& \phantom{= log\Bigg(p\bigg(x|\;} + \sigma(S(En(x))) \bigg)\Bigg) \\
\end{aligned}
\label{eq:ELBO_VBVAE}
\end{equation} %]]></script>

<p>which unlike <strong>VQVAE</strong> does not require a separate optimisation target to provide training signals for the codebook, and therefore involves no regularisers.</p>

<h2 id="the-advantanges-of-vbvae-over-vqvae">The Advantanges of VBVAE over VQVAE</h2>

<p>Firstly, as mentioned earlier, all parameters in <strong>VBVAE</strong> receive (estimate) gradients with respect to \eqref{eq:ELBO_Qn} directly. That allows the removal of additional objectives and regularisers which are essential in <strong>VQVAE</strong> to the contrary, and thus the consequent change in the optimisation contour caused by those extra terms, which might have a negative impact on achieving the main goal, could be avoided.</p>

<p>Secondly, since codes in <strong>VBVAE</strong> are indexed by bit strings, the number of distinct codes that a codebook is capable of representing can be as large as $2^{|\text{the size of the codebook}|}$; in other words, to model a discrete space containing $K$ codes might only take a codebook of the size $log_2(K)$, rather than $K$ as required in <strong>VQVAE</strong>.</p>

<p>Thirdly, instead of \eqref{eq:K_Mean_online_EM} in <strong>VQVAE</strong>, where individual codes are trained only when being used, in <strong>VBVAE</strong> the quantisation to any code always involves the entire codebook (as well as $\Sigma$ and $a$), all of which are trained as a result; that implies every update in <strong>VBVAE</strong> will revise the whole code space, or equivalently, revise every code in it, regardless of the usage of codes, and hence <strong>VBVAE</strong> is less susceptible to the issue of <em>index collapse</em>.</p>

<p>Overall, the efficiency in code use and training suggests the competence of <strong>VBVAE</strong> for handling large code spaces.</p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1711.00937">Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete RepresentationLearning”. In:CoRR abs/1711.00937 (2017)</a></li>
  <li><a href="https://arxiv.org/abs/1803.03382">Lukasz Kaiser et al. “Fast Decoding in Sequence Models using Discrete Latent Variables”.In:CoRRabs/1803.03382 (2018)</a></li>
  <li><a href="https://arxiv.org/abs/1906.00446">Ali Razavi, Aäron van den Oord, and Oriol Vinyals. “Generating Diverse High-Fidelity Images with VQ-VAE-2”. In:CoRRabs/1906.00446 (2019)</a></li>
</ul>

</div>


</article>



<foot>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Derictory -->

	<script src="http://code.jquery.com/jquery-1.7.2.min.js"></script>
  <script src="http://yandex.st/highlightjs/6.2/highlight.min.js"></script>


<!-- Directory -->

<script src="/Blogs/assets/js/main.js"></script>

</foot>



  </div>

</body>


<footer class="site-footer">


	<center>
			
	<a class="tab-link" href="/Blogs/archive/">Archive</a> /
	<a class="tab-link" href="/Blogs/category/">Category</a> / 
	<a class="tab-link" href="/Blogs/tags/">Tags</a> 

	
	</center>
    

</footer>



</html>
