

<html data-theme="dark">
  <head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	
  <!-- font awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

  <link rel="stylesheet" href="/Blogs/static/main/css/classless.css">

  <style>

    @import url('https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&display=swap');


    html[data-theme='dark'] {
      /* foreground   | background color */
      --cfg:   #cdcdcd; --cbg:    #1b1b1b;
      --cdark: #999;    --clight: #343434; /*#333;*/
      --cmed:  #566;
      --clink: #1ad;
      --cemph: #0b9;    --cemphbg: #0b91;


      --font-p: 1.2em/1.7 "Fira Mono", monospace;
      --font-h: .9em/1.5 "Fira Mono", monospace; 
      --font-c: .9em/1.4 "Fira Mono", monospace;

      --width: 54rem;
      --navpos: fixed;
	    --ornament: "";
    }

    article * {
      scroll-margin-top: 9rem; 
    }

    p,
    article table{
      margin-top: 3rem;
    }

    article figure {
      text-align: center;
    }

    article figure img {
      background: #cdcdcd;
    }

    article h3:before {
      left: -0.5rem;
      margin-left: 0rem;
      font-size: 1.5rem;
    }

    article h2:before {
      left: -0.5rem;
      margin-left: -2rem;
      font-size: 1.5rem;
    }

    article h2.reference-title:before {
      display: none;
    }

    article ul {
      overflow: hidden;
    }

    

    a[href] {
      text-decoration: none;
    }

    

    nav ul:not(:first-child) li {
      padding: 0;
      margin: 0;
    }


    nav input[type="checkbox"]:hover + label, 
    nav input[type="checkbox"]:focus + label, 
    nav a.nav-active,
    nav a:hover, 
    nav a:focus {
      background: black;
    }


    nav li > a {
      display: block;
    }

    nav > span {
      height: inherit;
      background: inherit;
    }

    nav > span > input[type="checkbox"] {
      width: 0;
    }

    nav > span > input[type="checkbox"] + label { 
      color: var(--clink); cursor: pointer; 
    }


    nav > span > ul {
      background: inherit;
      display: inline-block;
      width: auto;
      margin: 0;
      padding: 0;
    }

    nav > span > input[type="checkbox"] + label {
      height: inherit;
      padding: 1rem 0.6rem;
    }

    nav > span > ul > li {
      height: 4rem;
      display: inline-block;
    }

    nav > span > ul > li > a {
      height: inherit;
      padding: 1rem 0.6rem;
    }

   
    nav > span.left-menu {
      float: left;
    }

    nav > span.left-menu > input[type="checkbox"] + label {
      float: left;
      display: none;
    }

    nav > span.left-menu > input[type="checkbox"] + label + ul {
      float: left;
      clear: left;
      
    }
    
    nav > span.left-menu > input[type="checkbox"] + label + ul > li{
      float: left;
    }

     
    nav > span.right-menu {
      float: right;
    }


    nav > span.right-menu > input[type="checkbox"] + label {
      float: right;
      display: inline-block;
    }

    nav > span.right-menu > input[type="checkbox"] + label + ul {
      display: none;
      float: right;
      clear: right;
      overflow-y: auto;
      max-height: 50vh;

      border: var(--border);
      border-radius: 4px;

    }

    nav > span.right-menu > input[type="checkbox"] + label + ul > li {
      display: block;
    }

    nav > span.right-menu > input[type="checkbox"]:hover + label + ul,
    nav > span.right-menu > input[type="checkbox"]:focus + label + ul,
    nav > span.right-menu > input[type="checkbox"] ~ ul:hover {
        display: inline-block;
    }

   
    body>nav {
      
      left: max(calc(50vw - var(--width)/2), 0vw);
      right: max(calc(50vw - var(--width)/2), 0vw);
      width: auto;
      height: 4rem;
      box-shadow: none;
      z-index: 100;
      overflow-y: visible;

    }

		article div.post-tags span {
			white-space: nowrap;
		}

    article div.post-tags span:not(:first-child):before
    {
			content: "|";
    }
  
   
    @media (max-width: 40rem) 
    {
      nav > span.left-menu > input[type="checkbox"] + label + ul > li {
        float: none;
        display: block;
      }
   
      nav > span.left-menu > input[type="checkbox"] + label {
        display: inline-block;
      }
      
      nav > span.left-menu > input[type="checkbox"] + label + ul {
        display: none;
      }

      nav > span.left-menu > input[type="checkbox"]:hover + label + ul,
      nav > span.left-menu > input[type="checkbox"]:focus + label + ul,
      nav > span.left-menu > input[type="checkbox"] + label + ul:hover {
        display: inline-block;
     

        overflow-y: auto;
        max-height: 50vh;

        border: var(--border);
        border-radius: 4px;
      }
          

    }

  </style>

  <title>
A Dissection of Attention: Kernel Interpretation
</title>
  
	
  </head>

<body style="margin-top: 0; padding-top: 0;">
<nav>
  <span class="left-menu">
  <input type="checkbox" id="nav-menu" />
  <label for="nav-menu">
  &nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;
  </label>

  <!--<a href="#">&nbsp;<i class="fas fa-bars"></i>&nbsp;&#x25BE;</a>-->
  <ul>
    <!--<li class="menu-hamburger">
    </li>-->
    
      
          
          <li>
            <a href="/Blogs/archives/"
            
            

            >Archives</a>                            
          </li>
      
          
          <li>
            <a href="/Blogs/tags/"
            
            

            >Tags</a>                            
          </li>
      
    
  </ul>
  </span>

  
  

</nav>

<main style='margin-top: 4rem; padding: 0 2rem; overflow-y: hidden;'>
  <article>
      
<script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    /*inlineMath: [['$','$'], ['\\(','\\)']],*/
    inlineMath: [['$','$']],
    processEscapes: true},
    jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
    TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
    equationNumbers: {
    autoNumber: "AMS"
    }
  }
});
</script>


<h1>A Dissection of Attention: Kernel Interpretation</h1>


<div class="post-tags">
  
  <span>
  <i class="fas fa-calendar"></i>&nbsp;<time>13.May.2021</time>
  </span>
  
  
  
  <span>
  <i class="fas fa-tags"></i>&nbsp;MachineLearning
  </span>
  
  
</div>



<p>In the field of machine learning, one of the most influential breakthroughs would be no doubt the advent of attention, and it has become a common technique applied in many different types of learning problems. Such prevalence, however, somehow had left me with the impression that it had been just a handy trick to conduct a data-dependent and normalised linear combination until recently, when I once again stood corrected. Brilliant researchers has now demonstrated how attention could be examined through the lens of kernels, which opens up the possibility for computing the attention weights in a much more efficient way. Surprisingly, its derivation only requires the basic understanding of the conventional <em>kernel trick</em>, and hence this resulting post contains my ~~successful~~ attempt to go through that idea with my rudimentary knowledge about kernels.</p>
<h2 id="what_are_kernels">What Are Kernels?</h2>
<p>Since the new interpretation is from the perspective of kernels, please bear with me digressing to that question for a moment, and here is my rather simplified and informal answer: an alternative way to compute the dot product of any two vectors under the associated feature mapping. For example, given a mapping function $\phi:\mathbb{R}^m \to \mathbb{R}^n$, for any two column vectors $x, t \in \mathbb{R}^m$ the kernel $Ker_{\phi}$ (corresponding to $\phi$) will satisfy ($\odot$ stands for the dot product):</p>
<p>$$
\begin{equation}
\begin{aligned}
& Ker_{\phi}(x, t) = Ker_{\phi}(t, x) = \phi(x) \odot \phi(t)
\end{aligned}
\label{eq:kernel_definition}
\end{equation}
$$</p>
<p>Note that $Ker_{\phi}$ is defined as a function taking $x$ and $t$ as its inputs. It suggests that if the evaluation of $Ker_{\phi}$ does not involve the associated feature mapping, then one can obtain the dot product of $\phi(x)$ and $\phi(t)$ without needing to know the explicit forms of those vectors under the mapping, and that is the most common use case when it comes to the application of the concept of kernels. Why? Imagine that $\phi$ is a complicated function which produces vectors of large dimensionality ($n \gg m$); a dot product in the output space of such a $\phi$ requires potentially high complexity of $O(n)$, whereas a cleverly designed $Ker_{\phi}$ might only take the complexity as small as $O(m)$. That is to say, even with a very expressive $\phi$ (which incurs the surge in the dimensionality of its output space) the complexity of computing the dot products can still be kept at a manageable level, via the corresponding kernel $Ker_{\phi}$.</p>
<p>To demonstrate such exploitation, consider the following kernel:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Ker_{\phi_{exp}}(x, t) = exp(x \odot t) = \prod_{i=0}^{m-1} exp(x_i * t_i)
\end{aligned}
\label{eq:exponetial_kernel}
\end{equation}
$$</p>
<p>Each term $exp(x_i * t_i)$ can be expressed in terms of <em>Taylor series</em> at $x_i * t_i = 0$, which yields</p>
<p>$$
\begin{equation}
\begin{aligned}
& exp(x_i * t_i) \\
& = 1 + \frac{x_i * t_i}{1!} + \frac{(x_i * t_i)^2}{2!} + \frac{(x_i * t_i)^3}{3!} + \dotsb \\
& = 1 + \frac{x_i}{\sqrt{1!}} * \frac{t_i}{\sqrt{1!}} + \frac{x_i^2}{\sqrt{2!}} * \frac{t_i^2}{\sqrt{2!}} + \frac{x_i^3}{\sqrt{3!}} * \frac{t_i^3}{\sqrt{3!}} + \dotsb \\
& = [1, \frac{x_i}{\sqrt{1!}}, \frac{x_i^2}{\sqrt{2!}}, \frac{x_i^3}{\sqrt{3!}}, \dotsc] \odot [1, \frac{t_i}{\sqrt{1!}}, \frac{t_i^2}{\sqrt{2!}}, \frac{t_i^3}{\sqrt{3!}}, \dotsc] \\
& = {\phi_{exp}}_{i}(x) \odot {\phi_{exp}}_{i}(t)
\end{aligned}
\label{eq:exponetial_kernel_term_i_in_taylor_series}
\end{equation}
$$</p>
<p>That indicates for $m = 1$, $\phi_{exp}$ in effect produces vectors of infinite dimensions; furthermore, following the same principle of \eqref{eq:exponetial_kernel_term_i_in_taylor_series}, it can be shown that when $m \ge 2$ the dimensionality of the output space can only become "more infinite" (if infinity can really be compared). For instance, below is the case where $m = 2$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& \prod_{i=0}^{1} exp(x_i * t_i) \\
& = exp(x_0 * t_0) * exp(x_1 * t_1) \\
& = (1 + \frac{x_0}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}} + \frac{x_0^2}{\sqrt{2!}} * \frac{t_0^2}{\sqrt{2!}} + \frac{x_0^3}{\sqrt{3!}} * \frac{t_0^3}{\sqrt{3!}} + \dotsb) * \\
& \phantom{=} (1 + \frac{x_1}{\sqrt{1!}} * \frac{t_1}{\sqrt{1!}} + \frac{x_1^2}{\sqrt{2!}} * \frac{t_1^2}{\sqrt{2!}} + \frac{x_1^3}{\sqrt{3!}} * \frac{t_1^3}{\sqrt{3!}} + \dotsb) \\
& = (1 + \frac{x_0}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}} + \frac{x_0^2}{\sqrt{2!}} * \frac{t_0^2}{\sqrt{2!}} + \dotsb) + \\
& \phantom{=} (\frac{x_1}{\sqrt{1!}} * \frac{t_1}{\sqrt{1!}} + \frac{x_0}{\sqrt{1!}}\frac{x_1}{\sqrt{1!}} * \frac{t_0}{\sqrt{1!}}\frac{t_1}{\sqrt{1!}} + \dotsb) + \\
& \phantom{=} (\frac{x_1^2}{\sqrt{2!}} * \frac{t_1^2}{\sqrt{2!}} + \frac{x_0}{\sqrt{1!}}\frac{x_1^2}{\sqrt{2!}} * \frac{t_0}{\sqrt{1!}}\frac{t_1^2}{\sqrt{2!}} + \dotsb) + \dotsb \\
& = [1, \frac{x_0}{\sqrt{1!}}, \frac{x_0^2}{\sqrt{2!}}, \dotsc, \frac{x_1}{\sqrt{1!}}, \frac{x_0}{\sqrt{1!}}\frac{x_1}{\sqrt{1!}}, \frac{x_0}{\sqrt{1!}}\frac{x_1^2}{\sqrt{2!}}, \dotsc] \odot \\
& \phantom{=} [1, \frac{t_0}{\sqrt{1!}}, \frac{t_0^2}{\sqrt{2!}}, \dotsc, \frac{t_1}{\sqrt{1!}}, \frac{t_0}{\sqrt{1!}}\frac{t_1}{\sqrt{1!}}, \frac{t_0}{\sqrt{1!}}\frac{t_1^2}{\sqrt{2!}}, \dotsc] \\
& = {\phi_{exp}}_{0,1}(x) \odot {\phi_{exp}}_{0,1}(t)
\end{aligned}
\label{eq:exponetial_kernel_term_0_1_in_taylor_series}
\end{equation}
$$</p>
<p>If \eqref{eq:exponetial_kernel_term_0_1_in_taylor_series} is evident enough for convincing you that $\phi_{exp}(x)$/$\phi_{exp}(t)$ will be infinite-dimensional, then from the conclusion it follows that conducting the direct dot product of them is literally impossible; nonetheless, its kernel alternative $Ker_{\phi_{exp}}(x, t)$ can still output the evaluated result in a computationally feasible way.</p>
<h2 id="the_connection_to_attention">The Connection to Attention</h2>
<p>The attention given a query $q \in \mathbb{R}^m$ over a set of $L$ key-value pairs $\{(k_j, v_j) | j \in \mathbb{Z}, 0 \le j \le L-1, k_j \in \mathbb{R}^m, v_j \in \mathbb{R}^o\}$ is commonly defined as follows:</p>
<p>$$
\begin{equation}
\begin{aligned}
& a = \sum_{j = 0}^{L-1} \frac{ exp(\frac{q \odot k_j}{\sqrt{m}})}{\sum_{l = 0}^{L-1} exp(\frac{q \odot k_l}{\sqrt{m}})} v_j \\
& \phantom{a} = \sum_{j = 0}^{L-1} \frac{ exp(\overbrace{\frac{q}{\sqrt[4]{m}}}^{\hat{q}} \odot \overbrace{\frac{k_j}{\sqrt[4]{m}}}^{\hat{k}_j})}{\sum_{r = 0}^{L-1} exp(\frac{q}{\sqrt[4]{m}} \odot \frac{k_r}{\sqrt[4]{m}})} v_j \\ 
& \phantom{a} = \sum_{j = 0}^{L-1} \frac{ exp(\hat{q} \odot \hat{k}_j)}{\sum_{r = 0}^{L-1} exp(\hat{q} \odot \hat{k}_r)} v_j 
\end{aligned}
\label{eq:basic_attention}
\end{equation}
$$</p>
<p>Note that those terms involving exponetial functions just match the definition of \eqref{eq:exponetial_kernel} discussed in the last <a href="#what_are_kernels">section</a>; therefore \eqref{eq:basic_attention} is equivalent to:</p>
<p>$$
\begin{equation}
\begin{aligned}
& a = \sum_{j = 0}^{L-1} \frac{ Ker_{\phi_{exp}}(\hat{q}, \hat{k}_j)}{\sum_{r = 0}^{L-1} Ker_{\phi_{exp}}(\hat{q}, \hat{k}_r)} v_j
\end{aligned}
\label{eq:exponential_kernel_attention}
\end{equation}
$$</p>
<p>\eqref{eq:exponential_kernel_attention} shows that the attention weights can be viewed as the results returned by the kernel function $Ker_{\phi_{exp}}$ with normalisation; and since there are no constraints on what types of kernels can be used except for having non-negative outputs (attention weights need to be $\ge 0$), it is possible to derive the formula based on different kernels, which yield this general form of attention:</p>
<p>$$
\begin{equation}
\begin{aligned}
& a_{\phi_p} = \sum_{j = 0}^{L-1} \frac{ Ker_{\phi_p}(\hat{q}, \hat{k}_j)}{\sum_{r = 0}^{L-1} Ker_{\phi_p}(\hat{q}, \hat{k}_r)} v_j  \\
& \phantom{a_{\phi_p}} = \sum_{j = 0}^{L-1} \frac{\phi_p(\hat{q}) \odot \phi_p(\hat{k}_j)}{\sum_{r = 0}^{L-1} \phi_p(\hat{q}) \odot \phi_p(\hat{k}_r)} v_j 
\end{aligned}
\label{eq:kernel_attention}
\end{equation}
$$</p>
<p>with $\phi_p:\mathbb{R}^m \to \mathbb{R}^n$ being some mapping function which satisfies $\phi_p(x) \odot \phi_p(t) \ge 0$ for any $x$, $t \in \mathbb{R}^m$.</p>
<h2 id="fast_attention">Fast Attention</h2>
<p>One of the main advantages of the attention mechanism is its capability for gathering distant information as every part of data being attended to can be accessed directly; however, that property is also a curse due to the fact that its complexity is largely dictated by the range of attention, and it could be rather costly when modelling data containing long-range dependencies. To see that, let us extend the attention definition of \eqref{eq:basic_attention} to the multi-query version written in matrix form (assuming there are $L$ queries):</p>
<p>$$
\begin{equation}
\begin{aligned}
& A = \underbrace{\Big(\frac{exp(\frac{Q K^T}{\sqrt{m}})}{\big( exp(\frac{Q K^T}{\sqrt{m}})1_L \big) }\Big)}_{= W} V
\end{aligned}
\label{eq:basic_attention_in_matrix_form}
\end{equation}
$$</p>
<p>where $1_L$ is the all-ones column vector of $L$ rows; both the $exp(.)$ and the division are applied elementwise and broadcast if necessary; $Q \in \mathbb{R}^{L*m}$, $K \in \mathbb{R}^{L*m}$, and $V \in \mathbb{R}^{L*o}$ stand for the matrices with each row representing a query, key, and value vector respectively. The analysis of \eqref{eq:basic_attention_in_matrix_form} suggests that the time complexity for obtaining $A$ is dominated by the matrix multiplications of $Q K^T$ and $W V$, which take $O(L^2m+L^2o) = O(L^2*max(m, o))$; while the storage of $Q$, $K$, $V$, and $W$ comprises the main part of the memory consumption, resulting in the space complexity of $O(L*max(m, L, o))$. Consequently, when the dimesion of the size $L$ becomes the largest one (as is usually the case), both of the time and space complexity would be quadratic with respect to $L$, and an increase in $L$ could easily render the computation of \eqref{eq:basic_attention_in_matrix_form} prohibitively expensive.</p>
<p>So how can we alleviate that cost requirement? Well, having a close look at \eqref{eq:basic_attention_in_matrix_form}, you might notice that the calculation of $Q K^T$ consitutes the major source of the computational demand; therefore its circumvention would be the key to reduce the complexities, and that is where the kernel connection comes into play. Recall the general form of attention under the kernel interpretation introduced in the last <a href="#the_connection_to_attention">section</a>; approaching the multi-query attention from that view, in particular from \eqref{eq:kernel_attention}, allows us to define the following formulation named fast attention: </p>
<p>$$
\begin{equation}
\begin{aligned}
& A_{\phi_p} = \Big(\frac{(\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T )}{(\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T 1_L )} \Big) V \\
& \phantom{A_{\phi_p}} = \frac{\hat{Q}_{\phi_p} \overbrace{\hat{K}_{\phi_p}^T V}^{= U}}{\hat{Q}_{\phi_p} \underbrace{\hat{K}_{\phi_p}^T 1_L}_{= S}} \\
& \phantom{A_{\phi_p}} = \frac{\hat{Q}_{\phi_p} U}{\hat{Q}_{\phi_p} S } \\
\end{aligned}
\label{eq:dot_attention_in_matrix_form}
\end{equation}
$$</p>
<p>with the i-th row of $\hat{Q}_{\phi_p}$ (denoted by $(\hat{Q}_{\phi_p})_i$) being the output of $\phi_p$ given as input the i-th row of $\frac{Q}{\sqrt[4]{m}}$, and the j-th row of $\hat{K}_{\phi_p}$ (denoted by $(\hat{K}_{\phi_p})_j$) being the output of $\phi_p$ given as input the j-th row of $\frac{K}{\sqrt[4]{m}}$, for every row $i$ in $Q$ and every row $j$ in $K$, respectively. Note that how the calculation of the product $\hat{Q}_{\phi_p} \hat{K}_{\phi_p}^T$ (analogous to the term $Q K^T$ in \eqref{eq:basic_attention_in_matrix_form}) is avoided by the intermediate results of $U$ and $S$ according to the associative property of matrix multiplication enabled by the feature mapping $\phi_p$, and that is where the "fastness" comes from: the computation for $U$ and $S$ is only of a time complexity of $O(Lno+Ln) = O(Lno)$, while their storage requires merely $O(no+n) = O(no)$; furthermore, the evaluation of $A_{\phi_p}$ through $U$ and $S$ is also cheap, taking $O(Lno)$ in time and $O(Lo)$ in space. As a result, \eqref{eq:dot_attention_in_matrix_form} offers a potentially efficient way for conducting attention with the time complexity of $O(Lno)$ and the space complexity of $O(L*max(n, o))$, both of which are just linear with respect to $L$.</p>
<p>But, if you have noticed the word "potentially", there is a caveat. The entire discussion about the improvement in efficiency is premised on that $L$ remains the dominant factor in the complexity while the overheads of $\phi_p(.)$ are negligible, which implies \eqref{eq:dot_attention_in_matrix_form} can only be faster if 
1. The dimensionality of the output space of $\phi_p(.)$ is less than the number of queries/key-value pairs, i.e. $n \lt L$.
2. The cost of the evaluation of $\phi_p(\hat{q})$ and that of $\phi_p(\hat{k})$ are ralatively small compared with the one of \eqref{eq:dot_attention_in_matrix_form}.</p>
<p>Therefore the choice of the kernel (and thus its associated $\phi_p$) for producing attention weights really determines the performance of \eqref{eq:dot_attention_in_matrix_form}, and it is possible to end up being (desperately) worse than the original form! For example, the fast attention equivalent for \eqref{eq:basic_attention_in_matrix_form}, which indicates the use of $Ker_{\phi_{exp}}(., .)$, is $A_{\phi_{exp}}$; nonetheless, to evaluate it is completely impractical due to the infinite-dimensional output space of $\phi_{exp}(.)$ (that is, $n = \infty$), as pointed out in the previous <a href="#what_are_kernels">section</a>. In other words, if $A_{\phi_{exp}}$ were to be evaluated, it would require infinite time and space!</p>
<h2>Positive Random Features</h2>
<p>It is really disappointing that the fast attention can not be applied to speed up \eqref{eq:basic_attention_in_matrix_form}, and the main obstacle which blows up the complexity is the presence of $\phi_{exp}(.)$. Hence just as what has been done in \eqref{eq:dot_attention_in_matrix_form}, researchers sought to circumvent again that computationally demanding part, and in doing so they have found that actually $Ker_{\phi_{exp}}(\hat{q}, \hat{k})$ can be expressed in terms of the expectation over the standard normal distribution:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Ker_{\phi_{exp}}(\hat{q}, \hat{k}) = exp(\hat{q} \odot \hat{k}) \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) \\
& exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) = exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) * \\
& \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2} =} \underbrace{(2\pi)^{-\frac{m}{2}} \int exp(-\frac{\|z - (\hat{q} + \hat{k})\|_2^2}{2}) dz}_{= 1} \\
& \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2})} = (2\pi)^{-\frac{m}{2}} * \\
& \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2}) =} \int exp(-\|z\|_2^2) exp(z \odot (\hat{q} + \hat{k})) dz \\
& \phantom{exp(\frac{\| \hat{q} + \hat{k} \|_2^2}{2})} = E_{z\sim Normal(0, I)}[exp(z \odot (\hat{q} + \hat{k}))] \\
& \implies Ker_{\phi_{exp}}(\hat{q}, \hat{k}) = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\
& \phantom{\implies Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} E_{z\sim Normal(0, I)}[exp(z \odot (\hat{q} + \hat{k}))]
\end{aligned}
\label{eq:kernel_exp_with_expectation}
\end{equation}
$$</p>
<p>Then estimating the expectation in \eqref{eq:kernel_exp_with_expectation} via the <em>Monte Carlo method</em> with random samples $Z = \{z_0,\dotsc, z_i,\dotsc,z_{N-1} | z_i \sim Normal(0, I)\}$ yields:</p>
<p>$$
\begin{equation}
\begin{aligned}
& Ker_{\phi_{exp}}(\hat{q}, \hat{k}) \approx exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\ 
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \frac{1}{N} \sum_{i=0}^{N-1}exp(z_i \odot (\hat{q} + \hat{k})) \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = exp(-\frac{\|\hat{q}\|_2^2}{2}) exp(-\frac{\|\hat{k}\|_2^2}{2}) * \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \frac{1}{N} \sum_{i=0}^{N-1}exp(z_i \odot \hat{q}) exp(z_i \odot \hat{k}) \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = \underbrace{\begin{aligned} & \Big( \frac{exp(-\frac{\|\hat{q}\|_2^2}{2})}{\sqrt{N}} * \\
                                                                                                                            & \phantom{\Big(} [exp(z_0 \odot \hat{q}),\dotsc, exp(z_{N-1} \odot \hat{q})] \Big) \end{aligned}}_{= \phi_{Z}(\hat{q})} \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \odot \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k}) =} \underbrace{\begin{aligned} & \Big( \frac{exp(-\frac{\|\hat{k}\|_2^2}{2})}{\sqrt{N}} * \\
                                                                                                                            & \phantom{\Big(} [exp(z_0 \odot \hat{k}),\dotsc, exp(z_{N-1} \odot \hat{k})] \Big) \end{aligned}}_{= \phi_{Z}(\hat{k})} \\
& \phantom{Ker_{\phi_{exp}}(\hat{q}, \hat{k})} = \phi_{Z}(\hat{q}) \odot \phi_{Z}(\hat{k})
\end{aligned}
\end{equation}
$$</p>
<p>and there you go! a new feature mapping function $\phi_{Z}(.)$ for $Ker_{\phi_{exp}}(\hat{q}, \hat{k})$, which is one realisation of the notion called <em>Positive Random Features</em>, is attained. As the name suggests, all of the output features of $\phi_{Z}(.)$ are not only positive (just as desired for calculating attention weights), but also derived from random samples; in particular, the dimensionality of its output space is also decided by, or to be specific, equal to, the number of the samples utilised. That indicates through the trade-off between the approximation accuracy and the sample size, conducting the fast attention with $\phi_{Z}(.)$ can always be made affordable, and therefore $\phi_{Z}(.)$ serves as a viable as well as flexible alternative to $\phi_{exp}(.)$ when it comes to performing the fast attention version of \eqref{eq:basic_attention_in_matrix_form}.</p>
<p>It should also be mentioned that the nature of the uncertainty of <em>Positive Random Features</em> would inevitably bring about noise, which in turn creates the variance in the attention results and has shown to be harmful to training; so besides having an adequate number of samples, in the paper where this method was proposed other techniques such as <em>the orthogonalisation of random samples</em> or <em>periodic resampling</em> were also recommended to mitigate that negative impact. Interested readers are referred to <a href="https://arxiv.org/abs/2009.14794">the original paper</a> for more details. </p>
<h2>Extending to Autoregressive Attention</h2>
<p>This is also termed <em>unidirectional attention</em>, where each query is only allowed to attend to the previous information (excluding/including the current one) based on a given ordering. For instance, \eqref{eq:basic_attention_in_matrix_form} can be turned autoregressive by plugging in the $LT(.)$ operator which returns the lower triangular version of its input matrix:</p>
<p>$$
\begin{equation}
\begin{aligned}
& A^{ar} = \Big( \frac{ LT(exp(\frac{Q K^T}{\sqrt{m}})) }{ LT(exp(\frac{Q K^T}{\sqrt{m}}))1_L } \Big) V
\end{aligned}
\label{eq:basic_autoregressive_attention_in_matrix_form}
\end{equation}
$$</p>
<p>where the implicit ordering and autoregressive rule imposed by $LT(.)$ state that $K_j$/$V_j$, the j-th row of $K$/$V$, is available to $Q_i$, the i-th row of $Q$, if $j \le i$. In order to express \eqref{eq:basic_autoregressive_attention_in_matrix_form} in a fast-attention fashion, the trick done in \eqref{eq:dot_attention_in_matrix_form} has to be applied to individual queries separately due to the fact that the attention range of each query is now non-shareable:</p>
<p>$$
\begin{equation}
\begin{aligned}
& (A^{ar}_{\phi_Z})_i = \sum_{j = 0}^{i} \Big( \frac{(\hat{Q}_{\phi_Z})_i ((\hat{K}_{\phi_Z})_j)^T }{ \sum_{r = 0}^{i} (\hat{Q}_{\phi_Z})_i ((\hat{K}_{\phi_Z})_r)^T } \Big) V_j \\
& \phantom{(A^{ar}_{\phi_Z})_i} = \frac{(\hat{Q}_{\phi_Z})_i \overbrace{\big( \sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T V_j \big)}^{= U^i} }{ (\hat{Q}_{\phi_Z})_i \underbrace{\big(\sum_{r = 0}^{i} ((\hat{K}_{\phi_Z})_r)^T \big)}_{= S^i}  } \\
& \phantom{(A^{ar}_{\phi_Z})_i} = \frac{ \overbrace{(\hat{Q}_{\phi_Z})_i U^i}^{\hat{U}_{\phi_Z}^i} }{ \underbrace{(\hat{Q}_{\phi_Z})_i S^i}_{\hat{S}_{\phi_Z}^i} } = \frac{\hat{U}_{\phi_Z}^i}{\hat{S}_{\phi_Z}^i}
\end{aligned}
\label{eq:dot_autoregressive_attention_in_matrix_form}
\end{equation}
$$</p>
<p>Here in \eqref{eq:dot_autoregressive_attention_in_matrix_form} it is assumed that $\phi_{Z}(.)$ is adopted and $(A^{ar}_{\phi_Z})_i$ represents the i-th row of $A^{ar}_{\phi_Z}$, the approximation of $A^{ar}$ obtained via the corresponding fast attention mechanism; furthermore, by conducting a similar analysis as given for \eqref{eq:dot_attention_in_matrix_form}, it can be concluded that for any specific $i$, the time and space complexity are $O(i*no)$ and $O(i*max(n, o))$, which accounts for the iterations required by the summation, respectively.</p>
<p>One might think that because $0 \le j, r \le i \le L-1$, to obtain a full $A^{ar}_{\phi_Z}$ would requires $O(\frac{L*(L+1)}{2}*no) = O(L^2no)$ in time as well as $O(L^2*max(n, o))$ in space, and thus formulating autoregressive attention this way is no better than its orginal form in terms of computational efficiency; however, if based on the fact that</p>
<p>$$
\begin{equation}
\begin{aligned}
\begin{cases}
U^i = U^{i-1} + ((\hat{K}_{\phi_Z})_i)^T V_i  \\
S^i = S^{i-1} + ((\hat{K}_{\phi_Z})_i)^T
\end{cases}
\end{aligned}
\end{equation}
$$</p>
<p>\eqref{eq:dot_autoregressive_attention_in_matrix_form} is rewritten to</p>
<p>$$
\begin{equation}
\begin{aligned}
& (A^{ar}_{\phi_Z})_i = \frac{ (\hat{Q}_{\phi_Z})_i U^i }{  (\hat{Q}_{\phi_Z})_i S^i } \\ 
& \phantom{(A^{ar}_{\phi_Z})_i} = \frac{ (\hat{Q}_{\phi_Z})_i \big(U^{i-1} + ((\hat{K}_{\phi_Z})_i)^T V_i \big) }{ (\hat{Q}_{\phi_Z})_i \big( S^{i-1} + ((\hat{K}_{\phi_Z})_i)^T \big) }
\end{aligned}
\label{eq:cumulative_dot_autoregressive_attention_in_matrix_form}
\end{equation}
$$</p>
<p>with $U^{-1}$ and $S^{-1}$ equal to $0$; then a sequential algorithm can be designed accordingly such that it only takes the time complexity of $O(no)$ and the space complexity of $O(max(n, o))$ to evaluate $(A^{ar}_{\phi_Z})_i$ via the reuse of $U^{i-1}$ and $S^{i-1}$ obtained during the previous computation of $(A^{ar}_{\phi_Z})_{i-1}$, for every $i$. Consequently, the overall complexity linear with respect to $L$ is still achievable.</p>
<p>Another place where this idea of reuse can be shown applicable is the calculation of the associated gradients with \eqref{eq:dot_autoregressive_attention_in_matrix_form} for back-propagation. Given $\mathcal{L}$ as the single-valued objective for optimisation, the derivation of the gradient formulations similar to \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form} begins with the following concatenation matrix $J_i$ of the size $1$ by $o+1$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& J_i = [\hat{U}_{\phi_Z}^i, \hat{S}_{\phi_Z}^i] \\
& \phantom{J_i} = (\hat{Q_{\phi_Z}})_i [U^i, S^i] \\
& \phantom{J_i} = (\hat{Q_{\phi_Z}})_i \big(\sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T \underbrace{[V_j, 1]}_{= \tilde{V}_j} \big) \\
& \phantom{J_i} = (\hat{Q_{\phi_Z}})_i \big(\sum_{j = 0}^{i} ((\hat{K}_{\phi_Z})_j)^T \tilde{V}_j \big)
\end{aligned}
\end{equation}
$$</p>
<p>with respect to which the gradient of $\mathcal{L}$ is: </p>
<p>$$
\begin{equation}
\begin{aligned}
& \nabla_{J_i} \mathcal{L} = [\nabla_{\hat{U}_{\phi_Z}^i} \mathcal{L}, \nabla_{\hat{S}_{\phi_Z}^i} \mathcal{L}]
\end{aligned}
\label{eq:gradient_L_J_i}
\end{equation}
$$</p>
<p>Then the three essential gradients can be defined based on \eqref{eq:gradient_L_J_i}, including $\nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L}$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& \frac{\partial \mathcal{L}}{\partial (\hat{Q}_{\phi_Z})_{i, a}}  = \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \frac{\partial J_{i, b}}{\partial (\hat{Q}_{\phi_Z})_{i, a}} \\
& \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{Q}_{\phi_Z})_{i, a}}} = \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \big(\sum_{j = 0}^{i} (\tilde{V}_{j, b})^T (\hat{K}_{\phi_Z})_{j, a} \big) \\
& \implies \nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L} = \nabla_{J_i} \mathcal{L} \underbrace{\big(\sum_{j = 0}^{i} (\tilde{V_j})^T (\hat{K}_{\phi_Z})_j \big)}_{U_{\hat{Q}_{\phi_Z}}^i} \\
& \phantom{\implies \nabla_{(\hat{Q}_{\phi_Z})_i} \mathcal{L}} = \nabla_{J_i} \mathcal{L} \big(U_{\hat{Q}_{\phi_Z}}^{i-1} + (\tilde{V_i})^T (\hat{K}_{\phi_Z})_i \big) 
\end{aligned}
\label{eq:gradient_L_hat_Q_phi_Z_i}
\end{equation}
$$</p>
<p>$\nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L}$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& \frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}} = \sum_{i=j}^{L-1} \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} * \frac{\partial J_{i, b}}{\partial (\hat{K}_{\phi_Z})_{j, a}} \\
& \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}}} = \sum_{i=j}^{L-1} \sum_{b=0}^{o} \frac{\partial \mathcal{L}}{\partial J_{i, b}} (\hat{Q}_{\phi_Z})_{i, a} \tilde{V}_{j, b} \\
& \phantom{\frac{\partial \mathcal{L}}{\partial (\hat{K}_{\phi_Z})_{j, a}}} = \sum_{b=0}^{o} \tilde{V}_{j, b} * \big( \sum_{i=j}^{L-1}  \frac{\partial \mathcal{L}}{\partial J_{i, b}} (\hat{Q}_{\phi_Z})_{i, a} \big) \\
& \implies \nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L} = \tilde{V}_j \underbrace{\big(\sum_{i=j}^{L-1} (\nabla_{J_i} \mathcal{L})^T (\hat{Q}_{\phi_Z})_i \big)}_{= U_{\hat{K}_{\phi_Z}}^{j}} \\
& \phantom{\implies \nabla_{(\hat{K}_{\phi_Z})_j} \mathcal{L}} = \tilde{V}_j \big(U_{\hat{K}_{\phi_Z}}^{j+1} + (\nabla_{J_j} \mathcal{L})^T (\hat{Q}_{\phi_Z})_j \big)
\end{aligned}
\label{eq:gradient_L_hat_K_phi_Z_j}
\end{equation}
$$</p>
<p>and $\nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L}$:</p>
<p>$$
\begin{equation}
\begin{aligned}
& \frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}} = \sum_{i=j}^{L-1} \frac{\partial \mathcal{L}}{\partial J_{i, a}} * \frac{\partial J_{i, b}}{\partial (\tilde{V}_{\phi_Z})_{j, a}} \\
& \phantom{\frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}}} = \sum_{i=j}^{L-1} \frac{\partial \mathcal{L}}{\partial J_{i, a}} \big( \sum_{b=0}^{n-1} (\hat{Q}_{\phi_Z})_{i, b} (\hat{K}_{\phi_Z})_{j, b} \big) \\
& \phantom{\frac{\partial \mathcal{L}}{\partial (\tilde{V}_{\phi_Z})_{j, a}}} = \sum_{b=0}^{n-1} (\hat{K}_{\phi_Z})_{j, b} \big(\sum_{i=j}^{L-1} (\hat{Q}_{\phi_Z})_{i, b} \frac{\partial \mathcal{L}}{\partial J_{i, a}} \big)  \\
& \implies \nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L} = (\hat{K}_{\phi_Z})_j \underbrace{\big( \sum_{i=j}^{L-1} ((\hat{Q}_{\phi_Z})_i)^T \nabla_{J_i} \mathcal{L} \big)}_{= U_{\tilde{V}_{\phi_Z}}^{j}} \\
& \phantom{\implies \nabla_{(\tilde{V}_{\phi_Z})_j} \mathcal{L}} = (\hat{K}_{\phi_Z})_j \big(U_{\tilde{V}_{\phi_Z}}^{j+1} + ((\hat{Q}_{\phi_Z})_j)^T \nabla_{J_j} \mathcal{L} \big) \end{aligned}
\label{eq:gradient_L_tilde_V_j}
\end{equation}
$$</p>
<p>where the two subscripts of a matrix is used to index the entries of that matrix at locations encoded by (row, column) coordinates; for example, $(\hat{Q}_{\phi_Z})_{i, a}$ refers to the entry of $\hat{Q}_{\phi_Z}$ at the row $i$ and the column $a$. Since \eqref{eq:gradient_L_hat_Q_phi_Z_i}, \eqref{eq:gradient_L_hat_K_phi_Z_j}, and \eqref{eq:gradient_L_tilde_V_j} are expressed as some previous result plus an additonal update, they are certainly eligible for the sequential algorithm suggested for \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form}, albeit there is a subtle distinction: the computational sequence of \eqref{eq:cumulative_dot_autoregressive_attention_in_matrix_form} should be arranged from the position $0$ to the position $L-1$ (forward) following the autoregressive order, which is the same as the one for \eqref{eq:gradient_L_hat_Q_phi_Z_i}; in contrast, both \eqref{eq:gradient_L_hat_K_phi_Z_j} and \eqref{eq:gradient_L_tilde_V_j} would need to start their sequences at the position $L-1$ and progress down to the position $0$ (backward).</p>
<h2 class="reference-title">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/2009.14794">Krzysztof Choromanski et al. “Rethinking Attention with Performers”. In:CoRRabs/2009.14794 (2020)</a></li>
<li><a href="https://arxiv.org/abs/2006.16236">Angelos Katharopoulos et al. “Transformers are RNNs: Fast Autoregressive Transformerswith Linear Attention”. In:CoRRabs/2006.16236 (2020)</a></li>
</ul>

</br></br></br></br>

  </article>
</main>

</body>
</html>
